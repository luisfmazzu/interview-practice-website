{
  "technology": "python",
  "questions": [
    {
      "id": 9000,
      "tag": "python",
      "question": "What are the main data types in Python and how do you use type hints?",
      "answer": "Python has several built-in data types:\n\n**Numeric Types:**\n- `int`: Integers (e.g., 42, -17)\n- `float`: Floating-point numbers (e.g., 3.14, -2.5)\n- `complex`: Complex numbers (e.g., 3+4j)\n\n**Sequence Types:**\n- `str`: Strings (e.g., \"Hello\")\n- `list`: Mutable ordered collections (e.g., [1, 2, 3])\n- `tuple`: Immutable ordered collections (e.g., (1, 2, 3))\n\n**Mapping Type:**\n- `dict`: Key-value pairs (e.g., {\"name\": \"John\", \"age\": 30})\n\n**Set Types:**\n- `set`: Mutable unordered unique elements (e.g., {1, 2, 3})\n- `frozenset`: Immutable set\n\n**Boolean Type:**\n- `bool`: True or False\n\n**Type Hints** (Python 3.5+) improve code readability and enable static type checking:\n\n```python\nfrom typing import List, Dict, Optional\n\ndef process_data(numbers: List[int], name: str) -> Dict[str, int]:\n    return {\"count\": len(numbers), \"name_length\": len(name)}\n\ndef find_user(user_id: int) -> Optional[str]:\n    # Returns string or None\n    return \"John\" if user_id == 1 else None\n```\n\nType hints don't affect runtime behavior but help with IDE support, documentation, and tools like mypy for static type checking.",
      "keywords": ["data types", "type hints", "int", "float", "string", "list", "dict", "tuple", "set", "bool", "typing", "static typing"],
      "difficulty": "easy"
    },
    {
      "id": 9001,
      "tag": "python",
      "question": "Explain list comprehensions and generator expressions in Python with examples.",
      "answer": "**List Comprehensions** provide a concise way to create lists based on existing iterables:\n\n```python\n# Basic syntax: [expression for item in iterable if condition]\nnumbers = [1, 2, 3, 4, 5]\nsquares = [x**2 for x in numbers]  # [1, 4, 9, 16, 25]\n\n# With condition\neven_squares = [x**2 for x in numbers if x % 2 == 0]  # [4, 16]\n\n# Nested loops\nmatrix = [[1, 2], [3, 4], [5, 6]]\nflattened = [item for row in matrix for item in row]  # [1, 2, 3, 4, 5, 6]\n\n# String processing\nwords = [\"hello\", \"world\", \"python\"]\ncapitalized = [word.capitalize() for word in words]  # ['Hello', 'World', 'Python']\n```\n\n**Generator Expressions** use similar syntax but with parentheses, creating memory-efficient iterators:\n\n```python\n# Generator expression\nnumbers = [1, 2, 3, 4, 5]\nsquares_gen = (x**2 for x in numbers)  # Generator object\n\n# Lazy evaluation - values computed on demand\nfor square in squares_gen:\n    print(square)  # Prints: 1, 4, 9, 16, 25\n\n# Memory efficient for large datasets\nlarge_numbers = range(1000000)\nsum_of_squares = sum(x**2 for x in large_numbers)  # Memory efficient\n```\n\n**Key Differences:**\n- List comprehensions create entire list in memory\n- Generator expressions create iterator objects (lazy evaluation)\n- Generators are memory-efficient for large datasets\n- Use list comprehensions when you need the full list immediately\n- Use generators for one-time iteration or large datasets",
      "keywords": ["list comprehension", "generator expression", "iteration", "lazy evaluation", "memory efficiency", "comprehension syntax"],
      "difficulty": "easy"
    },
    {
      "id": 9002,
      "tag": "python",
      "question": "How do functions, arguments, and lambda functions work in Python?",
      "answer": "**Functions** are defined using the `def` keyword and can accept various types of arguments:\n\n```python\n# Basic function\ndef greet(name):\n    return f\"Hello, {name}!\"\n\n# Default arguments\ndef create_user(name, age=18, active=True):\n    return {\"name\": name, \"age\": age, \"active\": active}\n\n# Variable-length arguments\ndef sum_all(*args):  # *args for positional arguments\n    return sum(args)\n\ndef create_profile(**kwargs):  # **kwargs for keyword arguments\n    return {\"profile\": kwargs}\n\n# Mixed arguments (order: positional, *args, keyword, **kwargs)\ndef complex_function(required, *args, default=\"value\", **kwargs):\n    print(f\"Required: {required}\")\n    print(f\"Args: {args}\")\n    print(f\"Default: {default}\")\n    print(f\"Kwargs: {kwargs}\")\n```\n\n**Lambda Functions** are anonymous functions for simple operations:\n\n```python\n# Lambda syntax: lambda arguments: expression\nsquare = lambda x: x**2\nprint(square(5))  # 25\n\n# Common use with higher-order functions\nnumbers = [1, 2, 3, 4, 5]\nsquared = list(map(lambda x: x**2, numbers))  # [1, 4, 9, 16, 25]\neven_numbers = list(filter(lambda x: x % 2 == 0, numbers))  # [2, 4]\n\n# Sorting with lambda\nstudents = [(\"Alice\", 85), (\"Bob\", 90), (\"Charlie\", 78)]\nsorted_by_grade = sorted(students, key=lambda student: student[1])\n```\n\n**Function Annotations:**\n```python\ndef calculate_area(length: float, width: float) -> float:\n    \"\"\"Calculate rectangle area.\"\"\"\n    return length * width\n```\n\nLambdas are best for simple, one-line functions used with `map()`, `filter()`, `sorted()`, etc. For complex logic, use regular functions.",
      "keywords": ["functions", "arguments", "lambda", "default arguments", "args", "kwargs", "anonymous functions", "higher-order functions"],
      "difficulty": "easy"
    },
    {
      "id": 9003,
      "tag": "python",
      "question": "Explain object-oriented programming basics in Python including classes and inheritance.",
      "answer": "**Classes** are blueprints for creating objects with attributes and methods:\n\n```python\nclass Person:\n    # Class variable (shared by all instances)\n    species = \"Homo sapiens\"\n    \n    def __init__(self, name, age):\n        # Instance variables (unique to each instance)\n        self.name = name\n        self.age = age\n    \n    def introduce(self):\n        return f\"Hi, I'm {self.name} and I'm {self.age} years old.\"\n    \n    def have_birthday(self):\n        self.age += 1\n        return f\"Happy birthday! Now I'm {self.age}.\"\n\n# Creating instances\nperson1 = Person(\"Alice\", 25)\nperson2 = Person(\"Bob\", 30)\nprint(person1.introduce())  # \"Hi, I'm Alice and I'm 25 years old.\"\n```\n\n**Inheritance** allows classes to inherit attributes and methods from parent classes:\n\n```python\nclass Employee(Person):  # Employee inherits from Person\n    def __init__(self, name, age, job_title, salary):\n        super().__init__(name, age)  # Call parent constructor\n        self.job_title = job_title\n        self.salary = salary\n    \n    def introduce(self):  # Method overriding\n        return f\"Hi, I'm {self.name}, a {self.job_title}.\"\n    \n    def get_raise(self, amount):\n        self.salary += amount\n        return f\"Salary increased to ${self.salary}\"\n\nclass Manager(Employee):\n    def __init__(self, name, age, salary, team_size):\n        super().__init__(name, age, \"Manager\", salary)\n        self.team_size = team_size\n    \n    def manage_team(self):\n        return f\"Managing a team of {self.team_size} people.\"\n\n# Usage\nemployee = Employee(\"Charlie\", 28, \"Developer\", 75000)\nmanager = Manager(\"Diana\", 35, 95000, 5)\n\nprint(employee.introduce())  # \"Hi, I'm Charlie, a Developer.\"\nprint(manager.manage_team())  # \"Managing a team of 5 people.\"\n```\n\n**Key OOP Concepts:**\n- **Encapsulation**: Bundling data and methods together\n- **Inheritance**: Reusing code from parent classes\n- **Polymorphism**: Same method name, different implementations\n- **Abstraction**: Hiding implementation details",
      "keywords": ["classes", "objects", "inheritance", "constructor", "init", "super", "method overriding", "encapsulation", "polymorphism", "instance variables"],
      "difficulty": "easy"
    },
    {
      "id": 9004,
      "tag": "python",
      "question": "How does exception handling work in Python with try/except blocks?",
      "answer": "**Exception handling** in Python uses `try/except` blocks to gracefully handle errors:\n\n```python\n# Basic try/except\ntry:\n    result = 10 / 0\nexcept ZeroDivisionError:\n    print(\"Cannot divide by zero!\")\n    result = 0\n\n# Multiple exception types\ntry:\n    user_input = input(\"Enter a number: \")\n    number = int(user_input)\n    result = 100 / number\nexcept ValueError:\n    print(\"Invalid input - not a number\")\nexcept ZeroDivisionError:\n    print(\"Cannot divide by zero\")\nexcept Exception as e:\n    print(f\"Unexpected error: {e}\")\n\n# Complete try/except/else/finally structure\ntry:\n    file = open(\"data.txt\", \"r\")\n    data = file.read()\nexcept FileNotFoundError:\n    print(\"File not found\")\n    data = \"\"\nexcept PermissionError:\n    print(\"Permission denied\")\n    data = \"\"\nelse:\n    # Executes only if no exception occurred\n    print(\"File read successfully\")\nfinally:\n    # Always executes, regardless of exceptions\n    if 'file' in locals() and not file.closed:\n        file.close()\n        print(\"File closed\")\n```\n\n**Custom Exceptions:**\n```python\nclass InvalidAgeError(Exception):\n    \"\"\"Custom exception for invalid age values.\"\"\"\n    pass\n\ndef validate_age(age):\n    if age < 0 or age > 150:\n        raise InvalidAgeError(f\"Age {age} is not valid\")\n    return True\n\ntry:\n    validate_age(-5)\nexcept InvalidAgeError as e:\n    print(f\"Validation error: {e}\")\n```\n\n**Best Practices:**\n- Catch specific exceptions rather than using bare `except:`\n- Use `finally` for cleanup code\n- Log exceptions for debugging\n- Don't suppress exceptions unnecessarily\n- Use context managers (`with` statement) for resource management\n\n**Common Exception Types:**\n- `ValueError`: Invalid value for operation\n- `TypeError`: Wrong data type\n- `FileNotFoundError`: File doesn't exist\n- `KeyError`: Dictionary key not found\n- `IndexError`: List index out of range",
      "keywords": ["exception handling", "try except", "error handling", "finally", "else", "custom exceptions", "raise", "exception types"],
      "difficulty": "easy"
    },
    {
      "id": 9005,
      "tag": "python",
      "question": "Explain Python modules and packages and how to import and use them.",
      "answer": "**Modules** are Python files containing functions, classes, and variables. **Packages** are directories containing multiple modules.\n\n**Creating and Using Modules:**\n\n```python\n# math_utils.py (a module)\ndef add(a, b):\n    return a + b\n\ndef multiply(a, b):\n    return a * b\n\nPI = 3.14159\n\nclass Calculator:\n    def divide(self, a, b):\n        return a / b if b != 0 else None\n```\n\n**Import Methods:**\n\n```python\n# Import entire module\nimport math_utils\nresult = math_utils.add(5, 3)\nprint(math_utils.PI)\n\n# Import specific functions/classes\nfrom math_utils import add, multiply, Calculator\nresult = add(5, 3)\ncalc = Calculator()\n\n# Import with alias\nimport math_utils as math\nfrom math_utils import Calculator as Calc\n\n# Import all (not recommended)\nfrom math_utils import *\n```\n\n**Package Structure:**\n```\nmy_package/\n├── __init__.py          # Makes it a package\n├── core.py             # Module in package\n├── utils.py            # Another module\n└── subpackage/\n    ├── __init__.py\n    └── advanced.py\n```\n\n**Package Usage:**\n```python\n# my_package/__init__.py\nfrom .core import main_function\nfrom .utils import helper_function\n\n__version__ = \"1.0.0\"\n__all__ = [\"main_function\", \"helper_function\"]\n\n# Using the package\nimport my_package\nfrom my_package import main_function\nfrom my_package.subpackage import advanced\n```\n\n**Built-in Modules:**\n```python\nimport os           # Operating system interface\nimport sys          # System-specific parameters\nimport datetime     # Date and time handling\nimport json         # JSON encoder/decoder\nimport random       # Generate random numbers\nimport re           # Regular expressions\n\n# Usage examples\nprint(os.getcwd())  # Current working directory\nprint(sys.version)  # Python version\nprint(datetime.datetime.now())  # Current date/time\n```\n\n**Module Search Path:**\nPython searches for modules in:\n1. Current directory\n2. PYTHONPATH environment variable directories\n3. Standard library directories\n4. Site-packages directory (for installed packages)\n\n**Best Practices:**\n- Use descriptive module names\n- Include `__init__.py` in packages\n- Document modules with docstrings\n- Use relative imports within packages (`from .module import function`)",
      "keywords": ["modules", "packages", "import", "init", "pythonpath", "namespace", "relative imports", "built-in modules", "package structure"],
      "difficulty": "easy"
    },
    {
      "id": 9006,
      "tag": "python",
      "question": "What are decorators in Python and what are their common use cases?",
      "answer": "**Decorators** are functions that modify or extend the behavior of other functions or classes without permanently modifying their code. They follow the principle of \"wrapping\" functionality around existing code.\n\n**Basic Decorator Syntax:**\n\n```python\n# Function decorator\ndef my_decorator(func):\n    def wrapper(*args, **kwargs):\n        print(\"Before function execution\")\n        result = func(*args, **kwargs)\n        print(\"After function execution\")\n        return result\n    return wrapper\n\n# Using decorator with @ syntax\n@my_decorator\ndef greet(name):\n    return f\"Hello, {name}!\"\n\n# Equivalent to: greet = my_decorator(greet)\nprint(greet(\"Alice\"))  # Prints before/after messages + greeting\n```\n\n**Common Use Cases:**\n\n**1. Timing Functions:**\n```python\nimport time\nfrom functools import wraps\n\ndef timing_decorator(func):\n    @wraps(func)  # Preserves original function metadata\n    def wrapper(*args, **kwargs):\n        start = time.time()\n        result = func(*args, **kwargs)\n        end = time.time()\n        print(f\"{func.__name__} took {end - start:.4f} seconds\")\n        return result\n    return wrapper\n\n@timing_decorator\ndef slow_function():\n    time.sleep(1)\n    return \"Done\"\n```\n\n**2. Authentication/Authorization:**\n```python\ndef require_auth(func):\n    def wrapper(*args, **kwargs):\n        if not user_is_authenticated():\n            raise PermissionError(\"Authentication required\")\n        return func(*args, **kwargs)\n    return wrapper\n\n@require_auth\ndef delete_user(user_id):\n    # Only authenticated users can delete\n    pass\n```\n\n**3. Caching/Memoization:**\n```python\nfrom functools import lru_cache\n\n@lru_cache(maxsize=128)\ndef fibonacci(n):\n    if n < 2:\n        return n\n    return fibonacci(n-1) + fibonacci(n-2)\n```\n\n**4. Parameterized Decorators:**\n```python\ndef retry(max_attempts=3):\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            for attempt in range(max_attempts):\n                try:\n                    return func(*args, **kwargs)\n                except Exception as e:\n                    if attempt == max_attempts - 1:\n                        raise e\n                    time.sleep(1)  # Wait before retry\n        return wrapper\n    return decorator\n\n@retry(max_attempts=5)\ndef unreliable_api_call():\n    # Might fail, will retry up to 5 times\n    pass\n```\n\n**5. Class Decorators:**\n```python\ndef add_string_representation(cls):\n    def __str__(self):\n        return f\"{cls.__name__}({', '.join(f'{k}={v}' for k, v in self.__dict__.items())})\"\n    cls.__str__ = __str__\n    return cls\n\n@add_string_representation\nclass Person:\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\n```\n\nDecorators provide clean separation of concerns, making code more modular and reusable. They're extensively used in web frameworks (Flask, Django), testing frameworks, and many Python libraries.",
      "keywords": ["decorators", "wrapper functions", "functools", "wraps", "timing", "authentication", "caching", "memoization", "parameterized decorators", "class decorators"],
      "difficulty": "medium"
    },
    {
      "id": 9007,
      "tag": "python",
      "question": "Explain context managers and the 'with' statement in Python.",
      "answer": "**Context managers** provide a way to allocate and release resources precisely when you want to. The `with` statement ensures proper acquisition and release of resources, even if an exception occurs.\n\n**Basic Usage with Files:**\n\n```python\n# Without context manager (not recommended)\nfile = open(\"data.txt\", \"r\")\ndata = file.read()\nfile.close()  # Must remember to close\n\n# With context manager (recommended)\nwith open(\"data.txt\", \"r\") as file:\n    data = file.read()\n# File automatically closed when exiting the block\n```\n\n**How Context Managers Work:**\nContext managers implement two special methods:\n- `__enter__()`: Called when entering the `with` block\n- `__exit__()`: Called when exiting the `with` block (even on exceptions)\n\n**Creating Custom Context Managers:**\n\n**Method 1: Class-based**\n```python\nclass DatabaseConnection:\n    def __init__(self, db_name):\n        self.db_name = db_name\n        self.connection = None\n    \n    def __enter__(self):\n        print(f\"Connecting to {self.db_name}\")\n        self.connection = f\"connection_to_{self.db_name}\"\n        return self.connection\n    \n    def __exit__(self, exc_type, exc_value, traceback):\n        print(f\"Closing connection to {self.db_name}\")\n        if exc_type:\n            print(f\"Exception occurred: {exc_value}\")\n        self.connection = None\n        return False  # Don't suppress exceptions\n\n# Usage\nwith DatabaseConnection(\"users_db\") as conn:\n    print(f\"Using {conn}\")\n    # Connection automatically closed after this block\n```\n\n**Method 2: Using contextlib.contextmanager decorator**\n```python\nfrom contextlib import contextmanager\nimport time\n\n@contextmanager\ndef timing_context(operation_name):\n    print(f\"Starting {operation_name}\")\n    start_time = time.time()\n    try:\n        yield start_time  # Value passed to 'as' variable\n    finally:\n        end_time = time.time()\n        print(f\"{operation_name} completed in {end_time - start_time:.4f}s\")\n\n# Usage\nwith timing_context(\"data processing\") as start:\n    time.sleep(1)\n    print(f\"Started at: {start}\")\n```\n\n**Built-in Context Managers:**\n\n```python\n# Thread locks\nimport threading\nlock = threading.Lock()\nwith lock:\n    # Critical section - lock automatically acquired and released\n    shared_resource += 1\n\n# Temporary directory\nimport tempfile\nwith tempfile.TemporaryDirectory() as temp_dir:\n    # Use temp_dir for temporary files\n    print(f\"Working in: {temp_dir}\")\n# Directory automatically cleaned up\n\n# Decimal precision context\nfrom decimal import Decimal, localcontext\nwith localcontext() as ctx:\n    ctx.prec = 50  # 50 decimal places\n    result = Decimal(1) / Decimal(7)\n    print(result)  # High precision result\n```\n\n**Advanced Usage - Multiple Context Managers:**\n```python\n# Multiple contexts\nwith open(\"input.txt\") as infile, open(\"output.txt\", \"w\") as outfile:\n    data = infile.read()\n    outfile.write(data.upper())\n\n# Using contextlib.ExitStack for dynamic contexts\nfrom contextlib import ExitStack\nwith ExitStack() as stack:\n    files = [stack.enter_context(open(f\"file{i}.txt\")) for i in range(3)]\n    # All files automatically closed\n```\n\nContext managers ensure resource cleanup, exception safety, and cleaner code by eliminating the need for explicit try/finally blocks.",
      "keywords": ["context managers", "with statement", "enter", "exit", "resource management", "contextlib", "contextmanager", "exception safety", "cleanup"],
      "difficulty": "medium"
    },
    {
      "id": 9008,
      "tag": "python",
      "question": "What are iterators and generators in Python and how do they differ?",
      "answer": "**Iterators** and **generators** are fundamental concepts for creating memory-efficient, lazy-evaluated sequences in Python.\n\n**Iterators** are objects that implement the iterator protocol with `__iter__()` and `__next__()` methods:\n\n```python\nclass CountUp:\n    def __init__(self, start, end):\n        self.start = start\n        self.end = end\n    \n    def __iter__(self):\n        return self\n    \n    def __next__(self):\n        if self.start >= self.end:\n            raise StopIteration\n        self.start += 1\n        return self.start - 1\n\n# Usage\ncounter = CountUp(1, 4)\nfor num in counter:\n    print(num)  # Prints: 1, 2, 3\n\n# Manual iteration\ncounter = CountUp(1, 3)\nprint(next(counter))  # 1\nprint(next(counter))  # 2\n# print(next(counter))  # Would raise StopIteration\n```\n\n**Generators** provide a simpler way to create iterators using functions with `yield`:\n\n```python\n# Generator function\ndef count_up(start, end):\n    while start < end:\n        yield start\n        start += 1\n\n# Usage (same as iterator)\nfor num in count_up(1, 4):\n    print(num)  # Prints: 1, 2, 3\n\n# Generator maintains state between yields\ngen = count_up(1, 4)\nprint(next(gen))  # 1\nprint(next(gen))  # 2\nprint(list(gen))  # [3] - remaining values\n```\n\n**Generator Expressions** (similar to list comprehensions but lazy):\n\n```python\n# List comprehension - creates entire list in memory\nsquares_list = [x**2 for x in range(1000000)]  # Uses lots of memory\n\n# Generator expression - creates values on demand\nsquares_gen = (x**2 for x in range(1000000))   # Memory efficient\n\n# Processing large datasets\ndef process_large_file(filename):\n    with open(filename) as file:\n        for line in file:  # file object is an iterator\n            yield line.strip().upper()\n\n# Memory-efficient processing\nfor processed_line in process_large_file(\"huge_file.txt\"):\n    print(processed_line)\n```\n\n**Advanced Generator Features:**\n\n```python\n# Generator with send() method\ndef accumulator():\n    total = 0\n    while True:\n        value = yield total\n        if value is not None:\n            total += value\n\nacc = accumulator()\nnext(acc)  # Prime the generator\nprint(acc.send(10))  # 10\nprint(acc.send(5))   # 15\n\n# yield from - delegating to sub-generators\ndef sub_generator():\n    yield 1\n    yield 2\n\ndef main_generator():\n    yield \"start\"\n    yield from sub_generator()  # Delegate to sub_generator\n    yield \"end\"\n\nprint(list(main_generator()))  # ['start', 1, 2, 'end']\n```\n\n**Practical Example - Fibonacci Generator:**\n\n```python\ndef fibonacci():\n    a, b = 0, 1\n    while True:\n        yield a\n        a, b = b, a + b\n\n# Generate first 10 Fibonacci numbers\nfib = fibonacci()\nfirst_10 = [next(fib) for _ in range(10)]\nprint(first_10)  # [0, 1, 1, 2, 3, 5, 8, 13, 21, 34]\n```\n\n**Key Differences:**\n- **Iterators**: More control, implement protocol manually, maintain state in instance variables\n- **Generators**: Simpler syntax, automatic state management, function-based\n- **Memory**: Both are memory-efficient (lazy evaluation)\n- **Performance**: Generators have slight overhead but are generally preferred for simplicity\n\nGenerators are the preferred way to create iterators in Python due to their simplicity and readability.",
      "keywords": ["iterators", "generators", "yield", "lazy evaluation", "iterator protocol", "next", "StopIteration", "generator expressions", "memory efficiency", "yield from"],
      "difficulty": "medium"
    },
    {
      "id": 9009,
      "tag": "python",
      "question": "How does memory management and garbage collection work in Python?",
      "answer": "Python's **memory management** is automatic and involves several mechanisms including reference counting, garbage collection, and memory pools for efficient allocation.\n\n**Reference Counting:**\nPython tracks how many references point to each object. When the reference count reaches zero, the object is immediately deallocated:\n\n```python\nimport sys\n\n# Creating object increases reference count\nmy_list = [1, 2, 3]  # Reference count: 1\nanother_ref = my_list  # Reference count: 2\nprint(sys.getrefcount(my_list))  # Shows current reference count\n\n# Removing references decreases count\ndel another_ref  # Reference count: 1\nmy_list = None   # Reference count: 0 -> object deallocated\n```\n\n**Garbage Collection for Cycles:**\nReference counting can't handle circular references, so Python uses a cyclic garbage collector:\n\n```python\nimport gc\n\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self.parent = None\n        self.children = []\n\n# Creating circular reference\nparent = Node(\"parent\")\nchild = Node(\"child\")\nparent.children.append(child)\nchild.parent = parent  # Circular reference\n\n# Even after deleting references, objects stay in memory\ndel parent, child\n\n# Force garbage collection to clean up cycles\ncollected = gc.collect()\nprint(f\"Collected {collected} objects\")\n```\n\n**Memory Pools and Object Allocation:**\nPython uses different strategies for different object types:\n\n```python\n# Small integers are cached (-5 to 256)\na = 100\nb = 100\nprint(a is b)  # True - same object in memory\n\nc = 1000\nd = 1000\nprint(c is d)  # False - different objects\n\n# String interning for identifiers\nstr1 = \"hello\"\nstr2 = \"hello\"\nprint(str1 is str2)  # True - strings are interned\n\n# Lists are not interned\nlist1 = [1, 2, 3]\nlist2 = [1, 2, 3]\nprint(list1 is list2)  # False - different objects\n```\n\n**Memory Monitoring and Optimization:**\n\n```python\nimport gc\nimport sys\nfrom memory_profiler import profile  # pip install memory-profiler\n\n# Check garbage collection statistics\nprint(\"GC stats:\", gc.get_stats())\n\n# Monitor object creation\nprint(\"Objects before:\", len(gc.get_objects()))\ndata = [i for i in range(10000)]\nprint(\"Objects after:\", len(gc.get_objects()))\n\n# Weak references to avoid cycles\nimport weakref\n\nclass Parent:\n    def __init__(self):\n        self.children = []\n\nclass Child:\n    def __init__(self, parent):\n        self.parent = weakref.ref(parent)  # Weak reference\n        parent.children.append(self)\n\n# Memory profiling function\n@profile\ndef memory_intensive_function():\n    # Large list creation\n    big_list = [i**2 for i in range(100000)]\n    \n    # Dictionary with many entries\n    big_dict = {i: str(i) for i in range(50000)}\n    \n    # Process data\n    result = sum(big_list)\n    return result\n```\n\n**Best Practices for Memory Management:**\n\n```python\n# 1. Use generators for large datasets\ndef process_large_data():\n    # Instead of loading all data at once\n    # data = [expensive_operation(i) for i in range(1000000)]\n    \n    # Use generator for memory efficiency\n    for i in range(1000000):\n        yield expensive_operation(i)\n\n# 2. Use __slots__ to reduce memory overhead\nclass OptimizedClass:\n    __slots__ = ['name', 'age', 'email']  # Prevents __dict__ creation\n    \n    def __init__(self, name, age, email):\n        self.name = name\n        self.age = age\n        self.email = email\n\n# 3. Explicitly break circular references\nclass TreeNode:\n    def __init__(self, value):\n        self.value = value\n        self.children = []\n        self.parent = None\n    \n    def cleanup(self):\n        # Break circular references manually if needed\n        for child in self.children:\n            child.parent = None\n        self.children.clear()\n\n# 4. Use context managers for resource cleanup\nwith open(\"large_file.txt\") as f:\n    data = f.read()\n# File automatically closed, memory can be reclaimed\n```\n\n**Memory Debugging:**\n```python\n# Check for memory leaks\nimport tracemalloc\n\ntracemalloc.start()\n# ... run your code ...\ncurrent, peak = tracemalloc.get_traced_memory()\nprint(f\"Current memory usage: {current / 1024 / 1024:.1f} MB\")\nprint(f\"Peak memory usage: {peak / 1024 / 1024:.1f} MB\")\ntracemalloc.stop()\n```\n\nPython's garbage collector runs automatically, but understanding these mechanisms helps write more memory-efficient code.",
      "keywords": ["memory management", "garbage collection", "reference counting", "circular references", "memory pools", "gc module", "weak references", "slots", "memory profiling", "tracemalloc"],
      "difficulty": "medium"
    },
    {
      "id": 9010,
      "tag": "python",
      "question": "How do you handle file I/O operations and work with different file types in Python?",
      "answer": "Python provides comprehensive file I/O capabilities through built-in functions and modules for handling various file types.\n\n**Basic File Operations:**\n\n```python\n# Reading files\nwith open('data.txt', 'r') as file:\n    content = file.read()          # Read entire file\n    # file.readline()              # Read one line\n    # file.readlines()             # Read all lines as list\n\n# Writing files\nwith open('output.txt', 'w') as file:\n    file.write('Hello World\\n')\n    file.writelines(['Line 1\\n', 'Line 2\\n'])\n\n# Appending to files\nwith open('log.txt', 'a') as file:\n    file.write('New log entry\\n')\n```\n\n**File Modes and Encoding:**\n\n```python\n# Different file modes\nwith open('file.txt', 'r') as f:    # Read (default)\nwith open('file.txt', 'w') as f:    # Write (overwrites)\nwith open('file.txt', 'a') as f:    # Append\nwith open('file.txt', 'r+') as f:   # Read and write\nwith open('file.bin', 'rb') as f:   # Read binary\nwith open('file.bin', 'wb') as f:   # Write binary\n\n# Specify encoding for text files\nwith open('unicode.txt', 'r', encoding='utf-8') as f:\n    content = f.read()\n```\n\n**Working with CSV Files:**\n\n```python\nimport csv\n\n# Reading CSV\nwith open('data.csv', 'r') as file:\n    reader = csv.reader(file)\n    headers = next(reader)  # Get headers\n    for row in reader:\n        print(row)  # Each row is a list\n\n# Reading CSV as dictionaries\nwith open('data.csv', 'r') as file:\n    reader = csv.DictReader(file)\n    for row in reader:\n        print(row['name'], row['age'])  # Access by column name\n\n# Writing CSV\ndata = [['Name', 'Age'], ['Alice', 25], ['Bob', 30]]\nwith open('output.csv', 'w', newline='') as file:\n    writer = csv.writer(file)\n    writer.writerows(data)\n```\n\n**JSON File Operations:**\n\n```python\nimport json\n\n# Reading JSON\nwith open('data.json', 'r') as file:\n    data = json.load(file)\n    print(data['key'])\n\n# Writing JSON\ndata = {'name': 'Alice', 'age': 25, 'skills': ['Python', 'JavaScript']}\nwith open('output.json', 'w') as file:\n    json.dump(data, file, indent=2)\n\n# JSON string operations\njson_string = json.dumps(data, indent=2)\ndata_from_string = json.loads(json_string)\n```\n\n**Path Operations:**\n\n```python\nfrom pathlib import Path\nimport os\n\n# Modern approach with pathlib\nfile_path = Path('data/file.txt')\nif file_path.exists():\n    content = file_path.read_text()\n    file_path.write_text('New content')\n\n# Get file information\nprint(f'Size: {file_path.stat().st_size} bytes')\nprint(f'Modified: {file_path.stat().st_mtime}')\n\n# Traditional os.path approach\nif os.path.exists('file.txt'):\n    size = os.path.getsize('file.txt')\n    dirname = os.path.dirname('/path/to/file.txt')\n    basename = os.path.basename('/path/to/file.txt')\n```",
      "keywords": ["file io", "open", "read", "write", "csv", "json", "pathlib", "encoding", "file modes", "context managers"],
      "difficulty": "easy"
    },
    {
      "id": 9011,
      "tag": "python",
      "question": "How do regular expressions work in Python and what are common use cases?",
      "answer": "Python's `re` module provides powerful regular expression capabilities for pattern matching, searching, and text manipulation.\n\n**Basic Pattern Matching:**\n\n```python\nimport re\n\n# Basic search\ntext = \"Contact us at support@example.com or sales@company.org\"\npattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n\n# Find first match\nmatch = re.search(pattern, text)\nif match:\n    print(f\"Found email: {match.group()}\")  # support@example.com\n\n# Find all matches\nemail_list = re.findall(pattern, text)\nprint(email_list)  # ['support@example.com', 'sales@company.org']\n\n# Check if pattern matches from start\nif re.match(r'^Contact', text):\n    print(\"Text starts with 'Contact'\")\n```\n\n**Common Regex Patterns:**\n\n```python\n# Phone number validation\nphone_pattern = r'^\\+?1?-?\\(?\\d{3}\\)?-?\\d{3}-?\\d{4}$'\nphone_numbers = ['(555) 123-4567', '+1-555-123-4567', '5551234567']\n\nfor phone in phone_numbers:\n    if re.match(phone_pattern, phone):\n        print(f\"Valid: {phone}\")\n\n# Extract data with groups\nlog_pattern = r'(\\d{4}-\\d{2}-\\d{2}) (\\d{2}:\\d{2}:\\d{2}) (\\w+): (.+)'\nlog_entry = \"2024-01-15 14:30:25 ERROR: Database connection failed\"\n\nmatch = re.search(log_pattern, log_entry)\nif match:\n    date, time, level, message = match.groups()\n    print(f\"Date: {date}, Time: {time}, Level: {level}\")\n\n# Named groups for clarity\nnamed_pattern = r'(?P<date>\\d{4}-\\d{2}-\\d{2}) (?P<time>\\d{2}:\\d{2}:\\d{2}) (?P<level>\\w+): (?P<message>.+)'\nmatch = re.search(named_pattern, log_entry)\nif match:\n    print(f\"Level: {match.group('level')}\")\n    print(f\"Message: {match.group('message')}\")\n```\n\n**Text Replacement and Substitution:**\n\n```python\n# Simple replacement\ntext = \"The quick brown fox jumps over the lazy dog\"\nnew_text = re.sub(r'\\bfox\\b', 'cat', text)\nprint(new_text)  # \"The quick brown cat jumps over the lazy dog\"\n\n# Advanced replacement with function\ndef capitalize_match(match):\n    return match.group().upper()\n\ntext = \"hello world python\"\nresult = re.sub(r'\\b\\w+\\b', capitalize_match, text)\nprint(result)  # \"HELLO WORLD PYTHON\"\n\n# Replace with backreferences\nhtml = \"<p>Hello</p><div>World</div>\"\nplain_text = re.sub(r'<[^>]+>(.*?)</[^>]+>', r'\\1', html)\nprint(plain_text)  # \"HelloWorld\"\n```\n\n**Compiled Patterns for Performance:**\n\n```python\n# Compile pattern for reuse\nemail_pattern = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b')\n\n# Use compiled pattern\ntext_samples = [\"Email me at john@example.com\", \"No email here\", \"Contact: admin@site.org\"]\n\nfor text in text_samples:\n    if email_pattern.search(text):\n        print(f\"Email found in: {text}\")\n```\n\n**Advanced Features:**\n\n```python\n# Lookahead and lookbehind\npassword_pattern = r'^(?=.*[a-z])(?=.*[A-Z])(?=.*\\d)(?=.*[@$!%*?&])[A-Za-z\\d@$!%*?&]{8,}$'\n# Positive lookahead: must contain lowercase, uppercase, digit, special char\n\n# Split text using regex\ntext = \"apple,banana;orange:grape\"\nfruits = re.split(r'[,;:]', text)\nprint(fruits)  # ['apple', 'banana', 'orange', 'grape']\n\n# Find iterator for large texts (memory efficient)\nlarge_text = \"word1 word2 word3 \" * 10000\nword_pattern = re.compile(r'\\b\\w+\\b')\nfor match in word_pattern.finditer(large_text):\n    if match.group() == 'word2':\n        print(f\"Found 'word2' at position {match.start()}-{match.end()}\")\n        break\n```\n\n**Common Use Cases:**\n- Email/phone validation\n- Log file parsing\n- Data extraction from text\n- Input sanitization\n- URL/HTML parsing\n- Configuration file processing",
      "keywords": ["regex", "regular expressions", "pattern matching", "re module", "search", "findall", "sub", "groups", "validation", "text processing"],
      "difficulty": "easy"
    },
    {
      "id": 9012,
      "tag": "python",
      "question": "What's the difference between threading and multiprocessing in Python?",
      "answer": "Python offers both **threading** and **multiprocessing** for concurrent execution, each with distinct characteristics due to Python's Global Interpreter Lock (GIL).\n\n**Global Interpreter Lock (GIL) Impact:**\nThe GIL allows only one thread to execute Python bytecode at a time, making threading less effective for CPU-bound tasks but suitable for I/O-bound operations.\n\n**Threading - Best for I/O-bound Tasks:**\n\n```python\nimport threading\nimport time\nimport requests\n\n# I/O-bound example - web requests\ndef fetch_url(url):\n    response = requests.get(url)\n    print(f\"Fetched {url}: {response.status_code}\")\n\n# Without threading\nstart_time = time.time()\nurls = ['http://httpbin.org/delay/1'] * 5\nfor url in urls:\n    fetch_url(url)  # Takes ~5 seconds\nprint(f\"Sequential: {time.time() - start_time:.2f}s\")\n\n# With threading\nstart_time = time.time()\nthreads = []\nfor url in urls:\n    thread = threading.Thread(target=fetch_url, args=(url,))\n    threads.append(thread)\n    thread.start()\n\nfor thread in threads:\n    thread.join()  # Wait for all threads to complete\nprint(f\"Threaded: {time.time() - start_time:.2f}s\")  # Takes ~1 second\n```\n\n**Thread Synchronization:**\n\n```python\nimport threading\n\n# Shared resource with lock\nshared_counter = 0\nlock = threading.Lock()\n\ndef increment_counter():\n    global shared_counter\n    for _ in range(100000):\n        with lock:  # Prevent race conditions\n            shared_counter += 1\n\nthreads = [threading.Thread(target=increment_counter) for _ in range(5)]\nfor thread in threads:\n    thread.start()\nfor thread in threads:\n    thread.join()\n\nprint(f\"Final counter: {shared_counter}\")  # Should be 500000\n```\n\n**Multiprocessing - Best for CPU-bound Tasks:**\n\n```python\nimport multiprocessing\nimport time\n\n# CPU-bound example - mathematical computation\ndef cpu_intensive_task(n):\n    result = sum(i * i for i in range(n))\n    return result\n\nif __name__ == '__main__':  # Required for Windows\n    numbers = [1000000] * 4\n    \n    # Sequential processing\n    start_time = time.time()\n    results = [cpu_intensive_task(n) for n in numbers]\n    sequential_time = time.time() - start_time\n    \n    # Multiprocessing\n    start_time = time.time()\n    with multiprocessing.Pool() as pool:\n        results = pool.map(cpu_intensive_task, numbers)\n    parallel_time = time.time() - start_time\n    \n    print(f\"Sequential: {sequential_time:.2f}s\")\n    print(f\"Parallel: {parallel_time:.2f}s\")\n    print(f\"Speedup: {sequential_time/parallel_time:.2f}x\")\n```\n\n**Process Communication:**\n\n```python\nimport multiprocessing\nimport queue\n\n# Using Queue for inter-process communication\ndef producer(q):\n    for i in range(5):\n        q.put(f\"Item {i}\")\n        print(f\"Produced: Item {i}\")\n    q.put(None)  # Sentinel to signal completion\n\ndef consumer(q):\n    while True:\n        item = q.get()\n        if item is None:\n            break\n        print(f\"Consumed: {item}\")\n        time.sleep(0.1)\n\nif __name__ == '__main__':\n    q = multiprocessing.Queue()\n    \n    producer_process = multiprocessing.Process(target=producer, args=(q,))\n    consumer_process = multiprocessing.Process(target=consumer, args=(q,))\n    \n    producer_process.start()\n    consumer_process.start()\n    \n    producer_process.join()\n    consumer_process.join()\n```\n\n**Concurrent.futures - Higher-level Interface:**\n\n```python\nfrom concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\nimport time\n\ndef task(n):\n    time.sleep(0.1)  # Simulate work\n    return n * n\n\n# ThreadPoolExecutor for I/O-bound\nwith ThreadPoolExecutor(max_workers=4) as executor:\n    futures = [executor.submit(task, i) for i in range(10)]\n    results = [future.result() for future in futures]\n\n# ProcessPoolExecutor for CPU-bound\nwith ProcessPoolExecutor(max_workers=4) as executor:\n    futures = [executor.submit(task, i) for i in range(10)]\n    results = [future.result() for future in futures]\n```\n\n**Key Differences:**\n\n| Aspect | Threading | Multiprocessing |\n|--------|-----------|----------------|\n| **GIL Impact** | Limited by GIL | No GIL limitation |\n| **Memory** | Shared memory space | Separate memory spaces |\n| **Communication** | Shared variables | IPC (queues, pipes) |\n| **Overhead** | Lower | Higher (process creation) |\n| **Best for** | I/O-bound tasks | CPU-bound tasks |\n| **Debugging** | Race conditions | Process coordination |\n\n**Choosing the Right Approach:**\n- **Threading**: File I/O, network requests, database queries\n- **Multiprocessing**: Mathematical computations, image processing, data analysis\n- **Asyncio**: High-concurrency I/O operations (alternative to threading)",
      "keywords": ["threading", "multiprocessing", "GIL", "concurrency", "parallelism", "CPU-bound", "IO-bound", "process", "thread", "concurrent.futures"],
      "difficulty": "easy"
    },
    {
      "id": 9013,
      "tag": "python",
      "question": "How do virtual environments and package management work in Python?",
      "answer": "**Virtual environments** isolate Python projects by creating separate environments with their own dependencies, preventing conflicts between different projects.\n\n**Creating Virtual Environments:**\n\n```bash\n# Using venv (built-in Python 3.3+)\npython -m venv myproject_env\n\n# Activate virtual environment\n# On Windows:\nmyproject_env\\Scripts\\activate\n# On macOS/Linux:\nsource myproject_env/bin/activate\n\n# Deactivate\ndeactivate\n```\n\n**Package Management with pip:**\n\n```bash\n# Install packages\npip install requests\npip install flask==2.0.1  # Specific version\npip install 'django>=3.0,<4.0'  # Version range\n\n# Install from requirements file\npip install -r requirements.txt\n\n# Generate requirements file\npip freeze > requirements.txt\n\n# Upgrade packages\npip install --upgrade requests\npip list --outdated  # Show outdated packages\n\n# Uninstall packages\npip uninstall requests\n```\n\n**Managing Dependencies:**\n\n```python\n# requirements.txt example\nrequests==2.28.1\nflask==2.2.2\nnumpy>=1.20.0\npandas\npytest>=6.0  # Development dependency\n\n# requirements-dev.txt for development dependencies\n-r requirements.txt  # Include base requirements\npytest==7.1.2\nblack==22.3.0\nflake8==4.0.1\nmypy==0.961\n```\n\n**Advanced Virtual Environment Management:**\n\n```python\n# Using virtualenv (more features than venv)\npip install virtualenv\nvirtualenv myproject_env\nvirtualenv -p python3.9 myproject_env  # Specific Python version\n\n# Using conda (package and environment manager)\nconda create -n myproject python=3.9\nconda activate myproject\nconda install numpy pandas matplotlib\nconda install -c conda-forge package_name  # From specific channel\nconda env export > environment.yml  # Export environment\nconda env create -f environment.yml  # Create from file\n```\n\n**Project Structure Best Practices:**\n\n```\nmyproject/\n├── venv/                    # Virtual environment (don't commit)\n├── src/                     # Source code\n│   └── myproject/\n│       ├── __init__.py\n│       └── main.py\n├── tests/                   # Test files\n├── requirements.txt         # Production dependencies\n├── requirements-dev.txt     # Development dependencies\n├── setup.py                 # Package configuration\n├── README.md\n└── .gitignore              # Exclude venv/, __pycache__, etc.\n```\n\n**Using pipenv (Alternative Tool):**\n\n```bash\n# Install pipenv\npip install pipenv\n\n# Create environment and install packages\npipenv install requests\npipenv install pytest --dev  # Development dependency\n\n# Generate Pipfile.lock (similar to package-lock.json)\npipenv lock\n\n# Install from Pipfile\npipenv install\npipenv install --dev  # Include dev dependencies\n\n# Activate shell\npipenv shell\n\n# Run commands in environment\npipenv run python script.py\n```\n\n**Poetry (Modern Package Manager):**\n\n```bash\n# Install poetry\ncurl -sSL https://install.python-poetry.org | python3 -\n\n# Initialize new project\npoetry new myproject\ncd myproject\n\n# Add dependencies\npoetry add requests\npoetry add pytest --group dev\n\n# Install dependencies\npoetry install\n\n# Activate virtual environment\npoetry shell\n\n# Run commands\npoetry run python main.py\n```\n\n**Environment Variables and Configuration:**\n\n```python\n# .env file for environment-specific settings\nDATABASE_URL=postgresql://localhost/mydb\nAPI_KEY=your_secret_key\nDEBUG=True\n\n# Loading environment variables\nimport os\nfrom dotenv import load_dotenv  # pip install python-dotenv\n\nload_dotenv()  # Load .env file\n\nDATABASE_URL = os.getenv('DATABASE_URL')\nAPI_KEY = os.getenv('API_KEY')\nDEBUG = os.getenv('DEBUG', 'False').lower() == 'true'\n```\n\n**Docker for Environment Consistency:**\n\n```dockerfile\n# Dockerfile example\nFROM python:3.9-slim\n\nWORKDIR /app\n\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\nCOPY . .\n\nCMD [\"python\", \"main.py\"]\n```\n\n**Best Practices:**\n\n1. **Always use virtual environments** for projects\n2. **Pin dependency versions** in production\n3. **Separate dev and production** dependencies\n4. **Use .gitignore** to exclude virtual environments\n5. **Document setup instructions** in README\n6. **Regular dependency updates** with testing\n7. **Use tools like safety** to check for security vulnerabilities\n\n```bash\n# Security scanning\npip install safety\nsafety check\n```\n\nVirtual environments ensure reproducible, isolated Python environments, making deployment and collaboration much more reliable.",
      "keywords": ["virtual environment", "venv", "pip", "requirements.txt", "package management", "conda", "pipenv", "poetry", "dependencies", "isolation"],
      "difficulty": "easy"
    },
    {
      "id": 9014,
      "tag": "python",
      "question": "What are the key modules in Python's standard library and their common uses?",
      "answer": "Python's **standard library** provides a rich collection of modules for common programming tasks without requiring external dependencies.\n\n**Core Data Structure Modules:**\n\n```python\n# collections - Specialized container datatypes\nfrom collections import defaultdict, Counter, deque, namedtuple\n\n# defaultdict - dict with default values\nword_count = defaultdict(int)\nfor word in ['apple', 'banana', 'apple']:\n    word_count[word] += 1  # No KeyError\n\n# Counter - counting hashable objects\nfruits = ['apple', 'banana', 'apple', 'orange', 'banana', 'apple']\ncounter = Counter(fruits)\nprint(counter.most_common(2))  # [('apple', 3), ('banana', 2)]\n\n# deque - double-ended queue\nqueue = deque([1, 2, 3])\nqueue.appendleft(0)  # [0, 1, 2, 3]\nqueue.pop()          # [0, 1, 2]\n\n# namedtuple - lightweight class alternative\nPoint = namedtuple('Point', ['x', 'y'])\np = Point(1, 2)\nprint(p.x, p.y)  # 1 2\n```\n\n**Date and Time Handling:**\n\n```python\nfrom datetime import datetime, date, time, timedelta\nimport calendar\n\n# Current date and time\nnow = datetime.now()\ntoday = date.today()\n\n# Create specific datetime\nmy_date = datetime(2024, 1, 15, 14, 30, 0)\n\n# Date arithmetic\ntomorrow = today + timedelta(days=1)\nlast_week = now - timedelta(weeks=1)\n\n# Formatting\nformatted = now.strftime('%Y-%m-%d %H:%M:%S')\nparsed = datetime.strptime('2024-01-15', '%Y-%m-%d')\n\n# Calendar operations\nprint(calendar.month(2024, 1))  # Display calendar\nprint(calendar.isleap(2024))    # True\n```\n\n**File and Path Operations:**\n\n```python\nimport os\nimport shutil\nfrom pathlib import Path\nimport glob\n\n# os module - operating system interface\nprint(os.getcwd())              # Current directory\nos.makedirs('new_dir', exist_ok=True)\nos.environ['PATH']              # Environment variables\n\n# shutil - high-level file operations\nshutil.copy2('source.txt', 'destination.txt')\nshutil.move('old_location', 'new_location')\nshutil.rmtree('directory')      # Remove directory tree\n\n# pathlib - modern path handling\npath = Path('data/file.txt')\nif path.exists():\n    content = path.read_text()\n    path.write_text('new content')\n\n# glob - filename pattern matching\nfor txt_file in glob.glob('*.txt'):\n    print(txt_file)\n```\n\n**Internet and Network Modules:**\n\n```python\nimport urllib.request\nimport urllib.parse\nimport http.server\nimport json\nimport email.mime.text\nimport smtplib\n\n# HTTP requests\nwith urllib.request.urlopen('https://httpbin.org/json') as response:\n    data = json.loads(response.read())\n\n# URL parsing and encoding\nurl = 'https://example.com/search?q=python'\nparsed = urllib.parse.urlparse(url)\nencoded = urllib.parse.quote('hello world')  # 'hello%20world'\n\n# Simple HTTP server\n# python -m http.server 8000\n\n# Email handling\nmsg = email.mime.text.MIMEText('Hello World')\nmsg['Subject'] = 'Test Email'\nmsg['From'] = 'sender@example.com'\nmsg['To'] = 'recipient@example.com'\n```\n\n**Data Processing and Math:**\n\n```python\nimport math\nimport statistics\nimport random\nimport itertools\nfrom decimal import Decimal\nfrom fractions import Fraction\n\n# Math operations\nprint(math.sqrt(16))           # 4.0\nprint(math.ceil(4.3))          # 5\nprint(math.factorial(5))       # 120\n\n# Statistics\ndata = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nprint(statistics.mean(data))    # 5.5\nprint(statistics.median(data))  # 5.5\nprint(statistics.stdev(data))   # Standard deviation\n\n# Random operations\nrandom.seed(42)                # Reproducible randomness\nprint(random.randint(1, 10))   # Random integer\nprint(random.choice(['a', 'b', 'c']))  # Random choice\nrandom.shuffle(data)           # Shuffle in place\n\n# Itertools - iterator functions\nfor combo in itertools.combinations([1, 2, 3, 4], 2):\n    print(combo)  # (1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (3, 4)\n\n# Precise decimal arithmetic\nresult = Decimal('0.1') + Decimal('0.2')  # Exact: 0.3\nfraction = Fraction(1, 3) + Fraction(1, 6)  # 1/2\n```\n\n**System and Runtime Information:**\n\n```python\nimport sys\nimport platform\nimport subprocess\nimport argparse\n\n# System information\nprint(sys.version)              # Python version\nprint(sys.platform)             # Platform identifier\nprint(sys.path)                 # Module search path\n\n# Platform details\nprint(platform.system())        # OS name\nprint(platform.processor())     # Processor info\nprint(platform.python_version()) # Python version\n\n# Command line arguments\nparser = argparse.ArgumentParser(description='My Script')\nparser.add_argument('--input', help='Input file')\nparser.add_argument('--verbose', action='store_true')\nargs = parser.parse_args()\n\n# Running external commands\nresult = subprocess.run(['ls', '-l'], capture_output=True, text=True)\nprint(result.stdout)\n```\n\n**Serialization and Configuration:**\n\n```python\nimport pickle\nimport json\nimport csv\nimport configparser\n\n# Pickle - Python object serialization\ndata = {'name': 'Alice', 'scores': [85, 90, 78]}\nwith open('data.pkl', 'wb') as f:\n    pickle.dump(data, f)\n\nwith open('data.pkl', 'rb') as f:\n    loaded_data = pickle.load(f)\n\n# JSON handling\nwith open('config.json', 'w') as f:\n    json.dump(data, f, indent=2)\n\n# Configuration files\nconfig = configparser.ConfigParser()\nconfig['DEFAULT'] = {'debug': 'True', 'timeout': '30'}\nconfig['database'] = {'host': 'localhost', 'port': '5432'}\nwith open('config.ini', 'w') as f:\n    config.write(f)\n```\n\n**Testing and Debugging:**\n\n```python\nimport unittest\nimport doctest\nimport logging\nimport pdb\n\n# Unit testing\nclass TestMath(unittest.TestCase):\n    def test_addition(self):\n        self.assertEqual(2 + 2, 4)\n    \n    def test_division(self):\n        with self.assertRaises(ZeroDivisionError):\n            1 / 0\n\n# Logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\nlogger.info('Application started')\nlogger.error('An error occurred')\n\n# Debugging with pdb\n# pdb.set_trace()  # Breakpoint\n```\n\nThe standard library eliminates the need for many external dependencies, making Python applications more portable and reducing complexity.",
      "keywords": ["standard library", "collections", "datetime", "os", "pathlib", "json", "math", "itertools", "unittest", "logging", "subprocess"],
      "difficulty": "easy"
    },
    {
      "id": 9015,
      "tag": "python",
      "question": "How do you write and run tests in Python using unittest and pytest?",
      "answer": "Python provides excellent testing frameworks, with **unittest** built into the standard library and **pytest** as a popular third-party alternative.\n\n**unittest - Built-in Testing Framework:**\n\n```python\n# calculator.py - Code to test\nclass Calculator:\n    def add(self, a, b):\n        return a + b\n    \n    def divide(self, a, b):\n        if b == 0:\n            raise ValueError(\"Cannot divide by zero\")\n        return a / b\n    \n    def is_even(self, n):\n        return n % 2 == 0\n\n# test_calculator.py - unittest tests\nimport unittest\nfrom calculator import Calculator\n\nclass TestCalculator(unittest.TestCase):\n    def setUp(self):\n        \"\"\"Run before each test method\"\"\"\n        self.calc = Calculator()\n    \n    def tearDown(self):\n        \"\"\"Run after each test method\"\"\"\n        pass  # Cleanup if needed\n    \n    def test_add_positive_numbers(self):\n        result = self.calc.add(2, 3)\n        self.assertEqual(result, 5)\n    \n    def test_add_negative_numbers(self):\n        result = self.calc.add(-1, -1)\n        self.assertEqual(result, -2)\n    \n    def test_divide_normal(self):\n        result = self.calc.divide(10, 2)\n        self.assertEqual(result, 5.0)\n    \n    def test_divide_by_zero(self):\n        with self.assertRaises(ValueError):\n            self.calc.divide(10, 0)\n    \n    def test_is_even(self):\n        self.assertTrue(self.calc.is_even(4))\n        self.assertFalse(self.calc.is_even(3))\n    \n    @unittest.skip(\"Skipping this test\")\n    def test_skip_example(self):\n        pass\n    \n    @unittest.skipIf(True, \"Condition met, skipping\")\n    def test_conditional_skip(self):\n        pass\n\nif __name__ == '__main__':\n    unittest.main()\n```\n\n**Running unittest Tests:**\n\n```bash\n# Run single test file\npython test_calculator.py\n\n# Run all tests in directory\npython -m unittest discover\n\n# Run specific test class or method\npython -m unittest test_calculator.TestCalculator\npython -m unittest test_calculator.TestCalculator.test_add_positive_numbers\n\n# Verbose output\npython -m unittest -v test_calculator.py\n```\n\n**pytest - More Pythonic Testing:**\n\n```python\n# Install pytest\n# pip install pytest\n\n# test_calculator_pytest.py\nfrom calculator import Calculator\nimport pytest\n\n# Fixture - reusable test setup\n@pytest.fixture\ndef calculator():\n    return Calculator()\n\n# Simple test functions (no class needed)\ndef test_add_positive_numbers(calculator):\n    assert calculator.add(2, 3) == 5\n\ndef test_add_negative_numbers(calculator):\n    assert calculator.add(-1, -1) == -2\n\ndef test_divide_normal(calculator):\n    assert calculator.divide(10, 2) == 5.0\n\ndef test_divide_by_zero(calculator):\n    with pytest.raises(ValueError, match=\"Cannot divide by zero\"):\n        calculator.divide(10, 0)\n\n# Parametrized tests - run same test with different inputs\n@pytest.mark.parametrize(\"a,b,expected\", [\n    (2, 3, 5),\n    (0, 0, 0),\n    (-1, 1, 0),\n    (10, -5, 5),\n])\ndef test_add_parametrized(calculator, a, b, expected):\n    assert calculator.add(a, b) == expected\n\n@pytest.mark.parametrize(\"number,expected\", [\n    (2, True),\n    (3, False),\n    (0, True),\n    (-2, True),\n    (-3, False),\n])\ndef test_is_even_parametrized(calculator, number, expected):\n    assert calculator.is_even(number) == expected\n\n# Markers for test organization\n@pytest.mark.slow\ndef test_slow_operation(calculator):\n    # Simulate slow test\n    import time\n    time.sleep(0.1)\n    assert True\n\n@pytest.mark.integration\ndef test_integration_example():\n    # Integration test\n    assert True\n\n# Skip tests\n@pytest.mark.skip(reason=\"Not implemented yet\")\ndef test_future_feature():\n    pass\n\n@pytest.mark.skipif(True, reason=\"Condition not met\")\ndef test_conditional_skip():\n    pass\n```\n\n**Advanced pytest Features:**\n\n```python\n# conftest.py - shared fixtures and configuration\nimport pytest\nfrom calculator import Calculator\n\n@pytest.fixture(scope=\"session\")\ndef database_connection():\n    \"\"\"Session-wide fixture\"\"\"\n    # Setup expensive resource once for all tests\n    conn = \"database_connection\"\n    yield conn\n    # Cleanup\n    print(\"Closing database connection\")\n\n@pytest.fixture(scope=\"function\")  # Default scope\ndef calculator():\n    return Calculator()\n\n@pytest.fixture\ndef sample_data():\n    return [1, 2, 3, 4, 5]\n\n# Fixture with parameters\n@pytest.fixture(params=[\"sqlite\", \"postgresql\", \"mysql\"])\ndef database_type(request):\n    return request.param\n\n# test_advanced.py\nimport pytest\n\n# Using multiple fixtures\ndef test_with_multiple_fixtures(calculator, sample_data):\n    total = sum(sample_data)\n    assert calculator.add(total, 0) == 15\n\n# Testing exceptions with more detail\ndef test_exception_details(calculator):\n    with pytest.raises(ValueError) as exc_info:\n        calculator.divide(10, 0)\n    \n    assert \"Cannot divide by zero\" in str(exc_info.value)\n\n# Temporary files and directories\ndef test_with_temp_file(tmp_path):\n    # tmp_path is a pytest built-in fixture\n    test_file = tmp_path / \"test.txt\"\n    test_file.write_text(\"Hello World\")\n    assert test_file.read_text() == \"Hello World\"\n\n# Capturing output\ndef test_print_output(capsys):\n    print(\"Hello stdout\")\n    print(\"Hello stderr\", file=sys.stderr)\n    \n    captured = capsys.readouterr()\n    assert \"Hello stdout\" in captured.out\n    assert \"Hello stderr\" in captured.err\n```\n\n**Running pytest Tests:**\n\n```bash\n# Run all tests\npytest\n\n# Verbose output\npytest -v\n\n# Run specific test file\npytest test_calculator_pytest.py\n\n# Run tests matching pattern\npytest -k \"add\"\n\n# Run tests with specific markers\npytest -m \"slow\"\npytest -m \"not slow\"  # Exclude slow tests\n\n# Generate coverage report\npip install pytest-cov\npytest --cov=calculator --cov-report=html\n\n# Run tests in parallel\npip install pytest-xdist\npytest -n 4  # Use 4 CPU cores\n\n# Stop after first failure\npytest -x\n\n# Show local variables on failure\npytest -l\n```\n\n**Test Configuration (pytest.ini):**\n\n```ini\n[tool:pytest]\nminversion = 6.0\ntestpaths = tests\npython_files = test_*.py *_test.py\npython_classes = Test*\npython_functions = test_*\nmarkers =\n    slow: marks tests as slow\n    integration: marks tests as integration tests\n    unit: marks tests as unit tests\naddopts = -v --strict-markers --cov=src\n```\n\n**Best Testing Practices:**\n\n1. **Test naming**: Use descriptive names that explain what's being tested\n2. **AAA pattern**: Arrange, Act, Assert\n3. **One assertion per test** (generally)\n4. **Use fixtures** for common setup\n5. **Test edge cases** and error conditions\n6. **Keep tests independent** - no test should depend on another\n7. **Mock external dependencies** for unit tests\n8. **Aim for high coverage** but focus on critical paths\n\nBoth unittest and pytest are excellent choices, with pytest offering more features and simpler syntax for most use cases.",
      "keywords": ["testing", "unittest", "pytest", "fixtures", "assertions", "parametrize", "mocking", "test coverage", "TDD", "test automation"],
      "difficulty": "easy"
    },
    {
      "id": 9016,
      "tag": "python",
      "question": "What are metaclasses in Python and when would you use them?",
      "answer": "**Metaclasses** are classes whose instances are classes themselves. They control the creation and behavior of classes, providing a way to customize class creation at a fundamental level. The saying \"metaclasses are classes whose instances are classes\" captures their essence.\n\n**Understanding the Class Creation Process:**\n\n```python\n# When you define a class, Python actually creates it using type()\nclass MyClass:\n    def method(self):\n        return \"Hello\"\n\n# This is equivalent to:\nMyClass = type('MyClass', (), {'method': lambda self: \"Hello\"})\n\n# type() is the default metaclass\nprint(type(MyClass))  # <class 'type'>\nprint(type(type))     # <class 'type'> - type is its own metaclass\n```\n\n**Creating Custom Metaclasses:**\n\n```python\n# Method 1: Inheriting from type\nclass SingletonMeta(type):\n    _instances = {}\n    \n    def __call__(cls, *args, **kwargs):\n        if cls not in cls._instances:\n            cls._instances[cls] = super().__call__(*args, **kwargs)\n        return cls._instances[cls]\n    \n    def __new__(mcs, name, bases, attrs):\n        # Customize class creation\n        print(f\"Creating class {name}\")\n        \n        # Add automatic string representation\n        if '__str__' not in attrs:\n            attrs['__str__'] = lambda self: f\"{name} instance\"\n        \n        return super().__new__(mcs, name, bases, attrs)\n\n# Using the metaclass\nclass Database(metaclass=SingletonMeta):\n    def __init__(self, connection_string):\n        self.connection = connection_string\n    \n    def query(self, sql):\n        return f\"Executing: {sql}\"\n\n# Test singleton behavior\ndb1 = Database(\"sqlite://memory\")\ndb2 = Database(\"postgresql://localhost\")\nprint(db1 is db2)  # True - same instance\nprint(db1.connection)  # Still \"sqlite://memory\"\n```\n\n**Metaclass for Attribute Validation:**\n\n```python\nclass ValidatedMeta(type):\n    def __new__(mcs, name, bases, attrs):\n        # Collect all typed attributes\n        typed_attrs = {}\n        for key, value in attrs.items():\n            if hasattr(value, '__annotations__'):\n                typed_attrs[key] = value\n        \n        # Create validation methods\n        def __setattr__(self, name, value):\n            if name in typed_attrs:\n                expected_type = typed_attrs[name].__annotations__.get('return')\n                if expected_type and not isinstance(value, expected_type):\n                    raise TypeError(f\"{name} must be of type {expected_type.__name__}\")\n            super(cls, self).__setattr__(name, value)\n        \n        attrs['__setattr__'] = __setattr__\n        cls = super().__new__(mcs, name, bases, attrs)\n        cls._typed_attrs = typed_attrs\n        return cls\n\nclass Person(metaclass=ValidatedMeta):\n    def __init__(self, name: str, age: int):\n        self.name = name\n        self.age = age\n    \n    def set_name(self, name: str) -> str:\n        self.name = name\n        return name\n    \n    def set_age(self, age: int) -> int:\n        self.age = age\n        return age\n\n# Usage with validation\nperson = Person(\"Alice\", 25)\n# person.set_age(\"invalid\")  # Would raise TypeError\n```\n\n**ORM-style Metaclass Example:**\n\n```python\nclass ModelMeta(type):\n    def __new__(mcs, name, bases, attrs):\n        # Skip for base Model class\n        if name == 'Model':\n            return super().__new__(mcs, name, bases, attrs)\n        \n        # Collect fields\n        fields = {}\n        for key, value in list(attrs.items()):\n            if isinstance(value, Field):\n                value.name = key\n                fields[key] = value\n                attrs.pop(key)  # Remove from class attributes\n        \n        attrs['_fields'] = fields\n        attrs['_table_name'] = name.lower()\n        \n        # Add automatic save method\n        def save(self):\n            field_names = list(self._fields.keys())\n            values = [getattr(self, name) for name in field_names]\n            sql = f\"INSERT INTO {self._table_name} ({', '.join(field_names)}) VALUES ({', '.join(['?'] * len(values))})\"\n            return sql, values\n        \n        attrs['save'] = save\n        return super().__new__(mcs, name, bases, attrs)\n\nclass Field:\n    def __init__(self, field_type, required=True):\n        self.field_type = field_type\n        self.required = required\n        self.name = None\n\nclass Model(metaclass=ModelMeta):\n    def __init__(self, **kwargs):\n        for field_name, field in self._fields.items():\n            value = kwargs.get(field_name)\n            if field.required and value is None:\n                raise ValueError(f\"{field_name} is required\")\n            setattr(self, field_name, value)\n\n# Usage\nclass User(Model):\n    username = Field(str)\n    email = Field(str)\n    age = Field(int, required=False)\n\nuser = User(username=\"alice\", email=\"alice@example.com\")\nprint(user.save())  # Returns SQL and values\nprint(User._table_name)  # \"user\"\n```\n\n**Decorator-based Metaclass Alternative:**\n\n```python\n# Often, class decorators can achieve similar results more simply\ndef singleton(cls):\n    instances = {}\n    \n    def get_instance(*args, **kwargs):\n        if cls not in instances:\n            instances[cls] = cls(*args, **kwargs)\n        return instances[cls]\n    \n    return get_instance\n\n@singleton\nclass DatabaseConnection:\n    def __init__(self, url):\n        self.url = url\n\n# Usage is the same as metaclass version\ndb1 = DatabaseConnection(\"sqlite://memory\")\ndb2 = DatabaseConnection(\"postgresql://localhost\")\nprint(db1 is db2)  # True\n```\n\n**When to Use Metaclasses:**\n\n**Good Use Cases:**\n1. **Framework development** - ORMs, web frameworks\n2. **API registration** - automatic registration of classes\n3. **Validation systems** - automatic validation of class attributes\n4. **Design patterns** - singleton, factory patterns at class level\n5. **Code generation** - automatic method generation\n\n**Avoid Metaclasses When:**\n1. **Simple decoration** - use class decorators instead\n2. **Instance-level behavior** - use `__init__` or properties\n3. **One-time modifications** - manual implementation is clearer\n\n**Metaclass vs Alternatives:**\n\n```python\n# Metaclass approach\nclass AutoPropertyMeta(type):\n    def __new__(mcs, name, bases, attrs):\n        for key, value in list(attrs.items()):\n            if key.startswith('_') and not key.startswith('__'):\n                property_name = key[1:]  # Remove leading underscore\n                attrs[property_name] = property(\n                    lambda self, k=key: getattr(self, k),\n                    lambda self, val, k=key: setattr(self, k, val)\n                )\n        return super().__new__(mcs, name, bases, attrs)\n\nclass Person(metaclass=AutoPropertyMeta):\n    def __init__(self, name):\n        self._name = name  # Automatically creates 'name' property\n\n# Class decorator approach (often simpler)\ndef auto_property(cls):\n    for attr_name in dir(cls):\n        if attr_name.startswith('_') and not attr_name.startswith('__'):\n            prop_name = attr_name[1:]\n            if not hasattr(cls, prop_name):\n                setattr(cls, prop_name, property(\n                    lambda self, a=attr_name: getattr(self, a),\n                    lambda self, val, a=attr_name: setattr(self, a, val)\n                ))\n    return cls\n\n@auto_property\nclass Person2:\n    def __init__(self, name):\n        self._name = name\n```\n\n**Key Principles:**\n- Metaclasses should be used sparingly - \"explicit is better than implicit\"\n- Consider class decorators or other alternatives first\n- Use metaclasses when you need to modify class creation itself\n- Document metaclass behavior thoroughly\n- Remember: \"If you're not sure whether you need metaclasses, you probably don't\"\n\nMetaclasses are powerful but complex. They're most valuable in framework development where the added complexity is justified by the functionality they enable.",
      "keywords": ["metaclasses", "type", "class creation", "singleton", "ORM", "validation", "frameworks", "class decorators", "magic methods", "advanced python"],
      "difficulty": "medium"
    },
    {
      "id": 9017,
      "tag": "python",
      "question": "How does asynchronous programming with asyncio work in Python?",
      "answer": "**Asyncio** enables asynchronous programming in Python, allowing you to write concurrent code that can handle many I/O operations efficiently without blocking.\n\n**Basic Concepts and Syntax:**\n\n```python\nimport asyncio\nimport aiohttp\nimport time\n\n# Basic async function\nasync def hello_world():\n    print(\"Hello\")\n    await asyncio.sleep(1)  # Non-blocking sleep\n    print(\"World\")\n    return \"Done\"\n\n# Running async functions\nasync def main():\n    result = await hello_world()\n    print(f\"Result: {result}\")\n\n# Run the event loop\nasyncio.run(main())\n```\n\n**Concurrent Execution with asyncio.gather():**\n\n```python\nimport asyncio\nimport aiohttp\nimport time\n\nasync def fetch_url(session, url):\n    \"\"\"Fetch a single URL asynchronously\"\"\"\n    try:\n        async with session.get(url) as response:\n            content = await response.text()\n            return f\"{url}: {response.status} ({len(content)} chars)\"\n    except Exception as e:\n        return f\"{url}: Error - {e}\"\n\nasync def fetch_multiple_urls():\n    urls = [\n        'https://httpbin.org/delay/1',\n        'https://httpbin.org/delay/2',\n        'https://httpbin.org/delay/1',\n        'https://httpbin.org/delay/3',\n        'https://httpbin.org/delay/1'\n    ]\n    \n    # Sequential approach (slow)\n    start_time = time.time()\n    async with aiohttp.ClientSession() as session:\n        results = []\n        for url in urls:\n            result = await fetch_url(session, url)\n            results.append(result)\n    sequential_time = time.time() - start_time\n    \n    # Concurrent approach (fast)\n    start_time = time.time()\n    async with aiohttp.ClientSession() as session:\n        tasks = [fetch_url(session, url) for url in urls]\n        results = await asyncio.gather(*tasks)\n    concurrent_time = time.time() - start_time\n    \n    print(f\"Sequential: {sequential_time:.2f}s\")\n    print(f\"Concurrent: {concurrent_time:.2f}s\")\n    print(f\"Speedup: {sequential_time/concurrent_time:.2f}x\")\n    \n    return results\n\nasyncio.run(fetch_multiple_urls())\n```\n\n**Task Management and Control:**\n\n```python\nimport asyncio\n\nasync def worker(name, work_queue):\n    \"\"\"Worker that processes items from a queue\"\"\"\n    while True:\n        try:\n            # Wait for work with timeout\n            item = await asyncio.wait_for(work_queue.get(), timeout=1.0)\n            print(f\"Worker {name} processing {item}\")\n            \n            # Simulate work\n            await asyncio.sleep(0.5)\n            \n            # Mark task as done\n            work_queue.task_done()\n            \n        except asyncio.TimeoutError:\n            print(f\"Worker {name} timed out waiting for work\")\n            break\n        except Exception as e:\n            print(f\"Worker {name} error: {e}\")\n            break\n\nasync def producer_consumer_example():\n    # Create queue\n    queue = asyncio.Queue(maxsize=5)\n    \n    # Add work items\n    for i in range(10):\n        await queue.put(f\"task_{i}\")\n    \n    # Create workers\n    workers = []\n    for i in range(3):\n        task = asyncio.create_task(worker(f\"Worker-{i}\", queue))\n        workers.append(task)\n    \n    # Wait for all tasks to be processed\n    await queue.join()\n    \n    # Cancel workers\n    for task in workers:\n        task.cancel()\n    \n    # Wait for workers to finish cancellation\n    await asyncio.gather(*workers, return_exceptions=True)\n    print(\"All work completed\")\n\nasyncio.run(producer_consumer_example())\n```\n\n**Error Handling and Timeouts:**\n\n```python\nimport asyncio\nimport aiohttp\n\nasync def robust_fetch(url, timeout=5):\n    \"\"\"Fetch URL with comprehensive error handling\"\"\"\n    try:\n        async with aiohttp.ClientSession() as session:\n            async with asyncio.timeout(timeout):  # Python 3.11+\n                async with session.get(url) as response:\n                    if response.status == 200:\n                        content = await response.text()\n                        return {\"success\": True, \"data\": content[:100], \"status\": response.status}\n                    else:\n                        return {\"success\": False, \"error\": f\"HTTP {response.status}\", \"status\": response.status}\n    \n    except asyncio.TimeoutError:\n        return {\"success\": False, \"error\": \"Request timed out\"}\n    except aiohttp.ClientError as e:\n        return {\"success\": False, \"error\": f\"Client error: {e}\"}\n    except Exception as e:\n        return {\"success\": False, \"error\": f\"Unexpected error: {e}\"}\n\nasync def fetch_with_retries(url, max_retries=3, delay=1):\n    \"\"\"Fetch URL with retry logic\"\"\"\n    for attempt in range(max_retries):\n        result = await robust_fetch(url)\n        if result[\"success\"]:\n            return result\n        \n        if attempt < max_retries - 1:\n            print(f\"Attempt {attempt + 1} failed: {result['error']}. Retrying in {delay}s...\")\n            await asyncio.sleep(delay)\n            delay *= 2  # Exponential backoff\n    \n    return result\n\nasync def error_handling_example():\n    urls = [\n        'https://httpbin.org/status/200',\n        'https://httpbin.org/status/404',\n        'https://httpbin.org/delay/10',  # Will timeout\n        'https://invalid-url-that-does-not-exist.com'\n    ]\n    \n    tasks = [fetch_with_retries(url) for url in urls]\n    results = await asyncio.gather(*tasks, return_exceptions=True)\n    \n    for url, result in zip(urls, results):\n        if isinstance(result, Exception):\n            print(f\"{url}: Exception - {result}\")\n        else:\n            print(f\"{url}: {result}\")\n\nasyncio.run(error_handling_example())\n```\n\n**Synchronization Primitives:**\n\n```python\nimport asyncio\nimport random\n\nclass AsyncCounter:\n    def __init__(self):\n        self._value = 0\n        self._lock = asyncio.Lock()\n    \n    async def increment(self):\n        async with self._lock:\n            current = self._value\n            # Simulate some async work\n            await asyncio.sleep(0.01)\n            self._value = current + 1\n    \n    @property\n    def value(self):\n        return self._value\n\nasync def semaphore_example():\n    \"\"\"Limit concurrent operations with semaphore\"\"\"\n    semaphore = asyncio.Semaphore(3)  # Allow max 3 concurrent operations\n    \n    async def limited_operation(name):\n        async with semaphore:\n            print(f\"Starting {name}\")\n            await asyncio.sleep(random.uniform(1, 3))\n            print(f\"Finished {name}\")\n            return name\n    \n    # Start 10 operations, but only 3 will run concurrently\n    tasks = [limited_operation(f\"Task-{i}\") for i in range(10)]\n    results = await asyncio.gather(*tasks)\n    print(f\"All tasks completed: {results}\")\n\nasync def event_coordination():\n    \"\"\"Coordinate multiple coroutines with events\"\"\"\n    start_event = asyncio.Event()\n    \n    async def waiter(name):\n        print(f\"{name} waiting for start signal...\")\n        await start_event.wait()\n        print(f\"{name} started!\")\n        await asyncio.sleep(random.uniform(0.5, 2))\n        print(f\"{name} finished!\")\n    \n    # Create multiple waiters\n    waiters = [asyncio.create_task(waiter(f\"Worker-{i}\")) for i in range(5)]\n    \n    # Wait a bit, then signal all to start\n    await asyncio.sleep(1)\n    print(\"Sending start signal...\")\n    start_event.set()\n    \n    # Wait for all to complete\n    await asyncio.gather(*waiters)\n\n# Run examples\nasync def synchronization_demo():\n    print(\"=== Counter Example ===\")\n    counter = AsyncCounter()\n    tasks = [counter.increment() for _ in range(100)]\n    await asyncio.gather(*tasks)\n    print(f\"Final counter value: {counter.value}\")\n    \n    print(\"\\n=== Semaphore Example ===\")\n    await semaphore_example()\n    \n    print(\"\\n=== Event Coordination ===\")\n    await event_coordination()\n\nasyncio.run(synchronization_demo())\n```\n\n**Integration with Synchronous Code:**\n\n```python\nimport asyncio\nimport concurrent.futures\nimport time\nimport threading\n\ndef cpu_intensive_task(n):\n    \"\"\"Synchronous CPU-bound task\"\"\"\n    result = sum(i * i for i in range(n))\n    return result\n\ndef blocking_io_task():\n    \"\"\"Synchronous I/O task\"\"\"\n    time.sleep(2)  # Simulate blocking I/O\n    return \"I/O completed\"\n\nasync def mixed_workload():\n    \"\"\"Mix async and sync operations\"\"\"\n    loop = asyncio.get_event_loop()\n    \n    # Run CPU-bound tasks in thread pool\n    with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n        cpu_tasks = [loop.run_in_executor(executor, cpu_intensive_task, n) \n                    for n in [100000, 200000, 150000]]\n    \n    # Run blocking I/O in thread pool\n    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n        io_tasks = [loop.run_in_executor(executor, blocking_io_task) \n                   for _ in range(3)]\n    \n    # Run truly async operations\n    async def async_operation(delay):\n        await asyncio.sleep(delay)\n        return f\"Async task completed after {delay}s\"\n    \n    async_tasks = [async_operation(delay) for delay in [0.5, 1.0, 1.5]]\n    \n    # Combine all tasks\n    all_tasks = cpu_tasks + io_tasks + async_tasks\n    \n    print(\"Starting mixed workload...\")\n    start_time = time.time()\n    \n    results = await asyncio.gather(*all_tasks)\n    \n    end_time = time.time()\n    print(f\"All tasks completed in {end_time - start_time:.2f}s\")\n    \n    return results\n\nasyncio.run(mixed_workload())\n```\n\n**Key Asyncio Principles:**\n\n1. **Cooperative Multitasking**: Functions must explicitly yield control with `await`\n2. **Single-threaded**: All async code runs in one thread (unless using executors)\n3. **Event Loop**: Manages and executes async tasks\n4. **Non-blocking I/O**: Ideal for I/O-bound operations\n5. **Coroutines**: Functions defined with `async def`\n6. **Awaitables**: Objects that can be used with `await`\n\n**Best Practices:**\n- Use `async with` for async context managers\n- Don't mix blocking and non-blocking operations\n- Use `asyncio.gather()` for concurrent execution\n- Handle exceptions properly in async code\n- Use connection pooling for HTTP clients\n- Prefer `asyncio.create_task()` for fire-and-forget operations\n\nAsyncio is particularly powerful for I/O-intensive applications like web servers, API clients, and real-time data processing systems.",
      "keywords": ["asyncio", "async", "await", "coroutines", "event loop", "concurrent", "aiohttp", "gather", "semaphore", "non-blocking", "asynchronous programming"],
      "difficulty": "medium"
    },
    {
      "id": 9018,
      "tag": "python",
      "question": "What are the key techniques for optimizing Python performance?",
      "answer": "Python performance optimization involves multiple strategies ranging from algorithmic improvements to leveraging compiled extensions. Understanding bottlenecks and applying appropriate techniques is crucial for high-performance applications.\n\n**1. Profiling and Benchmarking:**\n\n```python\nimport cProfile\nimport timeit\nimport memory_profiler\nfrom functools import wraps\nimport time\n\n# Simple timing decorator\ndef timing_decorator(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        start = time.perf_counter()\n        result = func(*args, **kwargs)\n        end = time.perf_counter()\n        print(f\"{func.__name__} took {end - start:.4f} seconds\")\n        return result\n    return wrapper\n\n# Profile code execution\n@timing_decorator\ndef slow_function():\n    return sum(i**2 for i in range(100000))\n\n# Using cProfile for detailed analysis\ndef profile_code():\n    cProfile.run('slow_function()', 'profile_output.prof')\n    \n    # View results\n    import pstats\n    stats = pstats.Stats('profile_output.prof')\n    stats.sort_stats('cumulative').print_stats(10)\n\n# Memory profiling\n@memory_profiler.profile\ndef memory_intensive_function():\n    # Create large data structures\n    data = [i for i in range(1000000)]\n    processed = [x**2 for x in data]\n    return sum(processed)\n\n# Microbenchmarking with timeit\ndef benchmark_operations():\n    # Compare list comprehension vs map\n    list_comp_time = timeit.timeit(\n        lambda: [x**2 for x in range(1000)], \n        number=1000\n    )\n    \n    map_time = timeit.timeit(\n        lambda: list(map(lambda x: x**2, range(1000))), \n        number=1000\n    )\n    \n    print(f\"List comprehension: {list_comp_time:.4f}s\")\n    print(f\"Map function: {map_time:.4f}s\")\n```\n\n**2. Data Structure Optimization:**\n\n```python\nimport array\nimport collections\nfrom collections import deque, defaultdict\nimport sys\n\n# Use appropriate data structures\nclass DataStructureOptimization:\n    def list_vs_deque_operations(self):\n        # List - O(n) for insertions at beginning\n        regular_list = []\n        for i in range(10000):\n            regular_list.insert(0, i)  # Expensive!\n        \n        # Deque - O(1) for insertions at both ends\n        efficient_deque = deque()\n        for i in range(10000):\n            efficient_deque.appendleft(i)  # Fast!\n    \n    def dict_vs_defaultdict(self):\n        # Regular dict with checking\n        word_count = {}\n        words = ['apple', 'banana', 'apple', 'cherry', 'banana', 'apple']\n        \n        for word in words:\n            if word in word_count:\n                word_count[word] += 1\n            else:\n                word_count[word] = 1\n        \n        # Defaultdict - cleaner and faster\n        word_count_dd = defaultdict(int)\n        for word in words:\n            word_count_dd[word] += 1  # No checking needed\n    \n    def memory_efficient_arrays(self):\n        # Python list (flexible but memory-heavy)\n        python_list = [i for i in range(1000000)]  # ~24MB\n        \n        # Array (type-specific, memory-efficient)\n        int_array = array.array('i', range(1000000))  # ~4MB\n        \n        print(f\"List size: {sys.getsizeof(python_list)} bytes\")\n        print(f\"Array size: {sys.getsizeof(int_array)} bytes\")\n        \n        return python_list, int_array\n\n# Slots for memory optimization\nclass OptimizedClass:\n    __slots__ = ['name', 'age', 'email']  # Reduces memory overhead\n    \n    def __init__(self, name, age, email):\n        self.name = name\n        self.age = age\n        self.email = email\n\nclass RegularClass:\n    def __init__(self, name, age, email):\n        self.name = name\n        self.age = age\n        self.email = email\n\n# Compare memory usage\noptimized_instances = [OptimizedClass(f\"User{i}\", i, f\"user{i}@example.com\") for i in range(10000)]\nregular_instances = [RegularClass(f\"User{i}\", i, f\"user{i}@example.com\") for i in range(10000)]\n```\n\n**3. Algorithm and Loop Optimization:**\n\n```python\nimport bisect\nimport heapq\nfrom functools import lru_cache\n\nclass AlgorithmOptimization:\n    def efficient_searching(self):\n        # Linear search - O(n)\n        data = list(range(100000))\n        target = 75000\n        \n        # Slow way\n        found = target in data  # O(n)\n        \n        # Fast way for sorted data - O(log n)\n        sorted_data = sorted(data)\n        index = bisect.bisect_left(sorted_data, target)\n        found_fast = index < len(sorted_data) and sorted_data[index] == target\n    \n    def set_operations_vs_list(self):\n        list1 = list(range(10000))\n        list2 = list(range(5000, 15000))\n        \n        # Slow - O(n*m)\n        intersection_slow = [x for x in list1 if x in list2]\n        \n        # Fast - O(n+m)\n        set1 = set(list1)\n        set2 = set(list2)\n        intersection_fast = list(set1 & set2)\n    \n    @lru_cache(maxsize=None)\n    def fibonacci_memoized(self, n):\n        \"\"\"Cached Fibonacci - O(n) instead of O(2^n)\"\"\"\n        if n < 2:\n            return n\n        return self.fibonacci_memoized(n-1) + self.fibonacci_memoized(n-2)\n    \n    def heap_operations(self):\n        # Finding top K elements efficiently\n        data = [23, 45, 12, 78, 34, 89, 56, 67, 91, 43]\n        k = 3\n        \n        # Inefficient - O(n log n)\n        top_k_slow = sorted(data, reverse=True)[:k]\n        \n        # Efficient - O(n log k)\n        top_k_fast = heapq.nlargest(k, data)\n        \n        return top_k_slow, top_k_fast\n```\n\n**4. String and I/O Optimization:**\n\n```python\nimport io\nfrom contextlib import contextmanager\n\nclass StringOptimization:\n    def string_concatenation(self):\n        # Slow - creates new string each time\n        result_slow = \"\"\n        for i in range(10000):\n            result_slow += str(i)  # O(n^2) behavior\n        \n        # Fast - join is optimized for this\n        result_fast = ''.join(str(i) for i in range(10000))  # O(n)\n        \n        # Using StringIO for building strings\n        string_buffer = io.StringIO()\n        for i in range(10000):\n            string_buffer.write(str(i))\n        result_buffer = string_buffer.getvalue()\n    \n    def format_strings_efficiently(self):\n        name = \"Alice\"\n        age = 25\n        score = 95.5\n        \n        # Old % formatting\n        result1 = \"Name: %s, Age: %d, Score: %.2f\" % (name, age, score)\n        \n        # str.format() method\n        result2 = \"Name: {}, Age: {}, Score: {:.2f}\".format(name, age, score)\n        \n        # f-strings (fastest and most readable)\n        result3 = f\"Name: {name}, Age: {age}, Score: {score:.2f}\"\n        \n        return result1, result2, result3\n\nclass IOOptimization:\n    def batch_file_operations(self):\n        # Reading large files efficiently\n        def read_large_file_memory_efficient(filename):\n            \"\"\"Read large file without loading entire content\"\"\"\n            with open(filename, 'r') as file:\n                for line in file:  # Iterator - memory efficient\n                    yield line.strip()\n        \n        # Batch writing\n        def write_batch_data(filename, data):\n            with open(filename, 'w') as file:\n                # Write in chunks instead of line by line\n                chunk_size = 1000\n                for i in range(0, len(data), chunk_size):\n                    chunk = data[i:i+chunk_size]\n                    file.writelines(f\"{item}\\n\" for item in chunk)\n    \n    @contextmanager\n    def buffered_file_writer(self, filename, buffer_size=8192):\n        \"\"\"Custom buffered writer\"\"\"\n        buffer = []\n        \n        def flush_buffer():\n            if buffer:\n                with open(filename, 'a') as f:\n                    f.writelines(buffer)\n                buffer.clear()\n        \n        try:\n            yield lambda line: (\n                buffer.append(line + '\\n'),\n                flush_buffer() if len(buffer) >= buffer_size else None\n            )\n        finally:\n            flush_buffer()\n```\n\n**5. Leveraging Built-in Functions and Libraries:**\n\n```python\nimport operator\nimport itertools\nfrom functools import reduce\nimport numpy as np\n\nclass BuiltinOptimization:\n    def use_builtin_functions(self):\n        data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n        \n        # Slow - manual implementation\n        total_slow = 0\n        for item in data:\n            total_slow += item\n        \n        # Fast - built-in sum\n        total_fast = sum(data)\n        \n        # Using operator module for better performance\n        product = reduce(operator.mul, data)  # Faster than manual loop\n        \n        # Filter with built-in functions\n        evens = list(filter(lambda x: x % 2 == 0, data))\n        \n        # Map operations\n        squares = list(map(lambda x: x**2, data))\n    \n    def itertools_optimizations(self):\n        # Efficient combinations and permutations\n        data = range(100)\n        \n        # Get first 10 combinations without generating all\n        first_10_combinations = list(itertools.islice(\n            itertools.combinations(data, 3), 10\n        ))\n        \n        # Chain iterators efficiently\n        list1 = range(1000)\n        list2 = range(1000, 2000)\n        list3 = range(2000, 3000)\n        \n        # Instead of concatenating lists\n        chained = itertools.chain(list1, list2, list3)\n        \n        # Groupby for efficient grouping\n        data_with_keys = [(i % 3, i) for i in range(20)]\n        grouped = itertools.groupby(sorted(data_with_keys), key=lambda x: x[0])\n    \n    def numpy_for_numerical_computing(self):\n        # Pure Python (slow)\n        python_list = list(range(1000000))\n        python_result = sum(x**2 for x in python_list)\n        \n        # NumPy (much faster for numerical operations)\n        numpy_array = np.arange(1000000)\n        numpy_result = np.sum(numpy_array**2)\n        \n        # Vectorized operations\n        matrix_a = np.random.rand(1000, 1000)\n        matrix_b = np.random.rand(1000, 1000)\n        \n        # Matrix multiplication (highly optimized)\n        result = np.dot(matrix_a, matrix_b)\n        \n        return python_result, numpy_result, result\n```\n\n**6. Caching and Memoization:**\n\n```python\nfrom functools import lru_cache, wraps\nimport pickle\nimport os\nfrom typing import Any, Callable\n\n# Persistent caching decorator\ndef disk_cache(cache_file: str):\n    def decorator(func: Callable) -> Callable:\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            # Create cache key\n            cache_key = (args, tuple(sorted(kwargs.items())))\n            \n            # Load existing cache\n            cache = {}\n            if os.path.exists(cache_file):\n                try:\n                    with open(cache_file, 'rb') as f:\n                        cache = pickle.load(f)\n                except:\n                    cache = {}\n            \n            # Check if result is cached\n            if cache_key in cache:\n                return cache[cache_key]\n            \n            # Compute result and cache it\n            result = func(*args, **kwargs)\n            cache[cache_key] = result\n            \n            # Save cache to disk\n            with open(cache_file, 'wb') as f:\n                pickle.dump(cache, f)\n            \n            return result\n        return wrapper\n    return decorator\n\nclass CachingExamples:\n    @lru_cache(maxsize=128)\n    def expensive_computation(self, n: int) -> int:\n        \"\"\"Simulate expensive computation with caching\"\"\"\n        print(f\"Computing for {n}...\")\n        return sum(i**2 for i in range(n))\n    \n    @disk_cache('fibonacci_cache.pkl')\n    def fibonacci_persistent(self, n: int) -> int:\n        \"\"\"Fibonacci with persistent disk caching\"\"\"\n        if n < 2:\n            return n\n        return self.fibonacci_persistent(n-1) + self.fibonacci_persistent(n-2)\n```\n\n**7. Compilation and Extensions:**\n\n```python\n# Using Numba for JIT compilation\nfrom numba import jit, vectorize\nimport numpy as np\n\n@jit(nopython=True)  # Compile to machine code\ndef fast_sum_of_squares(arr):\n    total = 0.0\n    for i in range(len(arr)):\n        total += arr[i] ** 2\n    return total\n\n@vectorize(['float64(float64)'], target='cpu')\ndef fast_square(x):\n    return x ** 2\n\n# Example with Cython (requires compilation)\n# Save as fast_math.pyx\n\"\"\"\ndef fast_fibonacci(int n):\n    cdef int a = 0\n    cdef int b = 1\n    cdef int i\n    \n    if n <= 1:\n        return n\n    \n    for i in range(2, n + 1):\n        a, b = b, a + b\n    \n    return b\n\"\"\"\n\n# Setup.py for compiling Cython\n\"\"\"\nfrom setuptools import setup\nfrom Cython.Build import cythonize\n\nsetup(\n    ext_modules = cythonize(\"fast_math.pyx\")\n)\n\"\"\"\n```\n\n**Performance Optimization Checklist:**\n\n1. **Profile first** - identify actual bottlenecks\n2. **Use appropriate data structures** - lists, sets, dicts, deques\n3. **Leverage built-in functions** - sum(), map(), filter()\n4. **Cache expensive computations** - @lru_cache, custom caching\n5. **Optimize loops** - list comprehensions, generators\n6. **Use NumPy for numerical work** - vectorized operations\n7. **Consider compilation** - Numba, Cython for critical paths\n8. **Memory management** - generators, __slots__, del unused objects\n9. **I/O optimization** - batch operations, buffering\n10. **Algorithm optimization** - better time complexity\n\n**Tools for Performance Analysis:**\n- `cProfile` - function-level profiling\n- `line_profiler` - line-by-line profiling\n- `memory_profiler` - memory usage analysis\n- `py-spy` - sampling profiler\n- `timeit` - microbenchmarking\n- `snakeviz` - visualizing cProfile output\n\nRemember: \"Premature optimization is the root of all evil\" - always profile first and optimize based on actual bottlenecks.",
      "keywords": ["performance optimization", "profiling", "cProfile", "algorithms", "data structures", "caching", "numpy", "numba", "cython", "memory optimization", "benchmarking"],
      "difficulty": "hard"
    },
    {
      "id": 9019,
      "tag": "python",
      "question": "What are advanced design patterns commonly used in Python development?",
      "answer": "Advanced design patterns in Python leverage the language's dynamic features like decorators, metaclasses, and first-class functions. Here are key patterns for enterprise-level applications.\n\n**1. Observer Pattern with Decorators:**\n\n```python\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, List, Callable, Any\nfrom functools import wraps\nimport weakref\n\nclass EventManager:\n    \"\"\"Central event management system\"\"\"\n    def __init__(self):\n        self._observers: Dict[str, List[weakref.WeakMethod]] = {}\n    \n    def subscribe(self, event_type: str, callback: Callable):\n        if event_type not in self._observers:\n            self._observers[event_type] = []\n        \n        # Use weak references to prevent memory leaks\n        if hasattr(callback, '__self__'):\n            self._observers[event_type].append(weakref.WeakMethod(callback))\n        else:\n            self._observers[event_type].append(weakref.ref(callback))\n    \n    def publish(self, event_type: str, data: Any = None):\n        if event_type not in self._observers:\n            return\n        \n        # Clean up dead references and call alive ones\n        alive_observers = []\n        for obs_ref in self._observers[event_type]:\n            observer = obs_ref()\n            if observer is not None:\n                alive_observers.append(obs_ref)\n                try:\n                    observer(data)\n                except Exception as e:\n                    print(f\"Observer error: {e}\")\n        \n        self._observers[event_type] = alive_observers\n\n# Global event manager\nevent_manager = EventManager()\n\n# Decorator for automatic event subscription\ndef listens_to(*event_types):\n    def decorator(func):\n        for event_type in event_types:\n            event_manager.subscribe(event_type, func)\n        return func\n    return decorator\n\n# Usage example\nclass UserService:\n    def create_user(self, username: str):\n        user = {'username': username, 'id': 123}\n        event_manager.publish('user_created', user)\n        return user\n\nclass EmailService:\n    @listens_to('user_created')\n    def send_welcome_email(self, user_data):\n        print(f\"Sending welcome email to {user_data['username']}\")\n\nclass AnalyticsService:\n    @listens_to('user_created')\n    def track_user_signup(self, user_data):\n        print(f\"Tracking signup for user {user_data['id']}\")\n```\n\n**2. Strategy Pattern with Dynamic Loading:**\n\n```python\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, Type\nimport importlib\nimport inspect\n\nclass PaymentStrategy(ABC):\n    @abstractmethod\n    def process_payment(self, amount: float, **kwargs) -> dict:\n        pass\n    \n    @abstractmethod\n    def validate_payment_data(self, **kwargs) -> bool:\n        pass\n\nclass CreditCardPayment(PaymentStrategy):\n    def process_payment(self, amount: float, **kwargs) -> dict:\n        card_number = kwargs.get('card_number')\n        return {\n            'status': 'success',\n            'transaction_id': f'cc_{card_number}_{amount}',\n            'amount': amount\n        }\n    \n    def validate_payment_data(self, **kwargs) -> bool:\n        return 'card_number' in kwargs and 'cvv' in kwargs\n\nclass PayPalPayment(PaymentStrategy):\n    def process_payment(self, amount: float, **kwargs) -> dict:\n        email = kwargs.get('email')\n        return {\n            'status': 'success',\n            'transaction_id': f'pp_{email}_{amount}',\n            'amount': amount\n        }\n    \n    def validate_payment_data(self, **kwargs) -> bool:\n        return 'email' in kwargs and '@' in kwargs['email']\n\nclass PaymentProcessor:\n    def __init__(self):\n        self._strategies: Dict[str, Type[PaymentStrategy]] = {}\n        self._load_strategies()\n    \n    def _load_strategies(self):\n        \"\"\"Dynamically load all payment strategies\"\"\"\n        # Get all classes in current module that inherit from PaymentStrategy\n        current_module = inspect.getmodule(self)\n        for name, obj in inspect.getmembers(current_module):\n            if (inspect.isclass(obj) and \n                issubclass(obj, PaymentStrategy) and \n                obj != PaymentStrategy):\n                strategy_name = name.replace('Payment', '').lower()\n                self._strategies[strategy_name] = obj\n    \n    def register_strategy(self, name: str, strategy_class: Type[PaymentStrategy]):\n        \"\"\"Register a new payment strategy\"\"\"\n        self._strategies[name] = strategy_class\n    \n    def process_payment(self, method: str, amount: float, **kwargs) -> dict:\n        if method not in self._strategies:\n            raise ValueError(f\"Unsupported payment method: {method}\")\n        \n        strategy = self._strategies[method]()\n        \n        if not strategy.validate_payment_data(**kwargs):\n            return {'status': 'error', 'message': 'Invalid payment data'}\n        \n        return strategy.process_payment(amount, **kwargs)\n    \n    def available_methods(self) -> List[str]:\n        return list(self._strategies.keys())\n\n# Usage\nprocessor = PaymentProcessor()\nresult = processor.process_payment('creditcard', 100.0, card_number='1234', cvv='123')\nprint(result)\n```\n\n**3. Command Pattern with Undo/Redo:**\n\n```python\nfrom abc import ABC, abstractmethod\nfrom typing import List, Optional, Any\nfrom dataclasses import dataclass\nfrom datetime import datetime\n\nclass Command(ABC):\n    @abstractmethod\n    def execute(self) -> Any:\n        pass\n    \n    @abstractmethod\n    def undo(self) -> Any:\n        pass\n    \n    @property\n    @abstractmethod\n    def description(self) -> str:\n        pass\n\n@dataclass\nclass Document:\n    content: str = \"\"\n    \n    def insert_text(self, position: int, text: str):\n        self.content = self.content[:position] + text + self.content[position:]\n    \n    def delete_text(self, position: int, length: int):\n        self.content = self.content[:position] + self.content[position + length:]\n    \n    def replace_text(self, old_text: str, new_text: str):\n        self.content = self.content.replace(old_text, new_text)\n\nclass InsertTextCommand(Command):\n    def __init__(self, document: Document, position: int, text: str):\n        self.document = document\n        self.position = position\n        self.text = text\n    \n    def execute(self) -> Any:\n        self.document.insert_text(self.position, self.text)\n        return f\"Inserted '{self.text}' at position {self.position}\"\n    \n    def undo(self) -> Any:\n        self.document.delete_text(self.position, len(self.text))\n        return f\"Removed '{self.text}' from position {self.position}\"\n    \n    @property\n    def description(self) -> str:\n        return f\"Insert '{self.text}' at {self.position}\"\n\nclass DeleteTextCommand(Command):\n    def __init__(self, document: Document, position: int, length: int):\n        self.document = document\n        self.position = position\n        self.length = length\n        self.deleted_text: Optional[str] = None\n    \n    def execute(self) -> Any:\n        self.deleted_text = self.document.content[self.position:self.position + self.length]\n        self.document.delete_text(self.position, self.length)\n        return f\"Deleted '{self.deleted_text}' from position {self.position}\"\n    \n    def undo(self) -> Any:\n        if self.deleted_text:\n            self.document.insert_text(self.position, self.deleted_text)\n            return f\"Restored '{self.deleted_text}' at position {self.position}\"\n    \n    @property\n    def description(self) -> str:\n        return f\"Delete {self.length} chars at {self.position}\"\n\nclass CommandHistory:\n    def __init__(self, max_history: int = 100):\n        self.history: List[Command] = []\n        self.current_index = -1\n        self.max_history = max_history\n    \n    def execute_command(self, command: Command) -> Any:\n        # Remove any commands after current index (for branching history)\n        self.history = self.history[:self.current_index + 1]\n        \n        # Execute and add to history\n        result = command.execute()\n        self.history.append(command)\n        self.current_index += 1\n        \n        # Limit history size\n        if len(self.history) > self.max_history:\n            self.history.pop(0)\n            self.current_index -= 1\n        \n        return result\n    \n    def undo(self) -> Optional[Any]:\n        if self.current_index >= 0:\n            command = self.history[self.current_index]\n            result = command.undo()\n            self.current_index -= 1\n            return result\n        return None\n    \n    def redo(self) -> Optional[Any]:\n        if self.current_index < len(self.history) - 1:\n            self.current_index += 1\n            command = self.history[self.current_index]\n            return command.execute()\n        return None\n    \n    def get_history(self) -> List[str]:\n        return [cmd.description for cmd in self.history]\n\n# Usage\ndocument = Document(\"Hello World\")\nhistory = CommandHistory()\n\n# Execute commands\nhistory.execute_command(InsertTextCommand(document, 5, \" Beautiful\"))\nprint(document.content)  # \"Hello Beautiful World\"\n\nhistory.execute_command(DeleteTextCommand(document, 0, 6))\nprint(document.content)  # \"Beautiful World\"\n\n# Undo operations\nhistory.undo()\nprint(document.content)  # \"Hello Beautiful World\"\n\nhistory.undo()\nprint(document.content)  # \"Hello World\"\n\n# Redo operations\nhistory.redo()\nprint(document.content)  # \"Hello Beautiful World\"\n```\n\n**4. Factory Pattern with Registration:**\n\n```python\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, Type, Any, Optional\nfrom functools import wraps\n\nclass DatabaseConnection(ABC):\n    @abstractmethod\n    def connect(self) -> str:\n        pass\n    \n    @abstractmethod\n    def execute_query(self, query: str) -> Any:\n        pass\n\nclass DatabaseFactory:\n    _connection_types: Dict[str, Type[DatabaseConnection]] = {}\n    _instances: Dict[str, DatabaseConnection] = {}\n    \n    @classmethod\n    def register(cls, name: str):\n        \"\"\"Decorator to register database connection types\"\"\"\n        def decorator(connection_class: Type[DatabaseConnection]):\n            cls._connection_types[name] = connection_class\n            return connection_class\n        return decorator\n    \n    @classmethod\n    def create_connection(cls, db_type: str, **kwargs) -> DatabaseConnection:\n        if db_type not in cls._connection_types:\n            raise ValueError(f\"Unknown database type: {db_type}\")\n        \n        # Singleton pattern for connections\n        cache_key = f\"{db_type}_{hash(tuple(sorted(kwargs.items())))}\"\n        \n        if cache_key not in cls._instances:\n            connection_class = cls._connection_types[db_type]\n            cls._instances[cache_key] = connection_class(**kwargs)\n        \n        return cls._instances[cache_key]\n    \n    @classmethod\n    def available_types(cls) -> List[str]:\n        return list(cls._connection_types.keys())\n    \n    @classmethod\n    def clear_cache(cls):\n        cls._instances.clear()\n\n# Auto-registration using decorator\n@DatabaseFactory.register('postgresql')\nclass PostgreSQLConnection(DatabaseConnection):\n    def __init__(self, host='localhost', port=5432, **kwargs):\n        self.host = host\n        self.port = port\n        self.config = kwargs\n    \n    def connect(self) -> str:\n        return f\"Connected to PostgreSQL at {self.host}:{self.port}\"\n    \n    def execute_query(self, query: str) -> Any:\n        return f\"PostgreSQL executing: {query}\"\n\n@DatabaseFactory.register('mysql')\nclass MySQLConnection(DatabaseConnection):\n    def __init__(self, host='localhost', port=3306, **kwargs):\n        self.host = host\n        self.port = port\n        self.config = kwargs\n    \n    def connect(self) -> str:\n        return f\"Connected to MySQL at {self.host}:{self.port}\"\n    \n    def execute_query(self, query: str) -> Any:\n        return f\"MySQL executing: {query}\"\n\n@DatabaseFactory.register('sqlite')\nclass SQLiteConnection(DatabaseConnection):\n    def __init__(self, database_path=':memory:', **kwargs):\n        self.database_path = database_path\n        self.config = kwargs\n    \n    def connect(self) -> str:\n        return f\"Connected to SQLite database: {self.database_path}\"\n    \n    def execute_query(self, query: str) -> Any:\n        return f\"SQLite executing: {query}\"\n\n# Usage\nconnections = {\n    'pg': DatabaseFactory.create_connection('postgresql', host='db.example.com'),\n    'mysql': DatabaseFactory.create_connection('mysql', host='mysql.example.com'),\n    'sqlite': DatabaseFactory.create_connection('sqlite', database_path='./app.db')\n}\n\nfor name, conn in connections.items():\n    print(f\"{name}: {conn.connect()}\")\n```\n\n**5. Dependency Injection Container:**\n\n```python\nfrom typing import Dict, Type, Any, Callable, Optional\nfrom functools import wraps\nimport inspect\nfrom dataclasses import dataclass\n\n@dataclass\nclass ServiceConfig:\n    service_type: Type\n    factory: Optional[Callable] = None\n    singleton: bool = True\n    dependencies: Optional[Dict[str, str]] = None\n\nclass DIContainer:\n    def __init__(self):\n        self._services: Dict[str, ServiceConfig] = {}\n        self._instances: Dict[str, Any] = {}\n    \n    def register(self, name: str, service_type: Type, \n                singleton: bool = True, factory: Optional[Callable] = None):\n        \"\"\"Register a service with the container\"\"\"\n        # Auto-detect dependencies from constructor\n        dependencies = {}\n        if hasattr(service_type, '__init__'):\n            sig = inspect.signature(service_type.__init__)\n            for param_name, param in sig.parameters.items():\n                if param_name != 'self' and param.annotation != inspect.Parameter.empty:\n                    dependencies[param_name] = param.annotation.__name__.lower()\n        \n        self._services[name] = ServiceConfig(\n            service_type=service_type,\n            factory=factory,\n            singleton=singleton,\n            dependencies=dependencies\n        )\n    \n    def resolve(self, name: str) -> Any:\n        \"\"\"Resolve a service and its dependencies\"\"\"\n        if name not in self._services:\n            raise ValueError(f\"Service '{name}' not registered\")\n        \n        config = self._services[name]\n        \n        # Return cached instance if singleton\n        if config.singleton and name in self._instances:\n            return self._instances[name]\n        \n        # Resolve dependencies\n        dependencies = {}\n        if config.dependencies:\n            for param_name, service_name in config.dependencies.items():\n                dependencies[param_name] = self.resolve(service_name)\n        \n        # Create instance\n        if config.factory:\n            instance = config.factory(**dependencies)\n        else:\n            instance = config.service_type(**dependencies)\n        \n        # Cache if singleton\n        if config.singleton:\n            self._instances[name] = instance\n        \n        return instance\n    \n    def inject(self, func: Callable) -> Callable:\n        \"\"\"Decorator for automatic dependency injection\"\"\"\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            sig = inspect.signature(func)\n            injected_kwargs = {}\n            \n            for param_name, param in sig.parameters.items():\n                if (param_name not in kwargs and \n                    param.annotation != inspect.Parameter.empty):\n                    service_name = param.annotation.__name__.lower()\n                    if service_name in self._services:\n                        injected_kwargs[param_name] = self.resolve(service_name)\n            \n            return func(*args, **kwargs, **injected_kwargs)\n        return wrapper\n\n# Example services\nclass Logger:\n    def log(self, message: str):\n        print(f\"LOG: {message}\")\n\nclass DatabaseService:\n    def __init__(self, logger: Logger):\n        self.logger = logger\n    \n    def save_data(self, data: Any):\n        self.logger.log(f\"Saving data: {data}\")\n        return f\"Data saved: {data}\"\n\nclass UserService:\n    def __init__(self, database: DatabaseService, logger: Logger):\n        self.database = database\n        self.logger = logger\n    \n    def create_user(self, username: str):\n        self.logger.log(f\"Creating user: {username}\")\n        return self.database.save_data({'username': username})\n\n# Setup container\ncontainer = DIContainer()\ncontainer.register('logger', Logger)\ncontainer.register('database', DatabaseService)\ncontainer.register('userservice', UserService)\n\n# Usage\nuser_service = container.resolve('userservice')\nresult = user_service.create_user('alice')\n\n# Decorator usage\n@container.inject\ndef process_user_data(data: dict, user_service: UserService):\n    return user_service.create_user(data['username'])\n\nresult = process_user_data({'username': 'bob'})\n```\n\n**6. Async Context Manager Pattern:**\n\n```python\nimport asyncio\nfrom contextlib import asynccontextmanager\nfrom typing import AsyncGenerator, Optional\nimport aiohttp\nimport time\n\nclass AsyncResourcePool:\n    def __init__(self, max_size: int = 10):\n        self.max_size = max_size\n        self.pool: asyncio.Queue = asyncio.Queue(maxsize=max_size)\n        self.created_count = 0\n        self.lock = asyncio.Lock()\n    \n    async def create_resource(self):\n        \"\"\"Override this method to create actual resources\"\"\"\n        return f\"Resource-{self.created_count}\"\n    \n    async def cleanup_resource(self, resource):\n        \"\"\"Override this method to cleanup resources\"\"\"\n        pass\n    \n    @asynccontextmanager\n    async def get_resource(self) -> AsyncGenerator[str, None]:\n        resource = None\n        try:\n            # Try to get existing resource from pool\n            try:\n                resource = self.pool.get_nowait()\n            except asyncio.QueueEmpty:\n                # Create new resource if pool is empty and under limit\n                async with self.lock:\n                    if self.created_count < self.max_size:\n                        resource = await self.create_resource()\n                        self.created_count += 1\n                    else:\n                        # Wait for resource to become available\n                        resource = await self.pool.get()\n            \n            yield resource\n        \n        finally:\n            # Return resource to pool\n            if resource and not self.pool.full():\n                await self.pool.put(resource)\n            elif resource:\n                await self.cleanup_resource(resource)\n\nclass HTTPClientPool(AsyncResourcePool):\n    async def create_resource(self) -> aiohttp.ClientSession:\n        return aiohttp.ClientSession()\n    \n    async def cleanup_resource(self, session: aiohttp.ClientSession):\n        await session.close()\n\n# Usage example\nasync def fetch_urls_with_pool():\n    pool = HTTPClientPool(max_size=3)\n    \n    urls = [\n        'https://httpbin.org/delay/1',\n        'https://httpbin.org/delay/2',\n        'https://httpbin.org/delay/1',\n        'https://httpbin.org/delay/3',\n    ]\n    \n    async def fetch_url(url: str):\n        async with pool.get_resource() as session:\n            async with session.get(url) as response:\n                return f\"{url}: {response.status}\"\n    \n    # Execute requests concurrently\n    results = await asyncio.gather(*[fetch_url(url) for url in urls])\n    return results\n```\n\nThese advanced patterns provide:\n\n- **Loose coupling** through dependency injection\n- **Extensibility** via registration and factory patterns\n- **Maintainability** through clear separation of concerns\n- **Testability** with mockable dependencies\n- **Performance** through resource pooling and caching\n- **Flexibility** with dynamic loading and configuration\n\nChoose patterns based on your specific needs and complexity requirements. Start simple and refactor to more complex patterns as your application grows.",
      "keywords": ["design patterns", "observer pattern", "strategy pattern", "command pattern", "factory pattern", "dependency injection", "singleton", "async patterns", "decorators", "metaclasses"],
      "difficulty": "hard"
    }
  ]
}