{
  "technology": "celery",
  "questions": [
    {
      "id": 23000,
      "tag": "celery",
      "question": "What is Celery and what problems does it solve?",
      "answer": "Celery is a distributed task queue for Python that allows you to run tasks asynchronously across multiple workers and machines.\n\n**What Celery Does:**\n- Executes tasks asynchronously in the background\n- Distributes work across multiple workers\n- Handles task scheduling and retry logic\n- Provides monitoring and management tools\n\n**Problems Celery Solves:**\n\n**1. Long-Running Tasks:**\n```python\n# Without Celery - blocks the web request\ndef send_email_view(request):\n    send_email_to_1000_users()  # Takes 30 seconds\n    return JsonResponse({'status': 'sent'})\n\n# With Celery - returns immediately\n@app.task\ndef send_email_task():\n    send_email_to_1000_users()\n\ndef send_email_view(request):\n    send_email_task.delay()  # Runs in background\n    return JsonResponse({'status': 'queued'})\n```\n\n**2. Resource-Intensive Operations:**\n- Image/video processing\n- Data analysis and reporting\n- File uploads and processing\n- Machine learning model training\n\n**3. Scheduled Tasks:**\n```python\nfrom celery.schedules import crontab\n\napp.conf.beat_schedule = {\n    'cleanup-expired-sessions': {\n        'task': 'myapp.tasks.cleanup_sessions',\n        'schedule': crontab(hour=2, minute=0),  # Daily at 2 AM\n    },\n}\n```\n\n**4. Scalability:**\n- Horizontal scaling with multiple workers\n- Load distribution across servers\n- Fault tolerance and reliability\n\n**Key Benefits:**\n- Improved user experience (no waiting)\n- Better resource utilization\n- Horizontal scalability\n- Built-in monitoring and error handling\n- Support for different message brokers",
      "keywords": ["distributed task queue", "asynchronous", "background tasks", "scalability", "message broker", "workers", "scheduling"],
      "difficulty": "easy"
    },
    {
      "id": 23001,
      "tag": "celery",
      "question": "Explain the core components of Celery architecture.",
      "answer": "Celery architecture consists of several key components that work together to provide distributed task processing.\n\n**Core Components:**\n\n**1. Celery Client (Producer):**\n- Sends tasks to the message broker\n- Part of your main application\n- Creates task messages and publishes them\n\n```python\n# Client code\nfrom myapp.tasks import process_data\n\n# Send task to queue\nresult = process_data.delay(data_id=123)\n```\n\n**2. Message Broker:**\n- Stores and routes task messages\n- Acts as intermediary between client and workers\n- Common brokers: Redis, RabbitMQ, Amazon SQS\n\n```python\n# Celery configuration\napp = Celery('myapp')\napp.conf.broker_url = 'redis://localhost:6379/0'\n```\n\n**3. Celery Workers:**\n- Execute the actual tasks\n- Pull tasks from broker queues\n- Can run on multiple machines\n- Process tasks concurrently\n\n```bash\n# Start workers\ncelery -A myapp worker --loglevel=info\ncelery -A myapp worker --concurrency=4\n```\n\n**4. Result Backend (Optional):**\n- Stores task results and status\n- Allows tracking task progress\n- Common backends: Redis, Database, Memcached\n\n```python\napp.conf.result_backend = 'redis://localhost:6379/0'\n\n# Check result\nresult = process_data.delay(123)\nif result.ready():\n    print(result.get())\n```\n\n**5. Celery Beat (Scheduler):**\n- Schedules periodic tasks\n- Sends scheduled tasks to broker\n- Single instance per deployment\n\n```bash\n# Start scheduler\ncelery -A myapp beat --loglevel=info\n```\n\n**6. Celery Monitor (Optional):**\n- Web-based monitoring interface\n- Tools like Flower, Celery Monitor\n- Real-time worker and task monitoring\n\n```bash\n# Start Flower monitoring\ncelery -A myapp flower\n```\n\n**Architecture Flow:**\n```\nClient → Message Broker → Worker → Result Backend\n   ↑                                      ↓\n   └─────── Result/Status ←───────────────┘\n```\n\n**Example Setup:**\n```python\n# celery_app.py\nfrom celery import Celery\n\napp = Celery('myapp')\napp.conf.update(\n    broker_url='redis://localhost:6379/0',\n    result_backend='redis://localhost:6379/0',\n    task_serializer='json',\n    accept_content=['json'],\n    result_serializer='json',\n    timezone='UTC',\n)\n\n# tasks.py\nfrom celery_app import app\n\n@app.task\ndef add_numbers(x, y):\n    return x + y\n```",
      "keywords": ["architecture", "message broker", "workers", "result backend", "celery beat", "producer", "consumer", "Redis", "RabbitMQ"],
      "difficulty": "medium"
    },
    {
      "id": 23002,
      "tag": "celery",
      "question": "How do you create and configure Celery tasks?",
      "answer": "Celery tasks are Python functions decorated with `@app.task` that can be executed asynchronously by workers.\n\n**Basic Task Creation:**\n```python\nfrom celery import Celery\n\napp = Celery('myapp', broker='redis://localhost:6379')\n\n@app.task\ndef add_numbers(x, y):\n    return x + y\n\n# Call the task\nresult = add_numbers.delay(4, 6)\nprint(result.get())  # 10\n```\n\n**Task Configuration Options:**\n\n**1. Task Binding:**\n```python\n@app.task(bind=True)\ndef retry_task(self, x, y):\n    try:\n        # Some operation that might fail\n        result = risky_operation(x, y)\n        return result\n    except Exception as exc:\n        # Retry the task\n        raise self.retry(exc=exc, countdown=60, max_retries=3)\n```\n\n**2. Task Name and Options:**\n```python\n@app.task(name='custom.task.name',\n          bind=True,\n          autoretry_for=(Exception,),\n          retry_kwargs={'max_retries': 3, 'countdown': 5},\n          retry_backoff=True)\ndef custom_task(self, data):\n    # Task implementation\n    return process_data(data)\n```\n\n**3. Task Routing:**\n```python\n@app.task(queue='high_priority')\ndef urgent_task():\n    return \"Processed urgently\"\n\n@app.task(queue='low_priority')\ndef background_task():\n    return \"Processed in background\"\n```\n\n**4. Class-Based Tasks:**\n```python\nfrom celery import Task\n\nclass DatabaseTask(Task):\n    def on_failure(self, exc, task_id, args, kwargs, einfo):\n        print(f'Task {task_id} failed: {exc}')\n    \n    def on_success(self, retval, task_id, args, kwargs):\n        print(f'Task {task_id} succeeded: {retval}')\n\n@app.task(base=DatabaseTask)\ndef database_operation(query):\n    # Database operation\n    return execute_query(query)\n```\n\n**Task Execution Methods:**\n\n**1. Asynchronous Execution:**\n```python\n# .delay() - simple async call\nresult = add_numbers.delay(4, 6)\n\n# .apply_async() - full control\nresult = add_numbers.apply_async(\n    args=[4, 6],\n    countdown=10,  # Execute after 10 seconds\n    expires=120,   # Expire after 2 minutes\n    retry=True,\n    queue='math_queue'\n)\n```\n\n**2. Synchronous Execution (for testing):**\n```python\n# Execute immediately (synchronous)\nresult = add_numbers.apply(args=[4, 6])\nprint(result.result)  # 10\n```\n\n**Task Configuration in Settings:**\n```python\n# celeryconfig.py\nbroker_url = 'redis://localhost:6379/0'\nresult_backend = 'redis://localhost:6379/0'\n\n# Task routing\ntask_routes = {\n    'myapp.tasks.urgent_task': {'queue': 'urgent'},\n    'myapp.tasks.slow_task': {'queue': 'slow'},\n}\n\n# Default task settings\ntask_serializer = 'json'\nresult_serializer = 'json'\naccept_content = ['json']\ntimezone = 'UTC'\nenable_utc = True\n\n# Task execution settings\ntask_always_eager = False  # Set True for testing\ntask_eager_propagates = True\ntask_ignore_result = False\n\n# Load configuration\napp.config_from_object('celeryconfig')\n```\n\n**Advanced Task Features:**\n\n**1. Task Callbacks:**\n```python\n@app.task\ndef success_callback(result):\n    print(f\"Task completed with result: {result}\")\n\n@app.task\ndef failure_callback(task_id, error, traceback):\n    print(f\"Task {task_id} failed: {error}\")\n\n# Use callbacks\nadd_numbers.apply_async(\n    args=[4, 6],\n    link=success_callback.s(),\n    link_error=failure_callback.s()\n)\n```\n\n**2. Task Groups and Chains:**\n```python\nfrom celery import group, chain\n\n# Group - parallel execution\njob = group(add_numbers.s(2, 2), add_numbers.s(4, 4))\nresult = job.apply_async()\n\n# Chain - sequential execution\njob = chain(add_numbers.s(2, 2), add_numbers.s(4))\nresult = job.apply_async()\n```",
      "keywords": ["task creation", "decorators", "task configuration", "retry", "routing", "class-based tasks", "async execution", "task options"],
      "difficulty": "medium"
    },
    {
      "id": 23003,
      "tag": "celery",
      "question": "What are the differences between Redis and RabbitMQ as Celery message brokers?",
      "answer": "Redis and RabbitMQ are the two most popular message brokers for Celery, each with distinct characteristics and use cases.\n\n**Redis as Message Broker:**\n\n**Pros:**\n- **Simple setup** - Easy to install and configure\n- **High performance** - Very fast for simple use cases\n- **Low latency** - In-memory storage provides quick access\n- **Dual purpose** - Can serve as both broker and result backend\n- **Good for development** - Quick to get started\n\n**Cons:**\n- **No guaranteed delivery** - Messages can be lost if Redis crashes\n- **Limited routing** - Basic queue functionality\n- **Memory bound** - All data must fit in memory\n- **Single point of failure** - Unless using Redis cluster\n\n```python\n# Redis configuration\napp = Celery('myapp')\napp.conf.update(\n    broker_url='redis://localhost:6379/0',\n    result_backend='redis://localhost:6379/0',\n    broker_transport_options={\n        'visibility_timeout': 3600,\n        'fanout_prefix': True,\n        'fanout_patterns': True\n    }\n)\n```\n\n**RabbitMQ as Message Broker:**\n\n**Pros:**\n- **Guaranteed delivery** - Messages are persisted to disk\n- **Advanced routing** - Complex routing patterns supported\n- **High availability** - Built-in clustering and mirroring\n- **Flow control** - Prevents memory exhaustion\n- **Standards compliant** - AMQP protocol support\n- **Enterprise features** - Dead letter queues, TTL, priorities\n\n**Cons:**\n- **Complex setup** - More configuration required\n- **Higher resource usage** - More memory and CPU overhead\n- **Learning curve** - AMQP concepts can be complex\n- **Slower for simple cases** - Overhead for guaranteed delivery\n\n```python\n# RabbitMQ configuration\napp = Celery('myapp')\napp.conf.update(\n    broker_url='pyamqp://guest@localhost//',\n    result_backend='rpc://',\n    task_serializer='json',\n    accept_content=['json'],\n    result_serializer='json',\n    broker_connection_retry_on_startup=True\n)\n```\n\n**Feature Comparison:**\n\n| Feature | Redis | RabbitMQ |\n|---------|-------|----------|\n| **Setup Complexity** | Simple | Moderate |\n| **Performance** | Very High | High |\n| **Message Persistence** | Optional | Yes |\n| **Guaranteed Delivery** | No | Yes |\n| **Routing Features** | Basic | Advanced |\n| **Clustering** | Redis Cluster | Native |\n| **Memory Usage** | High | Moderate |\n| **Dead Letter Queues** | Limited | Full Support |\n| **Message Priorities** | Limited | Full Support |\n\n**Advanced RabbitMQ Features:**\n\n**1. Queue Arguments:**\n```python\napp.conf.task_routes = {\n    'myapp.tasks.critical_task': {\n        'queue': 'critical',\n        'routing_key': 'critical',\n        'queue_arguments': {\n            'x-max-priority': 10,\n            'x-message-ttl': 60000\n        }\n    }\n}\n```\n\n**2. Dead Letter Exchange:**\n```python\napp.conf.task_annotations = {\n    'myapp.tasks.risky_task': {\n        'queue_arguments': {\n            'x-dead-letter-exchange': 'dlx',\n            'x-dead-letter-routing-key': 'failed'\n        }\n    }\n}\n```\n\n**Redis Advanced Configuration:**\n\n**1. Redis Sentinel (High Availability):**\n```python\napp.conf.broker_url = 'sentinel://localhost:26379'\napp.conf.broker_transport_options = {\n    'master_name':'mymaster',\n    'sentinel_kwargs': {\n        'socket_timeout': 0.1,\n        'socket_connect_timeout': 0.1,\n        'socket_keepalive': True,\n        'socket_keepalive_options': {},\n        'connection_pool_class': 'redis.sentinel.SentinelConnectionPool',\n        'connection_pool_class_kwargs': {\n            'socket_keepalive': True,\n            'socket_keepalive_options': {},\n        }\n    }\n}\n```\n\n**2. Redis Cluster:**\n```python\nfrom rediscluster import RedisCluster\n\napp.conf.broker_url = 'redis-cluster://localhost:7000/0'\napp.conf.broker_transport_options = {\n    'startup_nodes': [\n        {'host': '127.0.0.1', 'port': 7000},\n        {'host': '127.0.0.1', 'port': 7001},\n        {'host': '127.0.0.1', 'port': 7002},\n    ],\n    'skip_full_coverage_check': True\n}\n```\n\n**When to Choose Each:**\n\n**Choose Redis when:**\n- Simple task queuing needs\n- High performance is critical\n- Development/prototyping\n- Tasks are not mission-critical\n- You need both broker and result backend\n- Team is familiar with Redis\n\n**Choose RabbitMQ when:**\n- Message delivery guarantees are critical\n- Complex routing requirements\n- Enterprise/production environments\n- High availability requirements\n- Need advanced features (DLQ, TTL, priorities)\n- Compliance requirements for message persistence\n\n**Hybrid Approach:**\n```python\n# Use RabbitMQ for broker, Redis for results\napp.conf.update(\n    broker_url='pyamqp://guest@localhost//',\n    result_backend='redis://localhost:6379/0'\n)\n```",
      "keywords": ["Redis", "RabbitMQ", "message broker", "AMQP", "guaranteed delivery", "persistence", "clustering", "routing", "performance"],
      "difficulty": "medium"
    },
    {
      "id": 23004,
      "tag": "celery",
      "question": "How do you handle task failures, retries, and error handling in Celery?",
      "answer": "Celery provides comprehensive mechanisms for handling task failures, implementing retry logic, and managing errors gracefully.\n\n**Basic Retry Mechanism:**\n\n```python\nfrom celery import Celery\nfrom celery.exceptions import Retry\nimport requests\n\napp = Celery('myapp')\n\n@app.task(bind=True, autoretry_for=(requests.RequestException,), retry_kwargs={'max_retries': 3, 'countdown': 5})\ndef fetch_data(self, url):\n    try:\n        response = requests.get(url, timeout=10)\n        response.raise_for_status()\n        return response.json()\n    except requests.RequestException as exc:\n        # This will automatically retry due to autoretry_for\n        raise\n```\n\n**Manual Retry Control:**\n\n```python\n@app.task(bind=True)\ndef unreliable_task(self, data):\n    try:\n        # Attempt the operation\n        result = risky_operation(data)\n        return result\n    except TemporaryError as exc:\n        # Retry with exponential backoff\n        raise self.retry(\n            exc=exc,\n            countdown=2 ** self.request.retries,  # Exponential backoff\n            max_retries=5\n        )\n    except PermanentError as exc:\n        # Don't retry permanent errors\n        logger.error(f\"Permanent error in task: {exc}\")\n        raise\n```\n\n**Advanced Retry Strategies:**\n\n**1. Custom Retry Logic:**\n```python\n@app.task(bind=True)\ndef smart_retry_task(self, data):\n    try:\n        return process_data(data)\n    except (ConnectionError, TimeoutError) as exc:\n        # Different retry strategies based on error type\n        if isinstance(exc, ConnectionError):\n            countdown = 60  # Wait longer for connection issues\n        else:\n            countdown = 10  # Shorter wait for timeouts\n        \n        if self.request.retries < 3:\n            raise self.retry(exc=exc, countdown=countdown)\n        else:\n            # Final retry failed, handle gracefully\n            send_failure_notification(data, exc)\n            raise\n```\n\n**2. Conditional Retries:**\n```python\n@app.task(bind=True)\ndef api_call_task(self, endpoint, data):\n    try:\n        response = requests.post(endpoint, json=data)\n        if response.status_code == 429:  # Rate limited\n            retry_after = int(response.headers.get('Retry-After', 60))\n            raise self.retry(countdown=retry_after, max_retries=10)\n        elif response.status_code >= 500:  # Server error\n            raise self.retry(countdown=30, max_retries=3)\n        elif response.status_code >= 400:  # Client error, don't retry\n            raise ValueError(f\"Client error: {response.status_code}\")\n        return response.json()\n    except requests.RequestException as exc:\n        raise self.retry(exc=exc, countdown=10, max_retries=3)\n```\n\n**Error Handling Strategies:**\n\n**1. Task-Level Error Handling:**\n```python\n@app.task(bind=True)\ndef robust_task(self, data):\n    try:\n        # Main task logic\n        result = process_data(data)\n        \n        # Validate result\n        if not validate_result(result):\n            raise ValueError(\"Invalid result produced\")\n        \n        return result\n    \n    except ValidationError as exc:\n        # Handle validation errors without retry\n        logger.warning(f\"Validation failed for task {self.request.id}: {exc}\")\n        return {'error': 'validation_failed', 'details': str(exc)}\n    \n    except Exception as exc:\n        # Log unexpected errors\n        logger.error(f\"Unexpected error in task {self.request.id}: {exc}\", exc_info=True)\n        \n        # Decide whether to retry or fail\n        if self.request.retries < 2:\n            raise self.retry(exc=exc, countdown=30)\n        else:\n            # Send to dead letter queue or error handler\n            handle_final_failure(self.request.id, data, exc)\n            raise\n```\n\n**2. Custom Task Base Classes:**\n```python\nfrom celery import Task\nfrom celery.signals import task_failure, task_retry, task_success\n\nclass BaseTask(Task):\n    def on_failure(self, exc, task_id, args, kwargs, einfo):\n        \"\"\"Called when task fails after all retries\"\"\"\n        logger.error(f\"Task {task_id} failed permanently: {exc}\")\n        send_error_notification(task_id, exc, args, kwargs)\n    \n    def on_retry(self, exc, task_id, args, kwargs, einfo):\n        \"\"\"Called when task is retried\"\"\"\n        logger.warning(f\"Task {task_id} retry #{self.request.retries + 1}: {exc}\")\n    \n    def on_success(self, retval, task_id, args, kwargs):\n        \"\"\"Called when task succeeds\"\"\"\n        logger.info(f\"Task {task_id} completed successfully\")\n\n@app.task(base=BaseTask, bind=True)\ndef monitored_task(self, data):\n    return process_data(data)\n```\n\n**Global Error Handling:**\n\n**1. Signal Handlers:**\n```python\nfrom celery.signals import task_failure, task_retry\n\n@task_failure.connect\ndef task_failure_handler(sender=None, task_id=None, exception=None, traceback=None, einfo=None, **kwargs):\n    \"\"\"Handle all task failures\"\"\"\n    logger.error(f\"Task {task_id} failed: {exception}\")\n    \n    # Send to monitoring system\n    monitoring.record_task_failure(task_id, str(exception))\n    \n    # Send notification for critical tasks\n    if sender.name in CRITICAL_TASKS:\n        send_alert(f\"Critical task {task_id} failed: {exception}\")\n\n@task_retry.connect\ndef task_retry_handler(sender=None, task_id=None, reason=None, einfo=None, **kwargs):\n    \"\"\"Handle all task retries\"\"\"\n    logger.warning(f\"Task {task_id} retrying: {reason}\")\n    monitoring.record_task_retry(task_id, str(reason))\n```\n\n**Dead Letter Queue Implementation:**\n\n```python\n@app.task\ndef dead_letter_handler(task_name, task_id, args, kwargs, exception_info):\n    \"\"\"Handle tasks that failed after all retries\"\"\"\n    logger.error(f\"Dead letter: {task_name}({args}, {kwargs}) failed permanently\")\n    \n    # Store in database for manual review\n    FailedTask.objects.create(\n        task_name=task_name,\n        task_id=task_id,\n        args=args,\n        kwargs=kwargs,\n        exception_info=exception_info,\n        created_at=timezone.now()\n    )\n    \n    # Send notification\n    send_admin_notification(\n        subject=f\"Task {task_name} failed permanently\",\n        body=f\"Task ID: {task_id}\\nException: {exception_info}\"\n    )\n\n# Custom task that sends failures to dead letter queue\n@app.task(bind=True, base=BaseTask)\ndef resilient_task(self, data):\n    try:\n        return process_data(data)\n    except Exception as exc:\n        if self.request.retries >= self.max_retries:\n            # Send to dead letter queue\n            dead_letter_handler.delay(\n                task_name=self.name,\n                task_id=self.request.id,\n                args=self.request.args,\n                kwargs=self.request.kwargs,\n                exception_info=str(exc)\n            )\n        raise\n```\n\n**Configuration for Error Handling:**\n\n```python\n# celeryconfig.py\n\n# Retry settings\ntask_acks_late = True  # Acknowledge task only after completion\ntask_reject_on_worker_lost = True  # Reject task if worker dies\n\n# Error handling\ntask_annotations = {\n    '*': {\n        'autoretry_for': (Exception,),\n        'retry_kwargs': {'max_retries': 3, 'countdown': 5},\n        'retry_backoff': True,\n        'retry_backoff_max': 700,\n        'retry_jitter': False,\n    }\n}\n\n# Task routing for failed tasks\ntask_routes = {\n    'myapp.tasks.dead_letter_handler': {'queue': 'dead_letter'},\n}\n```\n\n**Best Practices:**\n1. **Distinguish error types** - Don't retry permanent errors\n2. **Use exponential backoff** - Avoid overwhelming failing services\n3. **Set reasonable retry limits** - Prevent infinite retry loops\n4. **Log appropriately** - Include context for debugging\n5. **Monitor failures** - Set up alerts for critical tasks\n6. **Handle dead letters** - Plan for tasks that can't be completed\n7. **Test error scenarios** - Verify retry logic works correctly\n8. **Use circuit breakers** - Prevent cascading failures",
      "keywords": ["error handling", "retry logic", "task failures", "exponential backoff", "dead letter queue", "signal handlers", "permanent errors", "temporary errors"],
      "difficulty": "hard"
    },
    {
      "id": 23005,
      "tag": "celery",
      "question": "How do you implement task scheduling and periodic tasks with Celery Beat?",
      "answer": "Celery Beat is Celery's task scheduler that enables periodic task execution, similar to cron but integrated with Celery's task system.\n\n**Basic Celery Beat Setup:**\n\n```python\n# celery_app.py\nfrom celery import Celery\nfrom celery.schedules import crontab\n\napp = Celery('myapp')\n\n# Periodic task configuration\napp.conf.beat_schedule = {\n    # Run every 30 seconds\n    'check-system-health': {\n        'task': 'myapp.tasks.health_check',\n        'schedule': 30.0,\n    },\n    \n    # Run daily at midnight\n    'daily-cleanup': {\n        'task': 'myapp.tasks.cleanup_old_data',\n        'schedule': crontab(hour=0, minute=0),\n    },\n    \n    # Run every Monday at 7:30 AM\n    'weekly-report': {\n        'task': 'myapp.tasks.generate_weekly_report',\n        'schedule': crontab(hour=7, minute=30, day_of_week=1),\n    },\n    \n    # Run with arguments\n    'process-analytics': {\n        'task': 'myapp.tasks.process_analytics',\n        'schedule': crontab(minute='*/15'),  # Every 15 minutes\n        'args': ('daily',),\n        'kwargs': {'detailed': True}\n    },\n}\n\napp.conf.timezone = 'UTC'\n```\n\n**Schedule Types:**\n\n**1. Interval Schedules:**\n```python\nfrom celery.schedules import crontab\nfrom datetime import timedelta\n\napp.conf.beat_schedule = {\n    # Every 30 seconds\n    'frequent-task': {\n        'task': 'myapp.tasks.frequent_check',\n        'schedule': 30.0,\n    },\n    \n    # Every 5 minutes using timedelta\n    'moderate-task': {\n        'task': 'myapp.tasks.moderate_check',\n        'schedule': timedelta(minutes=5),\n    },\n    \n    # Every hour\n    'hourly-task': {\n        'task': 'myapp.tasks.hourly_cleanup',\n        'schedule': timedelta(hours=1),\n    },\n}\n```\n\n**2. Cron-like Schedules:**\n```python\napp.conf.beat_schedule = {\n    # Every day at 2:30 AM\n    'nightly-backup': {\n        'task': 'myapp.tasks.backup_database',\n        'schedule': crontab(hour=2, minute=30),\n    },\n    \n    # Every weekday at 9 AM\n    'weekday-report': {\n        'task': 'myapp.tasks.daily_report',\n        'schedule': crontab(hour=9, minute=0, day_of_week='1-5'),\n    },\n    \n    # Last day of every month at 11:59 PM\n    'monthly-billing': {\n        'task': 'myapp.tasks.process_monthly_billing',\n        'schedule': crontab(hour=23, minute=59, day_of_month='28-31'),\n    },\n    \n    # Every 10 minutes during business hours\n    'business-hours-sync': {\n        'task': 'myapp.tasks.sync_data',\n        'schedule': crontab(minute='*/10', hour='9-17', day_of_week='1-5'),\n    },\n}\n```\n\n**Dynamic Scheduling with Database:**\n\n**1. Using Django-Celery-Beat:**\n```python\n# models.py (Django)\nfrom django_celery_beat.models import PeriodicTask, CrontabSchedule\nimport json\n\n# Create a cron schedule\nschedule, created = CrontabSchedule.objects.get_or_create(\n    minute='0',\n    hour='4',\n    day_of_week='*',\n    day_of_month='*',\n    month_of_year='*',\n)\n\n# Create periodic task\nPeriodicTask.objects.create(\n    crontab=schedule,\n    name='Daily Data Processing',\n    task='myapp.tasks.process_daily_data',\n    args=json.dumps(['arg1', 'arg2']),\n    kwargs=json.dumps({'key': 'value'}),\n    enabled=True,\n)\n```\n\n**2. Programmatic Schedule Management:**\n```python\nfrom django_celery_beat.models import PeriodicTask, IntervalSchedule\n\ndef create_user_task(user_id, interval_minutes):\n    \"\"\"Create a periodic task for a specific user\"\"\"\n    schedule, created = IntervalSchedule.objects.get_or_create(\n        every=interval_minutes,\n        period=IntervalSchedule.MINUTES,\n    )\n    \n    task = PeriodicTask.objects.create(\n        interval=schedule,\n        name=f'User Task {user_id}',\n        task='myapp.tasks.user_specific_task',\n        kwargs=json.dumps({'user_id': user_id}),\n        enabled=True,\n    )\n    return task\n\ndef disable_user_task(user_id):\n    \"\"\"Disable periodic task for a user\"\"\"\n    PeriodicTask.objects.filter(\n        name=f'User Task {user_id}'\n    ).update(enabled=False)\n```\n\n**Advanced Scheduling Patterns:**\n\n**1. Conditional Scheduling:**\n```python\n@app.task\ndef smart_cleanup():\n    \"\"\"Only run cleanup if needed\"\"\"\n    if should_run_cleanup():\n        cleanup_old_files()\n        return \"Cleanup completed\"\n    else:\n        return \"Cleanup skipped - not needed\"\n\n# Always run the check, but let task decide if work is needed\napp.conf.beat_schedule = {\n    'smart-cleanup': {\n        'task': 'myapp.tasks.smart_cleanup',\n        'schedule': crontab(minute=0),  # Every hour\n    },\n}\n```\n\n**2. Chain Scheduling:**\n```python\nfrom celery import chain\n\n@app.task\ndef start_daily_pipeline():\n    \"\"\"Start a chain of tasks\"\"\"\n    pipeline = chain(\n        extract_data.s(),\n        transform_data.s(),\n        load_data.s(),\n        send_completion_report.s()\n    )\n    return pipeline.apply_async()\n\napp.conf.beat_schedule = {\n    'daily-etl-pipeline': {\n        'task': 'myapp.tasks.start_daily_pipeline',\n        'schedule': crontab(hour=3, minute=0),\n    },\n}\n```\n\n**Running Celery Beat:**\n\n```bash\n# Start beat scheduler\ncelery -A myapp beat --loglevel=info\n\n# Start beat with custom schedule file\ncelery -A myapp beat --schedule=/path/to/celerybeat-schedule\n\n# Start beat with database scheduler (django-celery-beat)\ncelery -A myapp beat -l info --scheduler django_celery_beat.schedulers:DatabaseScheduler\n```\n\n**Beat Configuration:**\n\n```python\n# celeryconfig.py\n\n# Beat settings\nbeat_schedule_filename = 'celerybeat-schedule'\nbeat_scheduler = 'django_celery_beat.schedulers:DatabaseScheduler'\n\n# Timezone configuration\ntimezone = 'America/New_York'\nenable_utc = True\n\n# Beat sync settings\nbeat_sync_every = 1  # Sync every second\nbeat_max_loop_interval = 300  # Maximum time between beats\n```\n\n**Monitoring Scheduled Tasks:**\n\n```python\n# Task to monitor beat status\n@app.task\ndef monitor_scheduled_tasks():\n    \"\"\"Monitor the health of scheduled tasks\"\"\"\n    from django_celery_beat.models import PeriodicTask\n    \n    tasks = PeriodicTask.objects.filter(enabled=True)\n    failed_tasks = []\n    \n    for task in tasks:\n        if task.last_run_at:\n            time_since_run = timezone.now() - task.last_run_at\n            expected_interval = task.interval.every * 60  # Convert to seconds\n            \n            if time_since_run.total_seconds() > expected_interval * 2:\n                failed_tasks.append(task.name)\n    \n    if failed_tasks:\n        send_alert(f\"Scheduled tasks may be failing: {failed_tasks}\")\n    \n    return f\"Monitored {len(tasks)} tasks, {len(failed_tasks)} potential issues\"\n```\n\n**Best Practices:**\n\n1. **Single Beat Instance** - Only run one beat scheduler per deployment\n2. **Database Scheduling** - Use django-celery-beat for dynamic scheduling\n3. **Timezone Awareness** - Always configure timezone properly\n4. **Error Handling** - Implement proper error handling in scheduled tasks\n5. **Monitoring** - Monitor beat scheduler health and task execution\n6. **Resource Management** - Consider task frequency and resource usage\n7. **Graceful Shutdown** - Handle beat scheduler shutdown properly\n8. **Overlap Prevention** - Ensure tasks don't overlap if they run longer than interval\n\n**Common Patterns:**\n\n```python\n# Prevent task overlap\n@app.task(bind=True)\ndef exclusive_task(self):\n    lock_id = f\"exclusive_task_lock\"\n    with app.backend.get_lock(lock_id, timeout=300):\n        # Task logic here\n        process_data()\n        return \"Task completed\"\n\n# Health check pattern\n@app.task\ndef system_health_check():\n    checks = {\n        'database': check_database_connection(),\n        'redis': check_redis_connection(),\n        'disk_space': check_disk_space(),\n    }\n    \n    failed_checks = [k for k, v in checks.items() if not v]\n    if failed_checks:\n        send_alert(f\"Health check failures: {failed_checks}\")\n    \n    return checks\n```",
      "keywords": ["celery beat", "periodic tasks", "crontab", "scheduling", "django-celery-beat", "intervals", "timezone", "monitoring", "dynamic scheduling"],
      "difficulty": "medium"
    },
    {
      "id": 23006,
      "tag": "celery",
      "question": "How do you monitor and debug Celery applications?",
      "answer": "Monitoring and debugging Celery applications requires a combination of built-in tools, third-party solutions, and proper logging strategies.\n\n**Built-in Monitoring Tools:**\n\n**1. Celery Events:**\n```bash\n# Enable events monitoring\ncelery -A myapp events --loglevel=info\n\n# Monitor events in real-time\ncelery -A myapp monitor\n```\n\n**2. Worker Status and Stats:**\n```bash\n# Check worker status\ncelery -A myapp status\n\n# Get detailed worker statistics\ncelery -A myapp inspect stats\n\n# List active tasks\ncelery -A myapp inspect active\n\n# List scheduled tasks\ncelery -A myapp inspect scheduled\n\n# List reserved tasks\ncelery -A myapp inspect reserved\n```\n\n**3. Programmatic Monitoring:**\n```python\nfrom celery import Celery\nfrom celery.app.control import Inspect\n\napp = Celery('myapp')\ni = Inspect(app=app)\n\n# Get worker statistics\nstats = i.stats()\nprint(f\"Active workers: {len(stats)}\")\n\nfor worker_name, worker_stats in stats.items():\n    print(f\"Worker: {worker_name}\")\n    print(f\"  Total tasks: {worker_stats.get('total', 'N/A')}\")\n    print(f\"  Pool processes: {worker_stats.get('pool', {}).get('processes', 'N/A')}\")\n\n# Get active tasks\nactive_tasks = i.active()\nfor worker, tasks in active_tasks.items():\n    print(f\"Worker {worker} has {len(tasks)} active tasks\")\n```\n\n**Flower - Web-based Monitoring:**\n\n```bash\n# Install Flower\npip install flower\n\n# Start Flower\ncelery -A myapp flower\n\n# Access web interface at http://localhost:5555\n```\n\n**Flower Configuration:**\n```python\n# flowerconfig.py\nport = 5555\naddress = '0.0.0.0'\nmax_tasks = 10000\ndb = 'flower.db'\n\n# Authentication\nbasic_auth = ['admin:password']\n\n# Auto-refresh interval\nauto_refresh = True\nrefresh_interval = 1000  # milliseconds\n```\n\n**Comprehensive Logging Setup:**\n\n```python\n# celery_app.py\nimport logging\nfrom celery import Celery\nfrom celery.signals import setup_logging, task_prerun, task_postrun, task_failure\n\napp = Celery('myapp')\n\n@setup_logging.connect\ndef config_loggers(*args, **kwargs):\n    from logging.config import dictConfig\n    \n    dictConfig({\n        'version': 1,\n        'disable_existing_loggers': False,\n        'formatters': {\n            'detailed': {\n                'format': '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n            },\n            'json': {\n                'format': '%(asctime)s %(name)s %(levelname)s %(message)s',\n                'class': 'pythonjsonlogger.jsonlogger.JsonFormatter'\n            }\n        },\n        'handlers': {\n            'console': {\n                'class': 'logging.StreamHandler',\n                'level': 'INFO',\n                'formatter': 'detailed',\n            },\n            'file': {\n                'class': 'logging.handlers.RotatingFileHandler',\n                'filename': 'celery.log',\n                'maxBytes': 10485760,  # 10MB\n                'backupCount': 5,\n                'level': 'DEBUG',\n                'formatter': 'json',\n            }\n        },\n        'loggers': {\n            'celery': {\n                'level': 'INFO',\n                'handlers': ['console', 'file'],\n                'propagate': False\n            },\n            'myapp': {\n                'level': 'DEBUG',\n                'handlers': ['console', 'file'],\n                'propagate': False\n            }\n        }\n    })\n\n# Task execution logging\n@task_prerun.connect\ndef task_prerun_handler(sender=None, task_id=None, task=None, args=None, kwargs=None, **kwds):\n    logger = logging.getLogger('myapp.tasks')\n    logger.info(f\"Task {task.name}[{task_id}] starting\", extra={\n        'task_id': task_id,\n        'task_name': task.name,\n        'args': args,\n        'kwargs': kwargs\n    })\n\n@task_postrun.connect\ndef task_postrun_handler(sender=None, task_id=None, task=None, args=None, kwargs=None, retval=None, state=None, **kwds):\n    logger = logging.getLogger('myapp.tasks')\n    logger.info(f\"Task {task.name}[{task_id}] completed with state {state}\", extra={\n        'task_id': task_id,\n        'task_name': task.name,\n        'state': state,\n        'retval': str(retval)[:200]  # Truncate long return values\n    })\n\n@task_failure.connect\ndef task_failure_handler(sender=None, task_id=None, exception=None, einfo=None, **kwds):\n    logger = logging.getLogger('myapp.tasks')\n    logger.error(f\"Task {sender.name}[{task_id}] failed: {exception}\", extra={\n        'task_id': task_id,\n        'task_name': sender.name,\n        'exception': str(exception),\n        'traceback': str(einfo)\n    })\n```\n\n**Custom Monitoring Tasks:**\n\n```python\n@app.task\ndef health_check():\n    \"\"\"Comprehensive health check task\"\"\"\n    import psutil\n    import redis\n    from celery.app.control import Inspect\n    \n    health_data = {\n        'timestamp': datetime.utcnow().isoformat(),\n        'status': 'healthy'\n    }\n    \n    try:\n        # Check Redis connection\n        r = redis.Redis.from_url(app.conf.broker_url)\n        r.ping()\n        health_data['redis'] = 'connected'\n    except Exception as e:\n        health_data['redis'] = f'error: {str(e)}'\n        health_data['status'] = 'unhealthy'\n    \n    # Check worker status\n    i = Inspect(app=app)\n    stats = i.stats()\n    health_data['workers'] = {\n        'count': len(stats) if stats else 0,\n        'details': stats\n    }\n    \n    # System metrics\n    health_data['system'] = {\n        'cpu_percent': psutil.cpu_percent(),\n        'memory_percent': psutil.virtual_memory().percent,\n        'disk_percent': psutil.disk_usage('/').percent\n    }\n    \n    # Queue lengths\n    try:\n        r = redis.Redis.from_url(app.conf.broker_url)\n        health_data['queues'] = {\n            'default': r.llen('celery'),\n            'high_priority': r.llen('high_priority'),\n            'low_priority': r.llen('low_priority')\n        }\n    except Exception as e:\n        health_data['queues'] = f'error: {str(e)}'\n    \n    # Send alert if unhealthy\n    if health_data['status'] == 'unhealthy':\n        send_alert('Celery system unhealthy', health_data)\n    \n    return health_data\n\n@app.task\ndef task_performance_monitor():\n    \"\"\"Monitor task performance metrics\"\"\"\n    from django_celery_results.models import TaskResult\n    from datetime import timedelta\n    \n    now = timezone.now()\n    last_hour = now - timedelta(hours=1)\n    \n    # Get task statistics from last hour\n    recent_tasks = TaskResult.objects.filter(\n        date_created__gte=last_hour\n    )\n    \n    metrics = {\n        'total_tasks': recent_tasks.count(),\n        'successful_tasks': recent_tasks.filter(status='SUCCESS').count(),\n        'failed_tasks': recent_tasks.filter(status='FAILURE').count(),\n        'pending_tasks': recent_tasks.filter(status='PENDING').count(),\n    }\n    \n    # Calculate average execution time\n    completed_tasks = recent_tasks.filter(\n        status__in=['SUCCESS', 'FAILURE'],\n        date_done__isnull=False\n    )\n    \n    if completed_tasks.exists():\n        avg_duration = completed_tasks.aggregate(\n            avg_duration=models.Avg(\n                models.F('date_done') - models.F('date_created')\n            )\n        )['avg_duration']\n        metrics['avg_execution_time'] = avg_duration.total_seconds()\n    \n    # Identify slow tasks\n    slow_tasks = completed_tasks.annotate(\n        duration=models.F('date_done') - models.F('date_created')\n    ).filter(\n        duration__gt=timedelta(minutes=5)\n    ).values('task_name', 'duration')[:10]\n    \n    metrics['slow_tasks'] = list(slow_tasks)\n    \n    # Alert on high failure rate\n    if metrics['total_tasks'] > 0:\n        failure_rate = metrics['failed_tasks'] / metrics['total_tasks']\n        if failure_rate > 0.1:  # More than 10% failure rate\n            send_alert(f'High task failure rate: {failure_rate:.2%}', metrics)\n    \n    return metrics\n```\n\n**Integration with External Monitoring:**\n\n**1. Prometheus Metrics:**\n```python\nfrom prometheus_client import Counter, Histogram, Gauge, start_http_server\nfrom celery.signals import task_prerun, task_postrun, task_failure\n\n# Metrics\ntask_counter = Counter('celery_tasks_total', 'Total number of tasks', ['task_name', 'status'])\ntask_duration = Histogram('celery_task_duration_seconds', 'Task duration', ['task_name'])\nactive_tasks = Gauge('celery_active_tasks', 'Number of active tasks', ['worker'])\n\n@task_prerun.connect\ndef increment_task_counter(sender=None, task_id=None, task=None, **kwargs):\n    task_counter.labels(task_name=task.name, status='started').inc()\n\n@task_postrun.connect\ndef record_task_duration(sender=None, task_id=None, task=None, retval=None, state=None, **kwargs):\n    # This would need to be implemented with timing logic\n    task_counter.labels(task_name=task.name, status=state.lower()).inc()\n\n# Start Prometheus metrics server\nstart_http_server(8000)\n```\n\n**2. Sentry Integration:**\n```python\nimport sentry_sdk\nfrom sentry_sdk.integrations.celery import CeleryIntegration\n\nsentry_sdk.init(\n    dsn=\"your-sentry-dsn\",\n    integrations=[\n        CeleryIntegration(monitor_beat_tasks=True),\n    ],\n    traces_sample_rate=0.1,\n)\n```\n\n**Debugging Techniques:**\n\n**1. Task Tracing:**\n```python\n@app.task(bind=True)\ndef traceable_task(self, data):\n    logger = logging.getLogger(__name__)\n    \n    # Log task start with context\n    logger.info(f\"Starting task {self.request.id}\", extra={\n        'task_id': self.request.id,\n        'correlation_id': self.request.correlation_id,\n        'data_size': len(str(data))\n    })\n    \n    try:\n        # Add checkpoints throughout task\n        logger.debug(\"Checkpoint 1: Data validation\")\n        validated_data = validate_data(data)\n        \n        logger.debug(\"Checkpoint 2: Processing data\")\n        result = process_data(validated_data)\n        \n        logger.debug(\"Checkpoint 3: Saving result\")\n        save_result(result)\n        \n        logger.info(f\"Task {self.request.id} completed successfully\")\n        return result\n        \n    except Exception as exc:\n        logger.error(f\"Task {self.request.id} failed at processing\", exc_info=True)\n        raise\n```\n\n**2. Development vs Production Configuration:**\n```python\n# Development settings\nif DEBUG:\n    app.conf.update(\n        task_always_eager=True,  # Execute tasks synchronously\n        task_eager_propagates=True,  # Propagate exceptions immediately\n        worker_log_level='DEBUG',\n        worker_hijack_root_logger=False\n    )\nelse:\n    # Production settings\n    app.conf.update(\n        task_always_eager=False,\n        worker_log_level='INFO',\n        task_track_started=True,\n        result_expires=3600,\n    )\n```\n\n**Best Practices:**\n1. **Structured Logging** - Use JSON format for better parsing\n2. **Unique Task IDs** - Include correlation IDs for tracing\n3. **Health Checks** - Regular automated health monitoring\n4. **Alerting** - Set up alerts for critical failures\n5. **Metrics Collection** - Track key performance indicators\n6. **Log Rotation** - Prevent log files from growing too large\n7. **Error Aggregation** - Use tools like Sentry for error tracking\n8. **Performance Monitoring** - Track task execution times and queue lengths",
      "keywords": ["monitoring", "debugging", "Flower", "logging", "health checks", "Prometheus", "Sentry", "task tracing", "performance monitoring", "alerts"],
      "difficulty": "medium"
    },
    {
      "id": 23007,
      "tag": "celery",
      "question": "How do you scale Celery applications and optimize performance?",
      "answer": "Scaling Celery applications involves optimizing worker configuration, implementing proper queue management, and tuning system resources for maximum throughput.\n\n**Worker Scaling Strategies:**\n\n**1. Horizontal Scaling (Multiple Workers):**\n```bash\n# Start multiple worker processes\ncelery -A myapp worker --concurrency=4 --loglevel=info\n\n# Start workers on different machines\n# Machine 1:\ncelery -A myapp worker --hostname=worker1@%h --concurrency=8\n\n# Machine 2:\ncelery -A myapp worker --hostname=worker2@%h --concurrency=8\n\n# Start specialized workers for different queues\ncelery -A myapp worker --queues=high_priority --concurrency=2\ncelery -A myapp worker --queues=low_priority --concurrency=6\n```\n\n**2. Auto-scaling Configuration:**\n```python\n# celeryconfig.py\n\n# Worker auto-scaling\nworker_autoscaler = 'celery.worker.autoscale:Autoscaler'\nworker_max_tasks_per_child = 1000  # Restart worker after 1000 tasks\nworker_prefetch_multiplier = 4  # Number of tasks to prefetch per worker\n\n# Pool settings\nworker_pool = 'prefork'  # or 'eventlet', 'gevent', 'threads'\nworker_concurrency = 4  # Number of concurrent processes/threads\n\n# Memory management\nworker_max_memory_per_child = 200000  # 200MB per child process\n```\n\n**3. Pool Types and Use Cases:**\n```bash\n# Prefork pool (default) - CPU-bound tasks\ncelery -A myapp worker --pool=prefork --concurrency=4\n\n# Eventlet pool - I/O-bound tasks with high concurrency\ncelery -A myapp worker --pool=eventlet --concurrency=1000\n\n# Gevent pool - I/O-bound tasks\ncelery -A myapp worker --pool=gevent --concurrency=1000\n\n# Thread pool - I/O-bound tasks with shared memory\ncelery -A myapp worker --pool=threads --concurrency=50\n```\n\n**Queue-Based Task Routing:**\n\n```python\n# Advanced queue configuration\napp.conf.task_routes = {\n    # CPU-intensive tasks\n    'myapp.tasks.process_video': {\n        'queue': 'cpu_intensive',\n        'routing_key': 'cpu.process',\n    },\n    \n    # I/O-bound tasks\n    'myapp.tasks.fetch_data': {\n        'queue': 'io_bound',\n        'routing_key': 'io.fetch',\n    },\n    \n    # Quick tasks\n    'myapp.tasks.send_notification': {\n        'queue': 'notifications',\n        'routing_key': 'quick.notify',\n    },\n    \n    # Long-running tasks\n    'myapp.tasks.generate_report': {\n        'queue': 'reports',\n        'routing_key': 'long.report',\n    },\n}\n\n# Queue-specific worker configuration\napp.conf.task_annotations = {\n    'myapp.tasks.process_video': {\n        'rate_limit': '10/m',  # Limit to 10 tasks per minute\n        'time_limit': 300,     # 5 minute time limit\n        'soft_time_limit': 240, # Soft limit at 4 minutes\n    },\n    'myapp.tasks.send_notification': {\n        'rate_limit': '100/s', # High throughput for notifications\n        'time_limit': 30,\n    },\n}\n```\n\n**Performance Optimization Techniques:**\n\n**1. Task Optimization:**\n```python\n# Efficient task design\n@app.task\ndef optimized_task(data_ids):\n    \"\"\"Process multiple items in one task instead of individual tasks\"\"\"\n    # Batch processing reduces overhead\n    results = []\n    for batch in chunk_list(data_ids, 100):\n        batch_results = process_batch(batch)\n        results.extend(batch_results)\n    return results\n\n# Use task immutability for better optimization\n@app.task(bind=True, immutable=True)\ndef immutable_task(self, data):\n    \"\"\"Immutable tasks can be optimized better\"\"\"\n    return process_data(data)\n\n# Minimize task arguments\n@app.task\ndef lightweight_task(data_id):\n    \"\"\"Pass IDs instead of full objects\"\"\"\n    data = get_data_from_db(data_id)  # Fetch in worker\n    return process_data(data)\n```\n\n**2. Connection Pooling and Caching:**\n```python\n# Redis connection pooling\nfrom redis.connection import ConnectionPool\n\npool = ConnectionPool.from_url(\n    'redis://localhost:6379/0',\n    max_connections=20,\n    retry_on_timeout=True\n)\n\napp.conf.broker_transport_options = {\n    'connection_pool': pool,\n    'retry_policy': {\n        'timeout': 5.0\n    }\n}\n\n# Database connection optimization\n@app.task\ndef db_optimized_task(item_ids):\n    \"\"\"Use connection pooling and bulk operations\"\"\"\n    # Use bulk operations instead of individual queries\n    items = Item.objects.filter(id__in=item_ids).select_related('category')\n    \n    # Process in batches to manage memory\n    for batch in chunked(items, 1000):\n        process_item_batch(batch)\n    \n    return len(item_ids)\n```\n\n**3. Memory Management:**\n```python\n# Memory-efficient task processing\n@app.task\ndef memory_efficient_task(file_path):\n    \"\"\"Process large files without loading everything into memory\"\"\"\n    results = []\n    \n    with open(file_path, 'r') as file:\n        for chunk in read_in_chunks(file, chunk_size=1024):\n            # Process chunk and discard\n            result = process_chunk(chunk)\n            results.append(result)\n            \n            # Explicit garbage collection for large datasets\n            if len(results) % 1000 == 0:\n                import gc\n                gc.collect()\n    \n    return results\n\n# Worker memory limits\napp.conf.worker_max_memory_per_child = 200 * 1024  # 200MB\napp.conf.worker_max_tasks_per_child = 1000\n```\n\n**Broker Optimization:**\n\n**1. Redis Optimization:**\n```python\n# Redis broker settings\napp.conf.broker_transport_options = {\n    'visibility_timeout': 3600,\n    'fanout_prefix': True,\n    'fanout_patterns': True,\n    'retry_policy': {\n        'timeout': 5.0\n    },\n    'connection_pool_kwargs': {\n        'max_connections': 20,\n        'retry_on_timeout': True,\n    },\n}\n\n# Result backend optimization\napp.conf.result_backend_transport_options = {\n    'master_name': 'mymaster',\n    'connection_pool_kwargs': {\n        'max_connections': 20,\n    },\n}\n```\n\n**2. RabbitMQ Optimization:**\n```python\n# RabbitMQ broker settings\napp.conf.broker_transport_options = {\n    'priority_steps': list(range(10)),\n    'sep': ':',\n    'queue_order_strategy': 'priority',\n    'connection_pool_kwargs': {\n        'max_connections': 20,\n    },\n}\n\n# Queue arguments for performance\napp.conf.task_annotations = {\n    '*': {\n        'queue_arguments': {\n            'x-max-priority': 10,\n            'x-queue-mode': 'lazy',  # For large queues\n        }\n    }\n}\n```\n\n**Monitoring and Autoscaling:**\n\n```python\n# Dynamic worker scaling based on queue length\n@app.task\ndef auto_scale_workers():\n    \"\"\"Scale workers based on queue metrics\"\"\"\n    import subprocess\n    from celery.app.control import Inspect\n    \n    i = Inspect(app=app)\n    \n    # Get queue lengths\n    queue_lengths = get_queue_lengths()\n    active_tasks = len(i.active() or {})\n    \n    # Scaling logic\n    total_queued = sum(queue_lengths.values())\n    \n    if total_queued > 1000 and active_tasks < 50:\n        # Scale up\n        subprocess.Popen([\n            'celery', '-A', 'myapp', 'worker',\n            '--detach', '--concurrency=4'\n        ])\n        logger.info(\"Scaled up workers due to high queue length\")\n    \n    elif total_queued < 100 and active_tasks > 10:\n        # Scale down (implementation depends on deployment)\n        logger.info(\"Consider scaling down workers\")\n    \n    return {\n        'queue_lengths': queue_lengths,\n        'active_tasks': active_tasks,\n        'action': 'checked'\n    }\n\n# Schedule the autoscaling task\napp.conf.beat_schedule = {\n    'auto-scale': {\n        'task': 'myapp.tasks.auto_scale_workers',\n        'schedule': 60.0,  # Every minute\n    },\n}\n```\n\n**Load Balancing and Distribution:**\n\n```python\n# Geographic distribution\napp.conf.task_routes = {\n    'myapp.tasks.process_us_data': {\n        'queue': 'us_workers',\n        'exchange': 'us_exchange',\n    },\n    'myapp.tasks.process_eu_data': {\n        'queue': 'eu_workers',\n        'exchange': 'eu_exchange',\n    },\n}\n\n# Load balancing with consistent hashing\n@app.task\ndef distributed_task(user_id, data):\n    \"\"\"Route task based on user_id for consistent processing\"\"\"\n    # This ensures same user always goes to same worker type\n    return process_user_data(user_id, data)\n\n# Custom router for load balancing\nclass LoadBalancingRouter:\n    def route_for_task(self, task, args=None, kwargs=None):\n        if task == 'myapp.tasks.distributed_task':\n            user_id = args[0] if args else kwargs.get('user_id')\n            # Route based on user_id hash\n            worker_num = hash(user_id) % 4\n            return {\n                'queue': f'worker_{worker_num}',\n                'routing_key': f'worker.{worker_num}',\n            }\n        return None\n\napp.conf.task_routes = (LoadBalancingRouter(),)\n```\n\n**Performance Testing and Benchmarking:**\n\n```python\n@app.task\ndef benchmark_task(iterations=1000):\n    \"\"\"Benchmark task for performance testing\"\"\"\n    import time\n    start_time = time.time()\n    \n    # Simulate work\n    total = 0\n    for i in range(iterations):\n        total += i * i\n    \n    end_time = time.time()\n    return {\n        'iterations': iterations,\n        'duration': end_time - start_time,\n        'rate': iterations / (end_time - start_time)\n    }\n\n# Load testing script\ndef load_test():\n    \"\"\"Generate load for testing\"\"\"\n    import concurrent.futures\n    \n    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n        futures = []\n        for i in range(100):\n            future = executor.submit(benchmark_task.delay, 1000)\n            futures.append(future)\n        \n        results = [future.result() for future in futures]\n    \n    return results\n```\n\n**Best Practices for Scaling:**\n\n1. **Profile Before Optimizing** - Identify actual bottlenecks\n2. **Use Appropriate Pool Types** - Match pool to task characteristics\n3. **Implement Queue Segregation** - Separate fast and slow tasks\n4. **Monitor Resource Usage** - Track CPU, memory, and I/O\n5. **Batch Operations** - Process multiple items per task\n6. **Connection Pooling** - Reuse database and broker connections\n7. **Memory Management** - Set limits and restart workers periodically\n8. **Geographic Distribution** - Place workers near data sources\n9. **Load Testing** - Test under realistic load conditions\n10. **Gradual Scaling** - Scale incrementally and monitor impact",
      "keywords": ["scaling", "performance", "worker pools", "queue routing", "connection pooling", "memory management", "autoscaling", "load balancing", "optimization", "benchmarking"],
      "difficulty": "hard"
    },
    {
      "id": 23008,
      "tag": "celery",
      "question": "What are Celery workflows (chains, groups, chords) and when would you use them?",
      "answer": "Celery workflows allow you to compose complex task execution patterns by combining individual tasks into sophisticated pipelines and parallel processing structures.\n\n**Basic Workflow Patterns:**\n\n**1. Chains - Sequential Execution:**\n```python\nfrom celery import chain\n\n# Simple chain\nworkflow = chain(\n    extract_data.s('source1'),\n    transform_data.s(),\n    load_data.s('destination')\n)\nresult = workflow.apply_async()\n\n# Chain with partial application\nprocessing_chain = (\n    fetch_user_data.s(user_id=123) |\n    validate_data.s() |\n    save_to_database.s() |\n    send_notification.s()\n)\nresult = processing_chain.apply_async()\n```\n\n**2. Groups - Parallel Execution:**\n```python\nfrom celery import group\n\n# Process multiple items in parallel\nparallel_jobs = group(\n    process_file.s('file1.txt'),\n    process_file.s('file2.txt'),\n    process_file.s('file3.txt'),\n    process_file.s('file4.txt')\n)\nresult = parallel_jobs.apply_async()\n\n# Dynamic group creation\nfile_list = ['file1.txt', 'file2.txt', 'file3.txt']\ndynamic_group = group(process_file.s(filename) for filename in file_list)\nresult = dynamic_group.apply_async()\n```\n\n**3. Chords - Group + Callback:**\n```python\nfrom celery import chord\n\n# Process items in parallel, then aggregate results\naggregation_workflow = chord(\n    group(\n        analyze_data.s(dataset) for dataset in datasets\n    ),\n    aggregate_results.s()  # Called with all results from group\n)\nresult = aggregation_workflow.apply_async()\n\n# Map-Reduce pattern\nmap_reduce = chord(\n    group(map_task.s(chunk) for chunk in data_chunks),\n    reduce_task.s()\n)\nresult = map_reduce.apply_async()\n```\n\n**Advanced Workflow Examples:**\n\n**1. ETL Pipeline with Error Handling:**\n```python\n@app.task(bind=True)\ndef extract_data(self, source):\n    try:\n        data = fetch_from_source(source)\n        if not data:\n            raise ValueError(\"No data found\")\n        return data\n    except Exception as exc:\n        # Log error and potentially retry\n        logger.error(f\"Extract failed for {source}: {exc}\")\n        raise self.retry(exc=exc, countdown=60, max_retries=3)\n\n@app.task\ndef transform_data(raw_data):\n    # Transform the data\n    cleaned_data = clean_data(raw_data)\n    enriched_data = enrich_data(cleaned_data)\n    return enriched_data\n\n@app.task\ndef load_data(transformed_data, destination):\n    # Load to destination\n    result = save_to_destination(transformed_data, destination)\n    return {'records_loaded': len(transformed_data), 'destination': destination}\n\n@app.task\ndef notify_completion(load_result):\n    # Send completion notification\n    send_notification(\n        f\"ETL completed: {load_result['records_loaded']} records loaded to {load_result['destination']}\"\n    )\n    return load_result\n\n# Complete ETL workflow\ndef run_etl_pipeline(sources, destination):\n    # Extract from multiple sources in parallel\n    extract_jobs = group(\n        extract_data.s(source) for source in sources\n    )\n    \n    # Chain: parallel extract -> transform -> load -> notify\n    etl_workflow = chain(\n        extract_jobs,\n        transform_data.s(),\n        load_data.s(destination),\n        notify_completion.s()\n    )\n    \n    return etl_workflow.apply_async()\n```\n\n**2. Image Processing Pipeline:**\n```python\n@app.task\ndef resize_image(image_path, size):\n    resized_path = f\"{image_path}_resized_{size}.jpg\"\n    resize_image_file(image_path, resized_path, size)\n    return resized_path\n\n@app.task\ndef add_watermark(image_path):\n    watermarked_path = f\"{image_path}_watermarked.jpg\"\n    add_watermark_to_image(image_path, watermarked_path)\n    return watermarked_path\n\n@app.task\ndef optimize_image(image_path):\n    optimized_path = f\"{image_path}_optimized.jpg\"\n    optimize_image_file(image_path, optimized_path)\n    return optimized_path\n\n@app.task\ndef upload_to_cdn(image_paths):\n    cdn_urls = []\n    for path in image_paths:\n        url = upload_file_to_cdn(path)\n        cdn_urls.append(url)\n    return cdn_urls\n\n# Image processing workflow\ndef process_uploaded_image(original_image_path):\n    # Create multiple sizes in parallel\n    resize_jobs = group(\n        resize_image.s(original_image_path, 150),   # Thumbnail\n        resize_image.s(original_image_path, 600),   # Medium\n        resize_image.s(original_image_path, 1200)   # Large\n    )\n    \n    # Process each size: resize -> watermark -> optimize\n    processing_workflow = chord(\n        resize_jobs,\n        chain(\n            group(\n                chain(add_watermark.s(path), optimize_image.s()) \n                for path in ['placeholder']  # Will be replaced with actual paths\n            ),\n            upload_to_cdn.s()\n        )\n    )\n    \n    return processing_workflow.apply_async()\n```\n\n**3. Machine Learning Pipeline:**\n```python\n@app.task\ndef prepare_training_data(dataset_ids):\n    training_data = []\n    for dataset_id in dataset_ids:\n        data = load_dataset(dataset_id)\n        processed_data = preprocess_data(data)\n        training_data.extend(processed_data)\n    return training_data\n\n@app.task\ndef train_model(training_data, model_config):\n    model = create_model(model_config)\n    trained_model = model.fit(training_data)\n    model_path = save_model(trained_model)\n    return model_path\n\n@app.task\ndef evaluate_model(model_path, test_data):\n    model = load_model(model_path)\n    metrics = evaluate_model_performance(model, test_data)\n    return {'model_path': model_path, 'metrics': metrics}\n\n@app.task\ndef select_best_model(evaluation_results):\n    best_model = max(evaluation_results, key=lambda x: x['metrics']['accuracy'])\n    deploy_model(best_model['model_path'])\n    return best_model\n\n# ML pipeline with hyperparameter tuning\ndef run_ml_pipeline(dataset_ids, model_configs):\n    # Prepare data once\n    data_prep = prepare_training_data.s(dataset_ids)\n    \n    # Train multiple models in parallel\n    training_jobs = group(\n        chain(\n            data_prep,\n            train_model.s(config),\n            evaluate_model.s(test_data)\n        ) for config in model_configs\n    )\n    \n    # Select best model after all evaluations complete\n    ml_workflow = chord(\n        training_jobs,\n        select_best_model.s()\n    )\n    \n    return ml_workflow.apply_async()\n```\n\n**Workflow Error Handling:**\n```python\n@app.task(bind=True)\ndef resilient_task(self, data, step_name):\n    try:\n        result = process_step(data, step_name)\n        return result\n    except Exception as exc:\n        # Log the error with workflow context\n        logger.error(f\"Step {step_name} failed in workflow {self.request.root_id}: {exc}\")\n        \n        # Decide whether to retry or fail the entire workflow\n        if is_retryable_error(exc):\n            raise self.retry(exc=exc, countdown=30, max_retries=3)\n        else:\n            # Signal workflow failure\n            workflow_failure_handler.delay(\n                workflow_id=self.request.root_id,\n                failed_step=step_name,\n                error=str(exc)\n            )\n            raise\n\n@app.task\ndef workflow_failure_handler(workflow_id, failed_step, error):\n    \"\"\"Handle workflow failures\"\"\"\n    logger.error(f\"Workflow {workflow_id} failed at step {failed_step}: {error}\")\n    \n    # Clean up any partial results\n    cleanup_workflow_artifacts(workflow_id)\n    \n    # Notify stakeholders\n    send_failure_notification(workflow_id, failed_step, error)\n    \n    # Store failure details for analysis\n    record_workflow_failure(workflow_id, failed_step, error)\n```\n\n**Monitoring Workflows:**\n```python\n@app.task\ndef workflow_monitor(workflow_id):\n    \"\"\"Monitor workflow progress\"\"\"\n    from celery.result import GroupResult, AsyncResult\n    \n    # Get workflow result\n    result = AsyncResult(workflow_id)\n    \n    if isinstance(result, GroupResult):\n        # Handle group results\n        completed = sum(1 for r in result.results if r.ready())\n        total = len(result.results)\n        \n        progress = {\n            'workflow_id': workflow_id,\n            'type': 'group',\n            'completed': completed,\n            'total': total,\n            'progress_percent': (completed / total) * 100,\n            'status': 'completed' if result.ready() else 'running'\n        }\n    else:\n        # Handle single task or chain\n        progress = {\n            'workflow_id': workflow_id,\n            'type': 'chain',\n            'status': result.status,\n            'current_task': result.name if hasattr(result, 'name') else 'unknown'\n        }\n    \n    # Store progress update\n    update_workflow_progress(workflow_id, progress)\n    return progress\n\n# Schedule workflow monitoring\ndef start_workflow_with_monitoring(workflow):\n    result = workflow.apply_async()\n    \n    # Schedule periodic monitoring\n    monitor_schedule = {\n        f'monitor-workflow-{result.id}': {\n            'task': 'myapp.tasks.workflow_monitor',\n            'schedule': 30.0,  # Every 30 seconds\n            'args': (result.id,),\n            'options': {'expires': 3600}  # Stop monitoring after 1 hour\n        }\n    }\n    \n    # Dynamically add to beat schedule\n    app.conf.beat_schedule.update(monitor_schedule)\n    \n    return result\n```\n\n**When to Use Each Pattern:**\n\n**Chains - Use When:**\n- Tasks must execute in specific order\n- Output of one task is input to next\n- ETL pipelines\n- Sequential data processing\n- State machines\n\n**Groups - Use When:**\n- Tasks can run independently\n- Processing multiple similar items\n- Parallel data processing\n- Batch operations\n- Fan-out scenarios\n\n**Chords - Use When:**\n- Need to aggregate results from parallel tasks\n- Map-Reduce patterns\n- Parallel processing followed by summarization\n- Collecting results from multiple data sources\n- Batch processing with final aggregation\n\n**Best Practices:**\n1. **Keep workflows simple** - Complex workflows are hard to debug\n2. **Handle errors gracefully** - Plan for partial failures\n3. **Monitor progress** - Track workflow execution\n4. **Use immutable tasks** - Avoid side effects in workflow tasks\n5. **Plan for retries** - Make workflows resilient\n6. **Store intermediate results** - Enable recovery and debugging\n7. **Test workflows thoroughly** - Verify all execution paths\n8. **Document workflow logic** - Make workflows understandable",
      "keywords": ["workflows", "chains", "groups", "chords", "parallel processing", "sequential execution", "ETL pipeline", "map-reduce", "error handling", "monitoring"],
      "difficulty": "hard"
    },
    {
      "id": 23009,
      "tag": "celery",
      "question": "How do you integrate Celery with Django and what are the best practices?",
      "answer": "Integrating Celery with Django creates a powerful combination for handling background tasks in web applications. Here's how to set it up properly and follow best practices.\n\n**Basic Django-Celery Setup:**\n\n**1. Project Structure:**\n```\nmyproject/\n├── myproject/\n│   ├── __init__.py\n│   ├── settings.py\n│   ├── celery.py        # Celery configuration\n│   ├── urls.py\n│   └── wsgi.py\n├── myapp/\n│   ├── __init__.py\n│   ├── models.py\n│   ├── views.py\n│   ├── tasks.py         # Celery tasks\n│   └── admin.py\n└── manage.py\n```\n\n**2. Celery Configuration (myproject/celery.py):**\n```python\nimport os\nfrom celery import Celery\nfrom django.conf import settings\n\n# Set default Django settings\nos.environ.setdefault('DJANGO_SETTINGS_MODULE', 'myproject.settings')\n\napp = Celery('myproject')\n\n# Load settings from Django settings file\napp.config_from_object('django.conf:settings', namespace='CELERY')\n\n# Auto-discover tasks from all Django apps\napp.autodiscover_tasks()\n\n# Optional: Add debug task\n@app.task(bind=True)\ndef debug_task(self):\n    print(f'Request: {self.request!r}')\n```\n\n**3. Django Settings (settings.py):**\n```python\n# Celery Configuration\nCELERY_BROKER_URL = 'redis://localhost:6379/0'\nCELERY_RESULT_BACKEND = 'redis://localhost:6379/0'\n\n# Celery Settings\nCELERY_ACCEPT_CONTENT = ['json']\nCELERY_TASK_SERIALIZER = 'json'\nCELERY_RESULT_SERIALIZER = 'json'\nCELERY_TIMEZONE = 'UTC'\nCELERY_ENABLE_UTC = True\n\n# Task routing\nCELERY_TASK_ROUTES = {\n    'myapp.tasks.send_email': {'queue': 'emails'},\n    'myapp.tasks.process_image': {'queue': 'images'},\n    'myapp.tasks.generate_report': {'queue': 'reports'},\n}\n\n# Worker settings\nCELERY_WORKER_PREFETCH_MULTIPLIER = 1\nCELERY_TASK_ACKS_LATE = True\nCELERY_WORKER_MAX_TASKS_PER_CHILD = 1000\n\n# Beat schedule for periodic tasks\nCELERY_BEAT_SCHEDULE = {\n    'cleanup-expired-sessions': {\n        'task': 'myapp.tasks.cleanup_expired_sessions',\n        'schedule': crontab(hour=2, minute=0),\n    },\n    'send-daily-reports': {\n        'task': 'myapp.tasks.send_daily_reports',\n        'schedule': crontab(hour=8, minute=0),\n    },\n}\n```\n\n**4. App Initialization (myproject/__init__.py):**\n```python\n# This will make sure the app is always imported when\n# Django starts so that shared_task will use this app.\nfrom .celery import app as celery_app\n\n__all__ = ('celery_app',)\n```\n\n**Creating Django-Celery Tasks:**\n\n**1. Using shared_task decorator:**\n```python\n# myapp/tasks.py\nfrom celery import shared_task\nfrom django.contrib.auth.models import User\nfrom django.core.mail import send_mail\nfrom django.template.loader import render_to_string\nfrom .models import Report, UserProfile\n\n@shared_task\ndef send_welcome_email(user_id):\n    \"\"\"Send welcome email to new user\"\"\"\n    try:\n        user = User.objects.get(id=user_id)\n        \n        subject = 'Welcome to our platform!'\n        html_content = render_to_string('emails/welcome.html', {\n            'user': user,\n            'site_url': settings.SITE_URL\n        })\n        \n        send_mail(\n            subject=subject,\n            message='',  # Plain text version\n            html_message=html_content,\n            from_email=settings.DEFAULT_FROM_EMAIL,\n            recipient_list=[user.email],\n            fail_silently=False\n        )\n        \n        return f\"Welcome email sent to {user.email}\"\n        \n    except User.DoesNotExist:\n        return f\"User with id {user_id} not found\"\n\n@shared_task(bind=True)\ndef generate_user_report(self, user_id, report_type='monthly'):\n    \"\"\"Generate user activity report\"\"\"\n    try:\n        user = User.objects.select_related('profile').get(id=user_id)\n        \n        # Update task state\n        self.update_state(\n            state='PROGRESS',\n            meta={'current': 10, 'total': 100, 'status': 'Collecting data...'}\n        )\n        \n        # Collect user data\n        user_data = collect_user_activity_data(user, report_type)\n        \n        self.update_state(\n            state='PROGRESS',\n            meta={'current': 50, 'total': 100, 'status': 'Generating report...'}\n        )\n        \n        # Generate report\n        report_data = generate_report_data(user_data, report_type)\n        \n        self.update_state(\n            state='PROGRESS',\n            meta={'current': 80, 'total': 100, 'status': 'Saving report...'}\n        )\n        \n        # Save report to database\n        report = Report.objects.create(\n            user=user,\n            report_type=report_type,\n            data=report_data,\n            generated_at=timezone.now()\n        )\n        \n        return {\n            'report_id': report.id,\n            'user_id': user_id,\n            'report_type': report_type,\n            'status': 'completed'\n        }\n        \n    except Exception as exc:\n        self.update_state(\n            state='FAILURE',\n            meta={'error': str(exc)}\n        )\n        raise\n```\n\n**2. Django Model Integration:**\n```python\n# myapp/models.py\nfrom django.db import models\nfrom django.contrib.auth.models import User\nfrom django.db.models.signals import post_save\nfrom django.dispatch import receiver\n\nclass UserProfile(models.Model):\n    user = models.OneToOneField(User, on_delete=models.CASCADE)\n    email_notifications = models.BooleanField(default=True)\n    report_frequency = models.CharField(\n        max_length=20,\n        choices=[('daily', 'Daily'), ('weekly', 'Weekly'), ('monthly', 'Monthly')],\n        default='weekly'\n    )\n    created_at = models.DateTimeField(auto_now_add=True)\n\nclass TaskResult(models.Model):\n    \"\"\"Track task execution results\"\"\"\n    task_id = models.CharField(max_length=255, unique=True)\n    task_name = models.CharField(max_length=255)\n    user = models.ForeignKey(User, on_delete=models.CASCADE, null=True, blank=True)\n    status = models.CharField(max_length=50)\n    result = models.JSONField(null=True, blank=True)\n    created_at = models.DateTimeField(auto_now_add=True)\n    completed_at = models.DateTimeField(null=True, blank=True)\n\n# Signal handlers\n@receiver(post_save, sender=User)\ndef handle_new_user(sender, instance, created, **kwargs):\n    \"\"\"Handle new user registration\"\"\"\n    if created:\n        # Create user profile\n        UserProfile.objects.create(user=instance)\n        \n        # Send welcome email asynchronously\n        from .tasks import send_welcome_email\n        send_welcome_email.delay(instance.id)\n```\n\n**Django Views Integration:**\n\n```python\n# myapp/views.py\nfrom django.http import JsonResponse\nfrom django.shortcuts import render, get_object_or_404\nfrom django.contrib.auth.decorators import login_required\nfrom django.views.decorators.http import require_http_methods\nfrom celery.result import AsyncResult\nfrom .tasks import generate_user_report, process_file_upload\nfrom .models import TaskResult\n\n@login_required\n@require_http_methods([\"POST\"])\ndef generate_report(request):\n    \"\"\"Start report generation task\"\"\"\n    report_type = request.POST.get('report_type', 'monthly')\n    \n    # Start the task\n    task = generate_user_report.delay(request.user.id, report_type)\n    \n    # Store task info\n    TaskResult.objects.create(\n        task_id=task.id,\n        task_name='generate_user_report',\n        user=request.user,\n        status='PENDING'\n    )\n    \n    return JsonResponse({\n        'task_id': task.id,\n        'status': 'started',\n        'message': 'Report generation started'\n    })\n\n@login_required\ndef check_task_status(request, task_id):\n    \"\"\"Check task progress and status\"\"\"\n    task = AsyncResult(task_id)\n    \n    if task.state == 'PENDING':\n        response = {\n            'state': task.state,\n            'current': 0,\n            'total': 1,\n            'status': 'Task is waiting to be processed...'\n        }\n    elif task.state == 'PROGRESS':\n        response = {\n            'state': task.state,\n            'current': task.info.get('current', 0),\n            'total': task.info.get('total', 1),\n            'status': task.info.get('status', '')\n        }\n    elif task.state == 'SUCCESS':\n        response = {\n            'state': task.state,\n            'current': 100,\n            'total': 100,\n            'status': 'Task completed successfully',\n            'result': task.info\n        }\n        \n        # Update TaskResult\n        TaskResult.objects.filter(task_id=task_id).update(\n            status='SUCCESS',\n            result=task.info,\n            completed_at=timezone.now()\n        )\n    else:\n        # Error occurred\n        response = {\n            'state': task.state,\n            'current': 100,\n            'total': 100,\n            'status': 'Task failed',\n            'error': str(task.info)\n        }\n        \n        TaskResult.objects.filter(task_id=task_id).update(\n            status='FAILURE',\n            result={'error': str(task.info)},\n            completed_at=timezone.now()\n        )\n    \n    return JsonResponse(response)\n\n@login_required\ndef file_upload(request):\n    \"\"\"Handle file upload with background processing\"\"\"\n    if request.method == 'POST' and request.FILES['file']:\n        uploaded_file = request.FILES['file']\n        \n        # Save file temporarily\n        file_path = handle_uploaded_file(uploaded_file)\n        \n        # Process file in background\n        task = process_file_upload.delay(file_path, request.user.id)\n        \n        return JsonResponse({\n            'task_id': task.id,\n            'message': 'File upload started processing'\n        })\n    \n    return render(request, 'upload.html')\n```\n\n**Admin Integration:**\n\n```python\n# myapp/admin.py\nfrom django.contrib import admin\nfrom django.utils.html import format_html\nfrom .models import TaskResult, UserProfile\nfrom .tasks import generate_user_report\n\n@admin.register(TaskResult)\nclass TaskResultAdmin(admin.ModelAdmin):\n    list_display = ['task_id', 'task_name', 'user', 'status', 'created_at', 'action_buttons']\n    list_filter = ['status', 'task_name', 'created_at']\n    search_fields = ['task_id', 'user__username', 'task_name']\n    readonly_fields = ['task_id', 'created_at', 'completed_at']\n    \n    def action_buttons(self, obj):\n        if obj.status == 'PENDING':\n            return format_html(\n                '<a class=\"button\" href=\"/admin/check-task/{}/\">Check Status</a>',\n                obj.task_id\n            )\n        return '-'\n    action_buttons.short_description = 'Actions'\n\n@admin.register(UserProfile)\nclass UserProfileAdmin(admin.ModelAdmin):\n    list_display = ['user', 'email_notifications', 'report_frequency', 'created_at', 'generate_report_button']\n    list_filter = ['email_notifications', 'report_frequency']\n    \n    def generate_report_button(self, obj):\n        return format_html(\n            '<a class=\"button\" href=\"/admin/generate-report/{}/\">Generate Report</a>',\n            obj.user.id\n        )\n    generate_report_button.short_description = 'Actions'\n```\n\n**Database Result Backend (Alternative):**\n\n```python\n# settings.py - Using database as result backend\nCELERY_RESULT_BACKEND = 'django-db'\nCELERY_CACHE_BACKEND = 'django-cache'\n\n# Install django-celery-results\n# pip install django-celery-results\n\nINSTALLED_APPS = [\n    # ... other apps\n    'django_celery_results',\n    'django_celery_beat',  # For periodic tasks\n]\n\n# Run migrations\n# python manage.py migrate django_celery_results\n# python manage.py migrate django_celery_beat\n```\n\n**Testing Celery Tasks:**\n\n```python\n# myapp/tests.py\nfrom django.test import TestCase, override_settings\nfrom django.contrib.auth.models import User\nfrom celery import current_app\nfrom .tasks import send_welcome_email, generate_user_report\n\nclass CeleryTaskTests(TestCase):\n    def setUp(self):\n        self.user = User.objects.create_user(\n            username='testuser',\n            email='test@example.com',\n            password='testpass123'\n        )\n    \n    @override_settings(CELERY_TASK_ALWAYS_EAGER=True)\n    def test_send_welcome_email(self):\n        \"\"\"Test welcome email task\"\"\"\n        with self.settings(EMAIL_BACKEND='django.core.mail.backends.locmem.EmailBackend'):\n            result = send_welcome_email.delay(self.user.id)\n            self.assertTrue(result.successful())\n            self.assertIn('Welcome email sent', result.result)\n    \n    @override_settings(CELERY_TASK_ALWAYS_EAGER=True)\n    def test_generate_user_report(self):\n        \"\"\"Test report generation task\"\"\"\n        result = generate_user_report.delay(self.user.id, 'monthly')\n        self.assertTrue(result.successful())\n        self.assertEqual(result.result['user_id'], self.user.id)\n        self.assertEqual(result.result['status'], 'completed')\n    \n    def test_task_routing(self):\n        \"\"\"Test task routing configuration\"\"\"\n        routes = current_app.conf.task_routes\n        self.assertEqual(routes['myapp.tasks.send_welcome_email']['queue'], 'emails')\n```\n\n**Best Practices for Django-Celery Integration:**\n\n1. **Use shared_task** - Makes tasks reusable across projects\n2. **Handle database connections properly** - Django connections may timeout\n3. **Pass IDs, not objects** - Serialize only what's necessary\n4. **Use database transactions carefully** - Ensure data consistency\n5. **Monitor task progress** - Provide user feedback for long tasks\n6. **Implement proper error handling** - Log errors and notify users\n7. **Use signals wisely** - Don't overload signal handlers with heavy tasks\n8. **Test with eager execution** - Simplify testing with synchronous execution\n9. **Separate task queues** - Route different types of tasks to different queues\n10. **Monitor production tasks** - Use tools like Flower or Celery Monitor",
      "keywords": ["Django", "shared_task", "database integration", "signals", "admin integration", "task routing", "result backend", "testing", "views", "models"],
      "difficulty": "medium"
    }
  ]
}