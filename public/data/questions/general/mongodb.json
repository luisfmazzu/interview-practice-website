{
  "technology": "mongodb",
  "questions": [
    {
      "id": 13000,
      "tag": "mongodb",
      "question": "What is MongoDB and how does it differ from traditional relational databases? Explain the concept of NoSQL.",
      "answer": "MongoDB is a document-oriented NoSQL database that stores data in flexible, JSON-like documents called BSON (Binary JSON). Unlike traditional relational databases that use tables with fixed schemas, MongoDB uses collections of documents that can have varying structures.\n\nKey differences from relational databases:\n\n**Schema Flexibility**: MongoDB documents don't require a predefined schema. Fields can be added, removed, or modified without altering the entire collection structure.\n\n**Data Structure**: Instead of rows and columns, MongoDB stores data as documents with key-value pairs, arrays, and nested objects.\n\n**Scalability**: MongoDB is designed for horizontal scaling (sharding) across multiple servers, while relational databases typically scale vertically.\n\n**Query Language**: Uses a rich query language with JavaScript-like syntax instead of SQL.\n\nNoSQL (Not Only SQL) databases like MongoDB are designed to handle:\n- Large volumes of unstructured or semi-structured data\n- Rapid development and iteration\n- Distributed computing environments\n- Applications requiring flexible data models\n\nExample MongoDB document:\n```json\n{\n  \"_id\": \"507f1f77bcf86cd799439011\",\n  \"name\": \"John Doe\",\n  \"email\": \"john@example.com\",\n  \"address\": {\n    \"street\": \"123 Main St\",\n    \"city\": \"New York\"\n  },\n  \"hobbies\": [\"reading\", \"swimming\"]\n}\n```\n\nThis flexibility makes MongoDB ideal for content management, real-time analytics, IoT applications, and modern web applications.",
      "keywords": ["mongodb", "nosql", "document database", "bson", "schema flexibility", "collections", "horizontal scaling"],
      "difficulty": "easy"
    },
    {
      "id": 13001,
      "tag": "mongodb",
      "question": "Explain MongoDB document structure and BSON. What are the advantages of BSON over JSON?",
      "answer": "MongoDB stores data in documents using BSON (Binary JSON), which is a binary-encoded serialization format. While documents appear as JSON-like structures, they're actually stored as BSON internally.\n\n**Document Structure**:\nMongoDB documents are composed of field-value pairs, similar to JSON objects:\n```json\n{\n  \"_id\": ObjectId(\"507f1f77bcf86cd799439011\"),\n  \"username\": \"alice\",\n  \"age\": 30,\n  \"isActive\": true,\n  \"tags\": [\"developer\", \"mongodb\"],\n  \"profile\": {\n    \"bio\": \"Software developer\",\n    \"location\": \"San Francisco\"\n  },\n  \"joinDate\": ISODate(\"2023-01-15T10:30:00Z\")\n}\n```\n\n**BSON Advantages over JSON**:\n\n1. **Extended Data Types**: BSON supports additional types not available in JSON:\n   - ObjectId (unique 12-byte identifier)\n   - Date/Timestamp objects\n   - Binary data\n   - Regular expressions\n   - 32-bit and 64-bit integers\n\n2. **Performance**: Binary encoding allows faster parsing and generation compared to text-based JSON.\n\n3. **Space Efficiency**: For certain data types, BSON can be more compact than JSON.\n\n4. **Traversability**: BSON includes length information, making it easier to skip over fields during parsing.\n\n**Key Features**:\n- Maximum document size: 16MB\n- Field names are strings and case-sensitive\n- Documents can contain arrays and nested documents\n- The `_id` field is automatically added if not specified\n\nThis structure enables MongoDB to efficiently store, index, and query complex hierarchical data while maintaining the flexibility that makes NoSQL databases attractive for modern applications.",
      "keywords": ["bson", "document structure", "json", "data types", "objectid", "binary encoding", "nested documents"],
      "difficulty": "easy"
    },
    {
      "id": 13002,
      "tag": "mongodb",
      "question": "Describe the basic CRUD operations in MongoDB. Provide examples of Create, Read, Update, and Delete operations.",
      "answer": "MongoDB CRUD operations allow you to Create, Read, Update, and Delete documents in collections. Here are the basic operations:\n\n**CREATE Operations**:\n```javascript\n// Insert single document\ndb.users.insertOne({\n  name: \"John Doe\",\n  email: \"john@example.com\",\n  age: 30\n});\n\n// Insert multiple documents\ndb.users.insertMany([\n  { name: \"Alice\", email: \"alice@example.com\", age: 25 },\n  { name: \"Bob\", email: \"bob@example.com\", age: 35 }\n]);\n```\n\n**READ Operations**:\n```javascript\n// Find all documents\ndb.users.find();\n\n// Find with conditions\ndb.users.find({ age: { $gte: 30 } });\n\n// Find one document\ndb.users.findOne({ email: \"john@example.com\" });\n\n// Find with projection (specific fields)\ndb.users.find({}, { name: 1, email: 1, _id: 0 });\n```\n\n**UPDATE Operations**:\n```javascript\n// Update single document\ndb.users.updateOne(\n  { email: \"john@example.com\" },\n  { $set: { age: 31, status: \"active\" } }\n);\n\n// Update multiple documents\ndb.users.updateMany(\n  { age: { $lt: 30 } },\n  { $set: { category: \"young\" } }\n);\n\n// Replace entire document\ndb.users.replaceOne(\n  { email: \"john@example.com\" },\n  { name: \"John Smith\", email: \"john@example.com\", age: 32 }\n);\n```\n\n**DELETE Operations**:\n```javascript\n// Delete single document\ndb.users.deleteOne({ email: \"john@example.com\" });\n\n// Delete multiple documents\ndb.users.deleteMany({ age: { $lt: 18 } });\n```\n\nThese operations form the foundation of data manipulation in MongoDB, providing flexible ways to manage document-based data with intuitive syntax.",
      "keywords": ["crud operations", "insertOne", "insertMany", "find", "updateOne", "updateMany", "deleteOne", "deleteMany", "mongodb queries"],
      "difficulty": "easy"
    },
    {
      "id": 13003,
      "tag": "mongodb",
      "question": "What are MongoDB collections and databases? How do they relate to each other and how do you work with them?",
      "answer": "In MongoDB, data is organized in a hierarchical structure consisting of databases and collections, which serve as containers for storing documents.\n\n**Database**:\nA database is a container for collections. Each MongoDB deployment can host multiple databases, and each database has its own set of files on the file system.\n\n**Collection**:\nA collection is a group of MongoDB documents, equivalent to a table in relational databases. Collections don't enforce a schema, so documents within a collection can have different fields and structures.\n\n**Relationship**:\n- One MongoDB instance can contain multiple databases\n- Each database contains multiple collections\n- Each collection contains multiple documents\n\n**Working with Databases**:\n```javascript\n// Show all databases\nshow dbs\n\n// Switch to/create database\nuse myapp\n\n// Show current database\ndb\n\n// Drop database\ndb.dropDatabase()\n```\n\n**Working with Collections**:\n```javascript\n// Show collections in current database\nshow collections\n\n// Create collection explicitly\ndb.createCollection(\"users\")\n\n// Create collection implicitly (when inserting first document)\ndb.products.insertOne({ name: \"Laptop\", price: 999 })\n\n// Drop collection\ndb.users.drop()\n\n// Get collection statistics\ndb.users.stats()\n```\n\n**Naming Conventions**:\n- Database names are case-sensitive\n- Collection names should be lowercase\n- Avoid special characters and spaces\n- Use descriptive names (e.g., \"users\", \"products\", \"orders\")\n\n**Key Features**:\n- Collections are created automatically when you first store data\n- No need to define schema beforehand\n- Collections can be indexed for better query performance\n- Each collection can have different access controls\n\nThis flexible structure allows MongoDB to adapt to changing application requirements without complex schema migrations.",
      "keywords": ["collections", "databases", "mongodb structure", "createCollection", "show collections", "database hierarchy"],
      "difficulty": "easy"
    },
    {
      "id": 13004,
      "tag": "mongodb",
      "question": "Explain MongoDB query operators and filtering. How do you use comparison, logical, and element operators?",
      "answer": "MongoDB provides a rich set of query operators for filtering and finding documents. These operators enable complex queries and precise data retrieval.\n\n**Comparison Operators**:\n```javascript\n// Equal to\ndb.users.find({ age: 30 })\n\n// Not equal to\ndb.users.find({ status: { $ne: \"inactive\" } })\n\n// Greater than / Greater than or equal\ndb.users.find({ age: { $gt: 25 } })\ndb.users.find({ age: { $gte: 21 } })\n\n// Less than / Less than or equal\ndb.users.find({ age: { $lt: 65 } })\ndb.users.find({ age: { $lte: 30 } })\n\n// In array of values\ndb.users.find({ status: { $in: [\"active\", \"pending\"] } })\n\n// Not in array\ndb.users.find({ status: { $nin: [\"banned\", \"deleted\"] } })\n```\n\n**Logical Operators**:\n```javascript\n// AND (implicit)\ndb.users.find({ age: { $gte: 18 }, status: \"active\" })\n\n// AND (explicit)\ndb.users.find({ $and: [{ age: { $gte: 18 } }, { status: \"active\" }] })\n\n// OR\ndb.users.find({ $or: [{ age: { $lt: 18 } }, { age: { $gt: 65 } }] })\n\n// NOT\ndb.users.find({ age: { $not: { $lt: 18 } } })\n\n// NOR (not or)\ndb.users.find({ $nor: [{ status: \"banned\" }, { age: { $lt: 13 } }] })\n```\n\n**Element Operators**:\n```javascript\n// Field exists\ndb.users.find({ email: { $exists: true } })\n\n// Field doesn't exist\ndb.users.find({ phone: { $exists: false } })\n\n// Field type check\ndb.users.find({ age: { $type: \"number\" } })\ndb.users.find({ tags: { $type: \"array\" } })\n```\n\n**Array Operators**:\n```javascript\n// All elements match\ndb.users.find({ tags: { $all: [\"mongodb\", \"database\"] } })\n\n// Array size\ndb.users.find({ tags: { $size: 3 } })\n\n// Element match\ndb.users.find({ \"addresses.city\": \"New York\" })\n```\n\nThese operators can be combined to create sophisticated queries that precisely filter data based on multiple conditions and criteria.",
      "keywords": ["query operators", "comparison operators", "logical operators", "element operators", "$gt", "$lt", "$in", "$exists", "mongodb filtering"],
      "difficulty": "easy"
    },
    {
      "id": 13005,
      "tag": "mongodb",
      "question": "What is indexing in MongoDB? Explain different types of indexes and their benefits for query performance.",
      "answer": "Indexing in MongoDB improves query performance by creating data structures that enable faster document retrieval. Without indexes, MongoDB performs collection scans, examining every document to find matches.\n\n**How Indexes Work**:\nIndexes store a small portion of data in an ordered, searchable structure. They contain references to documents in the collection, allowing MongoDB to quickly locate specific documents.\n\n**Types of Indexes**:\n\n**1. Single Field Index**:\n```javascript\n// Create index on single field\ndb.users.createIndex({ email: 1 })  // 1 for ascending, -1 for descending\n\n// Query benefits from index\ndb.users.find({ email: \"john@example.com\" })\n```\n\n**2. Compound Index**:\n```javascript\n// Create index on multiple fields\ndb.users.createIndex({ status: 1, age: -1 })\n\n// Efficient for queries on both fields\ndb.users.find({ status: \"active\", age: { $gte: 25 } })\n```\n\n**3. Multikey Index**:\n```javascript\n// Automatically created for array fields\ndb.users.createIndex({ tags: 1 })\n\n// Supports queries on array elements\ndb.users.find({ tags: \"mongodb\" })\n```\n\n**4. Text Index**:\n```javascript\n// Create text index for full-text search\ndb.articles.createIndex({ title: \"text\", content: \"text\" })\n\n// Search text\ndb.articles.find({ $text: { $search: \"mongodb tutorial\" } })\n```\n\n**Index Management**:\n```javascript\n// List all indexes\ndb.users.getIndexes()\n\n// Drop index\ndb.users.dropIndex({ email: 1 })\n\n// Index statistics\ndb.users.stats()\n```\n\n**Benefits**:\n- **Faster Queries**: Dramatically reduce query execution time\n- **Efficient Sorting**: Speed up sort operations\n- **Unique Constraints**: Ensure data uniqueness\n- **Reduced Resource Usage**: Lower CPU and memory consumption\n\n**Considerations**:\n- Indexes consume storage space\n- They slow down write operations slightly\n- Choose indexes based on query patterns\n- MongoDB automatically creates an index on `_id` field\n\nProper indexing strategy is crucial for optimal MongoDB performance in production environments.",
      "keywords": ["indexing", "single field index", "compound index", "multikey index", "text index", "query performance", "createIndex"],
      "difficulty": "easy"
    },
    {
      "id": 13006,
      "tag": "mongodb",
      "question": "Explain MongoDB's aggregation framework and pipelines. How do you perform complex data processing and analysis?",
      "answer": "MongoDB's aggregation framework provides a powerful way to process and analyze data through a pipeline of operations. It's similar to SQL's GROUP BY and JOIN operations but more flexible and expressive.\n\n**Aggregation Pipeline Concept**:\nAn aggregation pipeline consists of stages that process documents sequentially. Each stage transforms the documents and passes the results to the next stage.\n\n**Common Pipeline Stages**:\n\n**$match**: Filter documents\n```javascript\ndb.orders.aggregate([\n  { $match: { status: \"completed\", amount: { $gte: 100 } } }\n])\n```\n\n**$group**: Group documents and perform operations\n```javascript\ndb.orders.aggregate([\n  {\n    $group: {\n      _id: \"$customerId\",\n      totalAmount: { $sum: \"$amount\" },\n      orderCount: { $sum: 1 },\n      avgAmount: { $avg: \"$amount\" }\n    }\n  }\n])\n```\n\n**$project**: Select and transform fields\n```javascript\ndb.users.aggregate([\n  {\n    $project: {\n      fullName: { $concat: [\"$firstName\", \" \", \"$lastName\"] },\n      age: 1,\n      _id: 0\n    }\n  }\n])\n```\n\n**$sort**: Sort documents\n```javascript\ndb.products.aggregate([\n  { $sort: { price: -1, name: 1 } }\n])\n```\n\n**$lookup**: Join collections (like SQL JOIN)\n```javascript\ndb.orders.aggregate([\n  {\n    $lookup: {\n      from: \"customers\",\n      localField: \"customerId\",\n      foreignField: \"_id\",\n      as: \"customerInfo\"\n    }\n  }\n])\n```\n\n**Complex Example**:\n```javascript\ndb.orders.aggregate([\n  { $match: { orderDate: { $gte: new Date(\"2023-01-01\") } } },\n  {\n    $group: {\n      _id: { \n        month: { $month: \"$orderDate\" },\n        category: \"$category\"\n      },\n      totalRevenue: { $sum: \"$amount\" },\n      orderCount: { $sum: 1 }\n    }\n  },\n  { $sort: { \"_id.month\": 1, totalRevenue: -1 } },\n  {\n    $project: {\n      month: \"$_id.month\",\n      category: \"$_id.category\",\n      revenue: \"$totalRevenue\",\n      avgOrderValue: { $divide: [\"$totalRevenue\", \"$orderCount\"] }\n    }\n  }\n])\n```\n\n**Benefits**:\n- **Server-side Processing**: Reduces data transfer\n- **Memory Efficiency**: Processes data in chunks\n- **Complex Analytics**: Supports sophisticated data transformations\n- **Performance**: Often faster than client-side processing\n\nThe aggregation framework is essential for business intelligence, reporting, and real-time analytics in MongoDB applications.",
      "keywords": ["aggregation framework", "pipeline", "$match", "$group", "$project", "$lookup", "$sort", "data analysis", "mongodb aggregation"],
      "difficulty": "medium"
    },
    {
      "id": 13007,
      "tag": "mongodb",
      "question": "What is MongoDB replication and replica sets? How do they provide high availability and data redundancy?",
      "answer": "MongoDB replication is the process of synchronizing data across multiple servers to provide redundancy, high availability, and read scalability. Replica sets are MongoDB's primary replication mechanism.\n\n**Replica Set Architecture**:\nA replica set consists of multiple MongoDB instances (nodes) that maintain identical copies of data:\n\n- **Primary Node**: Receives all write operations and replicates changes to secondaries\n- **Secondary Nodes**: Maintain copies of primary's data and can serve read operations\n- **Arbiter** (optional): Participates in elections but doesn't store data\n\n**Replica Set Configuration**:\n```javascript\n// Initialize replica set\nrs.initiate({\n  _id: \"myReplicaSet\",\n  members: [\n    { _id: 0, host: \"mongodb1.example.com:27017\" },\n    { _id: 1, host: \"mongodb2.example.com:27017\" },\n    { _id: 2, host: \"mongodb3.example.com:27017\" }\n  ]\n})\n\n// Check replica set status\nrs.status()\n\n// Add member\nrs.add(\"mongodb4.example.com:27017\")\n```\n\n**Automatic Failover Process**:\n1. **Detection**: Secondaries detect primary failure (heartbeat timeout)\n2. **Election**: Eligible secondaries vote for new primary\n3. **Promotion**: Node with majority votes becomes primary\n4. **Recovery**: Original primary rejoins as secondary when available\n\n**Read Preferences**:\n```javascript\n// Read from primary only (default)\ndb.users.find().readPref(\"primary\")\n\n// Read from secondary if available\ndb.users.find().readPref(\"secondary\")\n\n// Read from nearest node\ndb.users.find().readPref(\"nearest\")\n```\n\n**Write Concerns**:\n```javascript\n// Wait for acknowledgment from majority of nodes\ndb.users.insertOne(\n  { name: \"John\" },\n  { writeConcern: { w: \"majority\", j: true } }\n)\n```\n\n**Oplog (Operations Log)**:\nMongoDB uses the oplog to track all changes and replicate them to secondaries:\n```javascript\n// View oplog entries\ndb.oplog.rs.find().sort({ $natural: -1 }).limit(5)\n```\n\n**Benefits**:\n- **High Availability**: Automatic failover ensures minimal downtime\n- **Data Safety**: Multiple copies prevent data loss\n- **Read Scalability**: Distribute read operations across secondaries\n- **Disaster Recovery**: Geographic distribution of replica set members\n- **Maintenance Windows**: Perform maintenance without downtime\n\n**Best Practices**:\n- Use odd number of voting members (minimum 3)\n- Deploy across different data centers\n- Monitor replica lag and health\n- Configure appropriate write concerns for data consistency\n\nReplica sets are fundamental for production MongoDB deployments requiring reliability and availability.",
      "keywords": ["replication", "replica sets", "primary node", "secondary node", "failover", "high availability", "oplog", "write concern", "read preference"],
      "difficulty": "medium"
    },
    {
      "id": 13008,
      "tag": "mongodb",
      "question": "Describe data modeling patterns in MongoDB. How do you design schemas for embedded vs. referenced relationships?",
      "answer": "MongoDB data modeling requires different approaches compared to relational databases due to its document-oriented nature. The key decision is choosing between embedding and referencing related data.\n\n**Embedded Data Model**:\nStore related data within the same document using nested objects or arrays.\n\n```javascript\n// User with embedded address\n{\n  _id: ObjectId(\"...\"),\n  name: \"John Doe\",\n  email: \"john@example.com\",\n  address: {\n    street: \"123 Main St\",\n    city: \"New York\",\n    zipcode: \"10001\"\n  },\n  orders: [\n    {\n      orderId: \"ORD001\",\n      date: ISODate(\"2023-01-15\"),\n      items: [\"laptop\", \"mouse\"],\n      total: 1200\n    }\n  ]\n}\n```\n\n**When to Use Embedding**:\n- One-to-one relationships\n- One-to-few relationships (limited array size)\n- Data accessed together frequently\n- Nested data doesn't change often\n- Strong ownership relationship\n\n**Referenced Data Model**:\nStore related data in separate collections with references.\n\n```javascript\n// User document\n{\n  _id: ObjectId(\"507f1f77bcf86cd799439011\"),\n  name: \"John Doe\",\n  email: \"john@example.com\"\n}\n\n// Order documents\n{\n  _id: ObjectId(\"507f1f77bcf86cd799439012\"),\n  userId: ObjectId(\"507f1f77bcf86cd799439011\"),\n  date: ISODate(\"2023-01-15\"),\n  items: [\"laptop\", \"mouse\"],\n  total: 1200\n}\n```\n\n**When to Use References**:\n- One-to-many relationships with large arrays\n- Many-to-many relationships\n- Data accessed independently\n- Referenced data changes frequently\n- Document size limitations (16MB max)\n\n**Common Design Patterns**:\n\n**1. Subset Pattern**: Embed frequently accessed fields, reference complete data\n```javascript\n// Product catalog with embedded summary\n{\n  _id: ObjectId(\"...\"),\n  title: \"MongoDB Guide\",\n  summary: \"Learn MongoDB basics...\",\n  author: {\n    name: \"Jane Smith\",\n    id: ObjectId(\"...\")  // Reference to full author document\n  },\n  reviews: [  // Embed recent reviews, reference full reviews collection\n    { rating: 5, comment: \"Great book!\", date: ISODate(\"...\") }\n  ]\n}\n```\n\n**2. Bucket Pattern**: Group related documents to optimize queries\n```javascript\n// Time-series data bucketed by hour\n{\n  _id: ObjectId(\"...\"),\n  sensor_id: \"temp_001\",\n  timestamp: ISODate(\"2023-01-15T10:00:00Z\"),\n  readings: [\n    { time: ISODate(\"2023-01-15T10:05:00Z\"), temp: 23.5 },\n    { time: ISODate(\"2023-01-15T10:10:00Z\"), temp: 24.1 }\n  ]\n}\n```\n\n**Schema Design Considerations**:\n- **Query Patterns**: Design based on how data will be accessed\n- **Update Frequency**: Embed static data, reference frequently changing data\n- **Data Growth**: Consider document size limits and array growth\n- **Atomicity**: Embedded updates are atomic, cross-document updates aren't\n- **Consistency**: Balance between flexibility and data integrity\n\nEffective MongoDB schema design requires understanding your application's access patterns and choosing the appropriate modeling approach for each relationship.",
      "keywords": ["data modeling", "embedded documents", "referenced documents", "schema design", "one-to-many", "many-to-many", "subset pattern", "bucket pattern"],
      "difficulty": "medium"
    },
    {
      "id": 13009,
      "tag": "mongodb",
      "question": "Explain MongoDB transactions and ACID properties. How do single-document and multi-document transactions work?",
      "answer": "MongoDB provides ACID transactions at both single-document and multi-document levels, ensuring data consistency and integrity in database operations.\n\n**ACID Properties in MongoDB**:\n\n**Atomicity**: Operations either complete entirely or not at all\n**Consistency**: Database remains in valid state before and after transactions\n**Isolation**: Concurrent transactions don't interfere with each other\n**Durability**: Committed changes persist even after system failures\n\n**Single-Document Transactions**:\nMongoDB guarantees ACID properties for operations on single documents by default:\n\n```javascript\n// Atomic update of multiple fields\ndb.accounts.updateOne(\n  { _id: \"account123\" },\n  {\n    $inc: { balance: -100 },\n    $push: { transactions: {\n      type: \"withdrawal\",\n      amount: 100,\n      date: new Date()\n    }}\n  }\n)\n```\n\n**Multi-Document Transactions**:\nFor operations spanning multiple documents or collections, MongoDB provides explicit transactions:\n\n```javascript\n// Start session and transaction\nconst session = db.getMongo().startSession()\nsession.startTransaction()\n\ntry {\n  // Transfer money between accounts\n  db.accounts.updateOne(\n    { _id: \"fromAccount\" },\n    { $inc: { balance: -100 } },\n    { session: session }\n  )\n  \n  db.accounts.updateOne(\n    { _id: \"toAccount\" },\n    { $inc: { balance: 100 } },\n    { session: session }\n  )\n  \n  // Log transaction\n  db.transactions.insertOne({\n    from: \"fromAccount\",\n    to: \"toAccount\",\n    amount: 100,\n    timestamp: new Date()\n  }, { session: session })\n  \n  // Commit transaction\n  session.commitTransaction()\n} catch (error) {\n  // Rollback on error\n  session.abortTransaction()\n  throw error\n} finally {\n  session.endSession()\n}\n```\n\n**Transaction Isolation Levels**:\nMongoDB uses snapshot isolation, providing consistent views of data:\n\n```javascript\n// Read concern for transactions\nsession.startTransaction({\n  readConcern: { level: \"snapshot\" },\n  writeConcern: { w: \"majority\" }\n})\n```\n\n**Best Practices**:\n\n1. **Keep Transactions Short**: Minimize lock duration\n2. **Limit Transaction Size**: Avoid large batch operations\n3. **Handle Retries**: Implement retry logic for transient failures\n4. **Use Appropriate Write Concerns**: Ensure durability requirements\n\n**Node.js Example**:\n```javascript\nconst { MongoClient } = require('mongodb')\n\nasync function transferMoney(fromId, toId, amount) {\n  const session = client.startSession()\n  \n  try {\n    await session.withTransaction(async () => {\n      await db.collection('accounts').updateOne(\n        { _id: fromId },\n        { $inc: { balance: -amount } },\n        { session }\n      )\n      \n      await db.collection('accounts').updateOne(\n        { _id: toId },\n        { $inc: { balance: amount } },\n        { session }\n      )\n    })\n  } finally {\n    await session.endSession()\n  }\n}\n```\n\n**Limitations**:\n- Transactions have performance overhead\n- Not supported on standalone instances (require replica sets)\n- Limited to 16MB of data modifications\n- 60-second default timeout limit\n\n**When to Use Transactions**:\n- Financial operations requiring consistency\n- Multi-step processes that must complete together\n- Operations where partial completion would corrupt data integrity\n\nMongoDB's transaction support enables ACID compliance while maintaining the flexibility of document-based storage.",
      "keywords": ["transactions", "acid properties", "multi-document transactions", "atomicity", "consistency", "isolation", "durability", "session", "commitTransaction"],
      "difficulty": "medium"
    },
    {
      "id": 13010,
      "tag": "mongodb",
      "question": "What is MongoDB schema validation and document validation? How do you implement validation rules to ensure data integrity?",
      "answer": "MongoDB schema validation allows you to enforce data structure and validation rules on documents within collections, ensuring data integrity and consistency while maintaining flexibility.\n\n**Document Validation Basics**:\nValidation rules are defined using JSON Schema or MongoDB's query operators when creating or modifying collections.\n\n**Creating Collection with Validation**:\n```javascript\ndb.createCollection(\"users\", {\n  validator: {\n    $jsonSchema: {\n      bsonType: \"object\",\n      required: [\"name\", \"email\", \"age\"],\n      properties: {\n        name: {\n          bsonType: \"string\",\n          description: \"must be a string and is required\"\n        },\n        email: {\n          bsonType: \"string\",\n          pattern: \"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$\",\n          description: \"must be a valid email address\"\n        },\n        age: {\n          bsonType: \"int\",\n          minimum: 0,\n          maximum: 120,\n          description: \"must be an integer between 0 and 120\"\n        },\n        status: {\n          enum: [\"active\", \"inactive\", \"pending\"],\n          description: \"must be one of the allowed values\"\n        }\n      }\n    }\n  },\n  validationLevel: \"strict\",\n  validationAction: \"error\"\n})\n```\n\n**Validation Levels**:\n- **strict**: Apply validation to all inserts and updates (default)\n- **moderate**: Apply validation to inserts and updates of valid documents\n\n**Validation Actions**:\n- **error**: Reject invalid documents (default)\n- **warn**: Log warnings but allow invalid documents\n\n**Adding Validation to Existing Collection**:\n```javascript\ndb.runCommand({\n  collMod: \"products\",\n  validator: {\n    $jsonSchema: {\n      bsonType: \"object\",\n      required: [\"name\", \"price\", \"category\"],\n      properties: {\n        name: { bsonType: \"string\", minLength: 1 },\n        price: { bsonType: \"number\", minimum: 0 },\n        category: { enum: [\"electronics\", \"clothing\", \"books\"] },\n        tags: {\n          bsonType: \"array\",\n          items: { bsonType: \"string\" },\n          maxItems: 10\n        }\n      }\n    }\n  }\n})\n```\n\n**Query Operator Validation**:\n```javascript\n// Alternative validation using MongoDB operators\ndb.createCollection(\"orders\", {\n  validator: {\n    $and: [\n      { amount: { $type: \"number\", $gt: 0 } },\n      { status: { $in: [\"pending\", \"processing\", \"completed\"] } },\n      { customerEmail: { $regex: /@.*\\../ } }\n    ]\n  }\n})\n```\n\n**Benefits**:\n- **Data Quality**: Ensures consistent document structure\n- **Application Safety**: Prevents invalid data from corrupting applications\n- **Documentation**: Schema serves as living documentation\n- **Flexibility**: Can be updated as requirements evolve\n\nValidation rules help maintain data quality while preserving MongoDB's schemaless flexibility, making them essential for production applications.",
      "keywords": ["schema validation", "document validation", "json schema", "data integrity", "validation rules", "collMod", "bsonType", "validation levels"],
      "difficulty": "easy"
    },
    {
      "id": 13011,
      "tag": "mongodb",
      "question": "Explain GridFS in MongoDB. How do you store and retrieve large files that exceed the document size limit?",
      "answer": "GridFS is MongoDB's specification for storing and retrieving large files that exceed the 16MB document size limit. It automatically divides large files into smaller chunks and stores metadata separately.\n\n**GridFS Architecture**:\nGridFS uses two collections:\n- **fs.files**: Stores file metadata (filename, size, upload date, etc.)\n- **fs.chunks**: Stores binary data in 255KB chunks\n\n**Storage Process**:\n1. Large file is divided into 255KB chunks\n2. Each chunk is stored as a separate document in fs.chunks\n3. File metadata is stored in fs.files with references to chunks\n\n**Using GridFS with MongoDB Shell**:\n```javascript\n// Store file using mongofiles utility\n// mongofiles -d mydb put largefile.pdf\n\n// Query files collection\ndb.fs.files.find()\n\n// Query chunks collection\ndb.fs.chunks.find({ files_id: ObjectId(\"...\") })\n```\n\n**Node.js GridFS Example**:\n```javascript\nconst { MongoClient, GridFSBucket } = require('mongodb')\nconst fs = require('fs')\n\nasync function uploadFile() {\n  const client = new MongoClient('mongodb://localhost:27017')\n  const db = client.db('myapp')\n  const bucket = new GridFSBucket(db, { bucketName: 'uploads' })\n  \n  // Upload file\n  const uploadStream = bucket.openUploadStream('example.pdf', {\n    metadata: {\n      description: 'Sample PDF document',\n      uploadedBy: 'user123'\n    }\n  })\n  \n  fs.createReadStream('./example.pdf')\n    .pipe(uploadStream)\n    .on('error', (error) => console.error('Upload failed:', error))\n    .on('finish', () => console.log('Upload successful'))\n}\n\nasync function downloadFile(fileId) {\n  const bucket = new GridFSBucket(db, { bucketName: 'uploads' })\n  \n  // Download file\n  const downloadStream = bucket.openDownloadStream(fileId)\n  downloadStream.pipe(fs.createWriteStream('./downloaded.pdf'))\n}\n\nasync function findFiles() {\n  const bucket = new GridFSBucket(db, { bucketName: 'uploads' })\n  \n  // Find files by metadata\n  const files = await bucket.find({\n    'metadata.uploadedBy': 'user123'\n  }).toArray()\n  \n  return files\n}\n```\n\n**GridFS Operations**:\n```javascript\n// Delete file\nawait bucket.delete(fileId)\n\n// Rename file\nawait bucket.rename(fileId, 'newname.pdf')\n\n// Stream file directly to HTTP response\napp.get('/files/:id', (req, res) => {\n  const downloadStream = bucket.openDownloadStream(ObjectId(req.params.id))\n  downloadStream.pipe(res)\n})\n```\n\n**Custom Bucket Configuration**:\n```javascript\nconst bucket = new GridFSBucket(db, {\n  bucketName: 'images',\n  chunkSizeBytes: 1024 * 1024,  // 1MB chunks\n  writeConcern: { w: 'majority' },\n  readPreference: 'secondary'\n})\n```\n\n**Use Cases**:\n- **Media Files**: Images, videos, audio files\n- **Documents**: PDFs, Word documents, presentations\n- **Backups**: Database dumps, log archives\n- **User Uploads**: Profile pictures, attachments\n\n**Advantages**:\n- **No Size Limit**: Store files larger than 16MB\n- **Atomic Operations**: File operations are atomic\n- **Efficient Streaming**: Stream large files without loading entirely in memory\n- **Replication**: Files replicated with other MongoDB data\n- **Sharding**: GridFS collections can be sharded\n\n**Considerations**:\n- **Performance**: Slightly slower than dedicated file systems\n- **Overhead**: Additional metadata storage\n- **Backup Complexity**: Files spread across multiple documents\n\nGridFS is ideal when you need to store large files alongside your MongoDB data with atomic operations and replication benefits.",
      "keywords": ["gridfs", "large files", "file storage", "fs.files", "fs.chunks", "gridfs bucket", "file upload", "file streaming"],
      "difficulty": "easy"
    },
    {
      "id": 13012,
      "tag": "mongodb",
      "question": "What are MongoDB security features and authentication mechanisms? How do you implement user authentication and authorization?",
      "answer": "MongoDB provides comprehensive security features including authentication, authorization, encryption, and auditing to protect data and control access.\n\n**Authentication Mechanisms**:\n\n**1. SCRAM (Default)**:\nSalted Challenge Response Authentication Mechanism\n```javascript\n// Enable authentication\nuse admin\ndb.createUser({\n  user: \"admin\",\n  pwd: \"securePassword123\",\n  roles: [{ role: \"userAdminAnyDatabase\", db: \"admin\" }]\n})\n\n// Start mongod with authentication\n// mongod --auth --dbpath /data/db\n```\n\n**2. x.509 Certificate Authentication**:\n```javascript\n// Create user with x.509 certificate\ndb.getSiblingDB(\"$external\").createUser({\n  user: \"CN=client,OU=IT,O=Example Corp,L=NYC,ST=NY,C=US\",\n  roles: [{ role: \"readWrite\", db: \"myapp\" }]\n})\n```\n\n**3. LDAP Authentication**:\n```javascript\n// LDAP user creation\ndb.getSiblingDB(\"$external\").createUser({\n  user: \"alice@EXAMPLE.COM\",\n  roles: [{ role: \"read\", db: \"inventory\" }]\n})\n```\n\n**Role-Based Access Control (RBAC)**:\n\n**Built-in Roles**:\n```javascript\n// Database-level roles\ndb.createUser({\n  user: \"developer\",\n  pwd: \"devPassword\",\n  roles: [\n    { role: \"readWrite\", db: \"myapp\" },\n    { role: \"read\", db: \"analytics\" }\n  ]\n})\n\n// Collection-level privileges\ndb.createUser({\n  user: \"analyst\",\n  pwd: \"analystPassword\",\n  roles: [{\n    role: \"read\",\n    db: \"sales\",\n    collection: \"orders\"\n  }]\n})\n```\n\n**Custom Roles**:\n```javascript\n// Create custom role\ndb.createRole({\n  role: \"salesManager\",\n  privileges: [\n    {\n      resource: { db: \"sales\", collection: \"orders\" },\n      actions: [\"find\", \"update\", \"insert\"]\n    },\n    {\n      resource: { db: \"sales\", collection: \"customers\" },\n      actions: [\"find\"]\n    }\n  ],\n  roles: []  // No inherited roles\n})\n\n// Assign custom role to user\ndb.createUser({\n  user: \"john\",\n  pwd: \"johnPassword\",\n  roles: [\"salesManager\"]\n})\n```\n\n**Encryption**:\n\n**1. Encryption at Rest**:\n```bash\n# Enable WiredTiger encryption\nmongod --enableEncryption \\\n       --encryptionKeyFile /path/to/keyfile \\\n       --dbpath /data/db\n```\n\n**2. Transport Layer Security (TLS)**:\n```bash\n# Enable TLS\nmongod --tlsMode requireTLS \\\n       --tlsCertificateKeyFile /path/to/server.pem \\\n       --tlsCAFile /path/to/ca.pem\n```\n\n**Client-Side Field Level Encryption**:\n```javascript\nconst { MongoClient, ClientEncryption } = require('mongodb')\n\nconst client = new MongoClient(uri, {\n  autoEncryption: {\n    keyVaultNamespace: 'encryption.__keyVault',\n    kmsProviders: {\n      local: {\n        key: masterKey\n      }\n    },\n    schemaMap: {\n      'myapp.users': {\n        bsonType: 'object',\n        properties: {\n          ssn: {\n            encrypt: {\n              keyId: dataKey,\n              bsonType: 'string',\n              algorithm: 'AEAD_AES_256_CBC_HMAC_SHA_512-Deterministic'\n            }\n          }\n        }\n      }\n    }\n  }\n})\n```\n\n**Security Best Practices**:\n\n1. **Network Security**:\n   - Bind to specific IP addresses\n   - Use firewalls and VPNs\n   - Enable TLS for all connections\n\n2. **User Management**:\n   - Follow principle of least privilege\n   - Regular password rotation\n   - Remove unused accounts\n\n3. **Auditing**:\n```javascript\n// Enable auditing\nmongod --auditDestination file \\\n       --auditFormat JSON \\\n       --auditPath /var/log/mongodb/audit.json\n```\n\n**Connection Security**:\n```javascript\n// Secure connection string\nconst uri = \"mongodb://username:password@host:port/database?authSource=admin&ssl=true\"\n```\n\nMongoDB's security features provide enterprise-grade protection for sensitive data and ensure compliance with security requirements.",
      "keywords": ["mongodb security", "authentication", "authorization", "scram", "rbac", "roles", "encryption", "tls", "user management", "field level encryption"],
      "difficulty": "easy"
    },
    {
      "id": 13013,
      "tag": "mongodb",
      "question": "How do you perform backup and restore operations in MongoDB? Explain different backup strategies and tools.",
      "answer": "MongoDB provides several backup and restore strategies to protect data and ensure business continuity. The choice depends on deployment type, data size, and recovery requirements.\n\n**Backup Strategies**:\n\n**1. mongodump and mongorestore**:\nLogical backup tools that export and import BSON data.\n\n```bash\n# Full database backup\nmongodump --host localhost:27017 --db myapp --out /backup/myapp\n\n# Specific collection backup\nmongodump --host localhost:27017 --db myapp --collection users --out /backup/\n\n# Compressed backup\nmongodump --host localhost:27017 --db myapp --gzip --out /backup/\n\n# Backup with authentication\nmongodump --host localhost:27017 --username admin --password --authenticationDatabase admin --db myapp --out /backup/\n\n# Restore database\nmongorestore --host localhost:27017 --db myapp /backup/myapp/\n\n# Restore with drop existing\nmongorestore --host localhost:27017 --db myapp --drop /backup/myapp/\n```\n\n**2. File System Snapshots**:\nPhysical backup of data files (faster for large datasets).\n\n```bash\n# Stop writes and create consistent snapshot\ndb.fsyncLock()\n\n# Create file system snapshot (example with LVM)\nsudo lvcreate --size 10G --snapshot --name mongodb-snapshot /dev/vg0/mongodb\n\n# Resume writes\ndb.fsyncUnlock()\n\n# Mount snapshot and copy files\nsudo mkdir /mnt/mongodb-snapshot\nsudo mount /dev/vg0/mongodb-snapshot /mnt/mongodb-snapshot\ncp -R /mnt/mongodb-snapshot/* /backup/mongodb-files/\n```\n\n**3. MongoDB Cloud Manager/Ops Manager**:\nEnterprise backup solution with continuous backup.\n\n```javascript\n// Configure continuous backup\n{\n  \"clusterId\": \"cluster123\",\n  \"snapshotIntervalHours\": 6,\n  \"snapshotRetentionDays\": 30,\n  \"pointInTimeWindowHours\": 24\n}\n```\n\n**Replica Set Backup**:\n\n```bash\n# Backup from secondary to reduce primary load\nmongodump --host secondary.example.com:27017 --readPreference secondary --db myapp --out /backup/\n\n# Backup with oplog for point-in-time recovery\nmongodump --host localhost:27017 --oplog --out /backup/myapp-$(date +%Y%m%d)/\n\n# Restore with oplog replay\nmongorestore --host localhost:27017 --oplogReplay /backup/myapp-20231201/\n```\n\n**Sharded Cluster Backup**:\n\n```bash\n# Stop balancer before backup\nsh.stopBalancer()\n\n# Backup config servers\nmongodump --host configserver1:27019 --out /backup/config/\n\n# Backup each shard\nmongodump --host shard1:27018 --out /backup/shard1/\nmongodump --host shard2:27018 --out /backup/shard2/\n\n# Restart balancer\nsh.startBalancer()\n```\n\n**Automated Backup Script**:\n\n```bash\n#!/bin/bash\n\n# MongoDB backup script\nDATE=$(date +%Y%m%d_%H%M%S)\nBACKUP_DIR=\"/backup/mongodb-$DATE\"\nLOG_FILE=\"/var/log/mongodb-backup.log\"\n\necho \"$(date): Starting MongoDB backup\" >> $LOG_FILE\n\n# Create backup directory\nmkdir -p $BACKUP_DIR\n\n# Perform backup\nmongodump --host localhost:27017 \\\n          --username backup_user \\\n          --password $MONGO_BACKUP_PASSWORD \\\n          --authenticationDatabase admin \\\n          --gzip \\\n          --out $BACKUP_DIR\n\nif [ $? -eq 0 ]; then\n    echo \"$(date): Backup completed successfully\" >> $LOG_FILE\n    \n    # Compress backup\n    tar -czf \"$BACKUP_DIR.tar.gz\" -C /backup \"mongodb-$DATE\"\n    rm -rf $BACKUP_DIR\n    \n    # Upload to cloud storage (example with AWS S3)\n    aws s3 cp \"$BACKUP_DIR.tar.gz\" s3://my-mongo-backups/\n    \n    # Clean old backups (keep last 7 days)\n    find /backup -name \"mongodb-*.tar.gz\" -mtime +7 -delete\nelse\n    echo \"$(date): Backup failed\" >> $LOG_FILE\n    exit 1\nfi\n```\n\n**Point-in-Time Recovery**:\n\n```bash\n# Backup with oplog\nmongodump --oplog --out /backup/pit-backup/\n\n# Restore to specific timestamp\nmongorestore --oplogReplay --oplogLimit 1640995200:1 /backup/pit-backup/\n```\n\n**Best Practices**:\n\n1. **Regular Testing**: Regularly test restore procedures\n2. **Multiple Locations**: Store backups in different locations\n3. **Monitoring**: Monitor backup success and failures\n4. **Encryption**: Encrypt backup files for sensitive data\n5. **Documentation**: Maintain backup and recovery procedures\n6. **Automation**: Automate backup processes with scheduling\n\n**Backup Strategy Selection**:\n- **Small databases**: mongodump/mongorestore\n- **Large databases**: File system snapshots\n- **Enterprise**: MongoDB Cloud Manager/Ops Manager\n- **High availability**: Replica set with continuous backup\n\nChoose the appropriate backup strategy based on your data size, recovery time objectives (RTO), and recovery point objectives (RPO).",
      "keywords": ["backup", "restore", "mongodump", "mongorestore", "file system snapshots", "oplog", "point-in-time recovery", "replica set backup", "automated backup"],
      "difficulty": "easy"
    },
    {
      "id": 13014,
      "tag": "mongodb",
      "question": "What is MongoDB Compass and what administrative tools are available? How do you use them for database management and monitoring?",
      "answer": "MongoDB Compass is the official graphical user interface (GUI) for MongoDB, providing an intuitive way to explore, analyze, and manage MongoDB databases without command-line knowledge.\n\n**MongoDB Compass Features**:\n\n**1. Visual Data Explorer**:\n- Browse collections and documents\n- Real-time server statistics\n- Visual query builder\n- Schema analysis and visualization\n\n**2. Query Performance**:\n- Query profiler and performance insights\n- Index recommendations\n- Explain plan visualization\n- Query optimization suggestions\n\n**3. Schema Analysis**:\n```javascript\n// Compass automatically analyzes schema patterns\n// Shows field types, frequency, and sample values\n{\n  \"_id\": \"ObjectId (100%)\",\n  \"name\": \"String (98%), Null (2%)\",\n  \"age\": \"Number (95%), String (5%)\",\n  \"tags\": \"Array (80%), Null (20%)\"\n}\n```\n\n**Administrative Tools and Features**:\n\n**1. MongoDB Shell (mongosh)**:\n```bash\n# Connect to database\nmongosh \"mongodb://localhost:27017/myapp\"\n\n# Administrative commands\ndb.adminCommand(\"serverStatus\")\ndb.stats()\ndb.runCommand({collStats: \"users\"})\n\n# User management\ndb.createUser({user: \"admin\", pwd: \"password\", roles: [\"dbAdmin\"]})\ndb.getUsers()\n```\n\n**2. MongoDB Database Tools**:\n```bash\n# Data import/export\nmongoimport --db myapp --collection users --file users.json\nmongoexport --db myapp --collection users --out users.json\n\n# Statistics\nmongostat --host localhost:27017\nmongotop --host localhost:27017\n\n# File operations\nmongofiles --db myapp put largefile.pdf\nmongofiles --db myapp list\n```\n\n**3. Profiler and Monitoring**:\n```javascript\n// Enable profiler\ndb.setProfilingLevel(2)  // Profile all operations\ndb.setProfilingLevel(1, { slowms: 100 })  // Profile slow operations\n\n// Query profiler collection\ndb.system.profile.find().sort({ts: -1}).limit(5)\n\n// Server status monitoring\ndb.serverStatus()\ndb.stats().dataSize\ndb.runCommand({top: 1})\n```\n\n**Compass Operations**:\n\n**Query Builder Interface**:\n```javascript\n// Visual query construction in Compass\n// Filter: { \"age\": { \"$gte\": 25 } }\n// Project: { \"name\": 1, \"email\": 1 }\n// Sort: { \"name\": 1 }\n// Limit: 10\n```\n\n**Index Management**:\n- Visual index creation and analysis\n- Performance impact assessment\n- Index usage statistics\n- Unused index identification\n\n**4. MongoDB Atlas (Cloud)**:\n```javascript\n// Atlas features\n{\n  \"monitoring\": {\n    \"realTimeMetrics\": true,\n    \"alerting\": true,\n    \"performanceAdvisor\": true\n  },\n  \"backup\": {\n    \"continuousBackup\": true,\n    \"pointInTimeRecovery\": true\n  },\n  \"security\": {\n    \"networkAccess\": \"IP whitelist\",\n    \"databaseAccess\": \"RBAC\",\n    \"encryption\": \"at rest and in transit\"\n  }\n}\n```\n\n**Monitoring with Third-Party Tools**:\n\n**Prometheus + Grafana**:\n```yaml\n# MongoDB exporter configuration\n- job_name: 'mongodb'\n  static_configs:\n    - targets: ['localhost:9216']\n  metrics_path: /metrics\n```\n\n**MongoDB Enterprise Tools**:\n\n**1. Ops Manager**:\n- Automated deployment and scaling\n- Continuous backup\n- Performance monitoring\n- Alerting and automation\n\n**2. MongoDB Connector for BI**:\n```sql\n-- Query MongoDB with SQL through BI connector\nSELECT name, AVG(age) \nFROM users \nWHERE status = 'active' \nGROUP BY department\n```\n\n**Administration Best Practices**:\n\n1. **Regular Monitoring**:\n   - Monitor server metrics\n   - Track slow queries\n   - Monitor index usage\n   - Check replication lag\n\n2. **Performance Optimization**:\n   - Use explain() for query analysis\n   - Create appropriate indexes\n   - Monitor connection pools\n   - Optimize document structure\n\n3. **Security Management**:\n   - Regular user access reviews\n   - Monitor authentication failures\n   - Update security configurations\n   - Audit database access\n\n**Compass Installation and Connection**:\n```bash\n# Download and install MongoDB Compass\n# Connection string format\nmongodb://username:password@host:port/database?options\n\n# SSL connection\nmongodb://user:pass@host:port/db?ssl=true&authSource=admin\n```\n\n**Key Benefits**:\n- **User-Friendly**: GUI interface for non-technical users\n- **Comprehensive**: All major operations in one tool\n- **Real-Time**: Live monitoring and statistics\n- **Cross-Platform**: Available on Windows, macOS, and Linux\n- **Integration**: Works with MongoDB Atlas and on-premises deployments\n\nMongoDB Compass and associated tools provide comprehensive database management capabilities, making MongoDB administration accessible and efficient.",
      "keywords": ["mongodb compass", "gui", "administrative tools", "mongosh", "mongostat", "mongotop", "profiler", "schema analysis", "query builder", "monitoring"],
      "difficulty": "easy"
    },
    {
      "id": 13015,
      "tag": "mongodb",
      "question": "How do you integrate MongoDB with Node.js using Mongoose ODM? Explain schema definition, models, and common operations.",
      "answer": "Mongoose is an Object Document Mapper (ODM) for MongoDB and Node.js that provides a schema-based solution for modeling application data with built-in type casting, validation, and query building.\n\n**Installation and Setup**:\n```bash\nnpm install mongoose\n```\n\n```javascript\nconst mongoose = require('mongoose')\n\n// Connect to MongoDB\nmongoose.connect('mongodb://localhost:27017/myapp', {\n  useNewUrlParser: true,\n  useUnifiedTopology: true\n})\n\nconst db = mongoose.connection\ndb.on('error', console.error.bind(console, 'connection error:'))\ndb.once('open', () => console.log('Connected to MongoDB'))\n```\n\n**Schema Definition**:\n```javascript\nconst userSchema = new mongoose.Schema({\n  name: {\n    type: String,\n    required: [true, 'Name is required'],\n    trim: true,\n    maxlength: 100\n  },\n  email: {\n    type: String,\n    required: true,\n    unique: true,\n    lowercase: true,\n    validate: {\n      validator: function(v) {\n        return /^[\\w-\\.]+@([\\w-]+\\.)+[\\w-]{2,4}$/.test(v)\n      },\n      message: 'Invalid email format'\n    }\n  },\n  age: {\n    type: Number,\n    min: [0, 'Age cannot be negative'],\n    max: 120\n  },\n  status: {\n    type: String,\n    enum: ['active', 'inactive', 'pending'],\n    default: 'pending'\n  },\n  tags: [String],\n  profile: {\n    bio: String,\n    website: String,\n    social: {\n      twitter: String,\n      linkedin: String\n    }\n  },\n  createdAt: {\n    type: Date,\n    default: Date.now\n  }\n}, {\n  timestamps: true,  // Adds createdAt and updatedAt\n  collection: 'users'  // Specify collection name\n})\n\n// Create model\nconst User = mongoose.model('User', userSchema)\n```\n\n**Schema Methods and Statics**:\n```javascript\n// Instance methods\nuserSchema.methods.getFullProfile = function() {\n  return {\n    name: this.name,\n    email: this.email,\n    bio: this.profile?.bio,\n    joinedAt: this.createdAt\n  }\n}\n\n// Static methods\nuserSchema.statics.findByEmail = function(email) {\n  return this.findOne({ email: email.toLowerCase() })\n}\n\n// Virtual properties\nuserSchema.virtual('displayName').get(function() {\n  return this.name.toUpperCase()\n})\n\n// Pre-save middleware\nuserSchema.pre('save', async function(next) {\n  if (this.isModified('email')) {\n    this.email = this.email.toLowerCase()\n  }\n  next()\n})\n```\n\n**CRUD Operations**:\n\n**Create Documents**:\n```javascript\n// Create single user\nconst newUser = new User({\n  name: 'John Doe',\n  email: 'john@example.com',\n  age: 30,\n  tags: ['developer', 'nodejs']\n})\n\ntry {\n  const savedUser = await newUser.save()\n  console.log('User created:', savedUser._id)\n} catch (error) {\n  console.error('Validation error:', error.message)\n}\n\n// Create multiple users\nconst users = await User.create([\n  { name: 'Alice', email: 'alice@example.com' },\n  { name: 'Bob', email: 'bob@example.com' }\n])\n```\n\n**Read Operations**:\n```javascript\n// Find operations\nconst allUsers = await User.find()\nconst activeUsers = await User.find({ status: 'active' })\nconst user = await User.findById(userId)\nconst userByEmail = await User.findByEmail('john@example.com')\n\n// Query with options\nconst paginatedUsers = await User.find()\n  .select('name email status')\n  .sort({ createdAt: -1 })\n  .limit(10)\n  .skip(20)\n\n// Population (similar to joins)\nconst orderSchema = new mongoose.Schema({\n  user: { type: mongoose.Schema.Types.ObjectId, ref: 'User' },\n  product: String,\n  amount: Number\n})\n\nconst Order = mongoose.model('Order', orderSchema)\n\n// Populate user data in orders\nconst ordersWithUsers = await Order.find()\n  .populate('user', 'name email')\n  .exec()\n```\n\n**Update Operations**:\n```javascript\n// Update single document\nconst updatedUser = await User.findByIdAndUpdate(\n  userId,\n  { $set: { status: 'active', age: 31 } },\n  { new: true, runValidators: true }\n)\n\n// Update multiple documents\nconst result = await User.updateMany(\n  { status: 'pending' },\n  { $set: { status: 'active' } }\n)\n\n// Update with instance method\nuser.age = 32\nuser.tags.push('mongodb')\nawait user.save()\n```\n\n**Delete Operations**:\n```javascript\n// Delete single document\nawait User.findByIdAndDelete(userId)\n\n// Delete multiple documents\nawait User.deleteMany({ status: 'inactive' })\n\n// Remove with instance method\nawait user.remove()\n```\n\n**Advanced Features**:\n\n**Aggregation**:\n```javascript\nconst userStats = await User.aggregate([\n  { $match: { status: 'active' } },\n  {\n    $group: {\n      _id: '$status',\n      count: { $sum: 1 },\n      avgAge: { $avg: '$age' }\n    }\n  }\n])\n```\n\n**Validation and Error Handling**:\n```javascript\ntry {\n  const user = new User({ email: 'invalid-email' })\n  await user.save()\n} catch (error) {\n  if (error.name === 'ValidationError') {\n    Object.keys(error.errors).forEach(key => {\n      console.log(`${key}: ${error.errors[key].message}`)\n    })\n  }\n}\n```\n\n**Connection Management**:\n```javascript\n// Connection options\nmongoose.connect('mongodb://localhost:27017/myapp', {\n  maxPoolSize: 10,\n  serverSelectionTimeoutMS: 5000,\n  socketTimeoutMS: 45000,\n  bufferCommands: false,\n  bufferMaxEntries: 0\n})\n\n// Graceful shutdown\nprocess.on('SIGINT', async () => {\n  await mongoose.connection.close()\n  process.exit(0)\n})\n```\n\nMongoose provides a powerful abstraction layer over MongoDB, offering schema validation, middleware, and intuitive APIs that make Node.js and MongoDB integration seamless and productive.",
      "keywords": ["mongoose", "odm", "nodejs", "schema", "models", "validation", "middleware", "crud operations", "population", "aggregation"],
      "difficulty": "easy"
    },
    {
      "id": 13016,
      "tag": "mongodb",
      "question": "Explain MongoDB sharding and horizontal scaling. How do you implement and manage a sharded cluster for large-scale applications?",
      "answer": "MongoDB sharding is a horizontal scaling method that distributes data across multiple machines (shards) to handle large datasets and high-throughput operations that exceed the capacity of a single server.\n\n**Sharding Architecture Components**:\n\n**1. Shard**: Individual MongoDB instances storing subset of data\n**2. Config Servers**: Store cluster metadata and configuration\n**3. Mongos (Query Router)**: Routes client requests to appropriate shards\n**4. Shard Key**: Field used to determine data distribution\n\n**Sharded Cluster Setup**:\n\n**Step 1: Start Config Server Replica Set**:\n```bash\n# Start config servers (minimum 3 for production)\nmongod --configsvr --replSet configReplSet --port 27019 --dbpath /data/configdb1\nmongod --configsvr --replSet configReplSet --port 27020 --dbpath /data/configdb2\nmongod --configsvr --replSet configReplSet --port 27021 --dbpath /data/configdb3\n\n# Initialize config replica set\nmongo --port 27019\nrs.initiate({\n  _id: \"configReplSet\",\n  configsvr: true,\n  members: [\n    { _id: 0, host: \"config1.example.com:27019\" },\n    { _id: 1, host: \"config2.example.com:27020\" },\n    { _id: 2, host: \"config3.example.com:27021\" }\n  ]\n})\n```\n\n**Step 2: Start Shard Replica Sets**:\n```bash\n# Shard 1\nmongod --shardsvr --replSet shard1 --port 27018 --dbpath /data/shard1a\nmongod --shardsvr --replSet shard1 --port 27118 --dbpath /data/shard1b\nmongod --shardsvr --replSet shard1 --port 27218 --dbpath /data/shard1c\n\n# Initialize shard 1\nmongo --port 27018\nrs.initiate({\n  _id: \"shard1\",\n  members: [\n    { _id: 0, host: \"shard1a.example.com:27018\" },\n    { _id: 1, host: \"shard1b.example.com:27118\" },\n    { _id: 2, host: \"shard1c.example.com:27218\" }\n  ]\n})\n```\n\n**Step 3: Start Mongos Router**:\n```bash\nmongos --configdb configReplSet/config1.example.com:27019,config2.example.com:27020,config3.example.com:27021 --port 27017\n```\n\n**Step 4: Add Shards to Cluster**:\n```javascript\n// Connect to mongos\nmongo --port 27017\n\n// Add shards\nsh.addShard(\"shard1/shard1a.example.com:27018,shard1b.example.com:27118,shard1c.example.com:27218\")\nsh.addShard(\"shard2/shard2a.example.com:27018,shard2b.example.com:27118,shard2c.example.com:27218\")\n\n// Enable sharding for database\nsh.enableSharding(\"myapp\")\n```\n\n**Shard Key Selection and Sharding**:\n\n**Good Shard Key Characteristics**:\n- High cardinality (many unique values)\n- Even distribution of data\n- Supports common query patterns\n- Avoids hotspots\n\n```javascript\n// Examples of shard key strategies\n\n// 1. Range-based sharding (user_id)\nsh.shardCollection(\"myapp.users\", { \"user_id\": 1 })\n\n// 2. Hash-based sharding (even distribution)\nsh.shardCollection(\"myapp.events\", { \"event_id\": \"hashed\" })\n\n// 3. Compound shard key\nsh.shardCollection(\"myapp.orders\", { \"customer_id\": 1, \"order_date\": 1 })\n\n// 4. Zone-based sharding for geographic distribution\nsh.addShardTag(\"shard1\", \"US\")\nsh.addShardTag(\"shard2\", \"EU\")\nsh.addTagRange(\n  \"myapp.users\",\n  { \"country\": \"US\", \"user_id\": MinKey },\n  { \"country\": \"US\", \"user_id\": MaxKey },\n  \"US\"\n)\n```\n\n**Cluster Management Operations**:\n\n**Monitoring Sharding Status**:\n```javascript\n// Check sharding status\nsh.status()\n\n// Check balancer status\nsh.getBalancerState()\nsh.isBalancerRunning()\n\n// View chunk distribution\ndb.chunks.find({ \"ns\": \"myapp.users\" }).count()\n\n// Check shard distribution\ndb.users.getShardDistribution()\n```\n\n**Balancer Management**:\n```javascript\n// Stop balancer for maintenance\nsh.stopBalancer()\n\n// Start balancer\nsh.startBalancer()\n\n// Configure balancer window\nuse config\ndb.settings.update(\n  { _id: \"balancer\" },\n  { $set: { activeWindow: { start: \"01:00\", stop: \"06:00\" } } },\n  { upsert: true }\n)\n```\n\n**Query Routing and Performance**:\n\n**Targeted vs Broadcast Queries**:\n```javascript\n// Targeted query (includes shard key)\ndb.users.find({ \"user_id\": 12345 })  // Routes to specific shard\n\n// Broadcast query (no shard key)\ndb.users.find({ \"email\": \"john@example.com\" })  // Queries all shards\n\n// Compound index for efficient queries\ndb.users.createIndex({ \"user_id\": 1, \"status\": 1 })\n```\n\n**Scaling Operations**:\n\n**Adding New Shard**:\n```javascript\n// Add new shard to cluster\nsh.addShard(\"shard3/shard3a.example.com:27018\")\n\n// MongoDB automatically rebalances chunks\n```\n\n**Chunk Management**:\n```javascript\n// Split chunks manually\nsh.splitAt(\"myapp.users\", { \"user_id\": 50000 })\n\n// Move chunk to specific shard\nsh.moveChunk(\"myapp.users\", { \"user_id\": 10000 }, \"shard2\")\n```\n\n**Best Practices for Sharded Clusters**:\n\n1. **Shard Key Design**:\n   - Avoid monotonically increasing keys\n   - Consider query patterns\n   - Plan for data growth\n\n2. **Hardware Considerations**:\n   - Use replica sets for each shard\n   - Separate config servers from shards\n   - Deploy mongos on application servers\n\n3. **Monitoring and Maintenance**:\n   - Monitor chunk distribution\n   - Watch for jumbo chunks\n   - Regular balancer monitoring\n   - Plan maintenance windows\n\n4. **Connection Management**:\n```javascript\n// Application connection to mongos\nconst client = new MongoClient(\n  'mongodb://mongos1.example.com:27017,mongos2.example.com:27017/myapp',\n  {\n    readPreference: 'secondaryPreferred',\n    maxPoolSize: 20\n  }\n)\n```\n\n**Limitations and Considerations**:\n- Cannot change shard key after sharding\n- Some operations not supported (e.g., unique indexes across shards)\n- Increased complexity in deployment and management\n- Network latency between shards affects performance\n\nSharding enables MongoDB to handle massive datasets and high-throughput applications by distributing data across multiple servers, providing horizontal scalability for enterprise applications.",
      "keywords": ["sharding", "horizontal scaling", "shard key", "mongos", "config servers", "balancer", "chunk management", "distributed data", "scalability"],
      "difficulty": "medium"
    },
    {
      "id": 13017,
      "tag": "mongodb",
      "question": "What is MongoDB Atlas and how do you deploy and manage MongoDB in the cloud? Explain Atlas features and cloud deployment strategies.",
      "answer": "MongoDB Atlas is MongoDB's fully managed cloud database service that automates deployment, management, and scaling of MongoDB clusters across AWS, Google Cloud, and Microsoft Azure.\n\n**Atlas Architecture and Features**:\n\n**1. Cluster Types**:\n- **Shared Clusters (M0, M2, M5)**: Free and low-cost tiers for development\n- **Dedicated Clusters (M10+)**: Production-grade with dedicated resources\n- **Serverless**: Pay-per-operation model for variable workloads\n\n**2. Multi-Cloud Deployment**:\n```javascript\n// Atlas cluster configuration\n{\n  \"clusterType\": \"REPLICASET\",\n  \"name\": \"myapp-prod\",\n  \"mongoDBMajorVersion\": \"6.0\",\n  \"providerSettings\": {\n    \"providerName\": \"AWS\",\n    \"instanceSizeName\": \"M30\",\n    \"regionName\": \"US_EAST_1\"\n  },\n  \"replicationSpecs\": [{\n    \"numShards\": 1,\n    \"regionsConfig\": {\n      \"US_EAST_1\": {\n        \"electableNodes\": 3,\n        \"priority\": 7,\n        \"readOnlyNodes\": 0\n      }\n    }\n  }]\n}\n```\n\n**Atlas Setup and Deployment**:\n\n**Creating Cluster via Atlas UI**:\n1. Sign up for MongoDB Atlas account\n2. Create new project\n3. Build cluster (choose provider, region, tier)\n4. Configure security (database users, network access)\n5. Connect to cluster\n\n**Atlas CLI Deployment**:\n```bash\n# Install Atlas CLI\ncurl -fLo atlascli-install.sh https://fastdl.mongodb.org/mongocli/atlas-cli-install.sh\nbash atlascli-install.sh\n\n# Authenticate\natlas auth login\n\n# Create cluster\natlas clusters create myapp-cluster \\\n  --provider AWS \\\n  --region US_EAST_1 \\\n  --tier M10 \\\n  --diskSizeGB 10 \\\n  --mdbVersion 6.0\n\n# List clusters\natlas clusters list\n\n# Get connection string\natlas clusters connectionStrings describe myapp-cluster\n```\n\n**Terraform Infrastructure as Code**:\n```hcl\n# Configure MongoDB Atlas provider\nterraform {\n  required_providers {\n    mongodbatlas = {\n      source = \"mongodb/mongodbatlas\"\n    }\n  }\n}\n\nprovider \"mongodbatlas\" {\n  public_key  = var.mongodb_atlas_public_key\n  private_key = var.mongodb_atlas_private_key\n}\n\n# Create Atlas project\nresource \"mongodbatlas_project\" \"myapp\" {\n  name   = \"myapp-production\"\n  org_id = var.atlas_org_id\n}\n\n# Create cluster\nresource \"mongodbatlas_cluster\" \"cluster\" {\n  project_id = mongodbatlas_project.myapp.id\n  name       = \"myapp-cluster\"\n\n  cluster_type = \"REPLICASET\"\n  \n  provider_name               = \"AWS\"\n  provider_instance_size_name = \"M30\"\n  provider_region_name        = \"US_EAST_1\"\n\n  mongodb_major_version = \"6.0\"\n  auto_scaling_disk_gb_enabled = true\n}\n\n# Database user\nresource \"mongodbatlas_database_user\" \"app_user\" {\n  username           = \"app-user\"\n  password           = var.database_password\n  project_id         = mongodbatlas_project.myapp.id\n  auth_database_name = \"admin\"\n\n  roles {\n    role_name     = \"readWrite\"\n    database_name = \"myapp\"\n  }\n}\n\n# Network access\nresource \"mongodbatlas_project_ip_access_list\" \"app_access\" {\n  project_id = mongodbatlas_project.myapp.id\n  cidr_block = \"10.0.0.0/16\"\n  comment    = \"Application servers\"\n}\n```\n\n**Atlas Security Configuration**:\n\n**Database Access Control**:\n```javascript\n// Create database user via Atlas API\nconst user = {\n  \"username\": \"myapp-user\",\n  \"password\": \"securePassword123\",\n  \"roles\": [\n    {\n      \"roleName\": \"readWrite\",\n      \"databaseName\": \"myapp\"\n    },\n    {\n      \"roleName\": \"read\",\n      \"databaseName\": \"analytics\"\n    }\n  ],\n  \"scopes\": [\n    {\n      \"name\": \"myapp-cluster\",\n      \"type\": \"CLUSTER\"\n    }\n  ]\n}\n```\n\n**Network Security**:\n```javascript\n// IP Access List configuration\n{\n  \"ipAddress\": \"203.0.113.0/24\",\n  \"comment\": \"Office network\"\n},\n{\n  \"awsSecurityGroup\": \"sg-0123456789abcdef0\",\n  \"comment\": \"AWS VPC peering\"\n}\n```\n\n**Atlas Monitoring and Alerting**:\n\n**Performance Monitoring**:\n```javascript\n// Atlas provides built-in metrics\n{\n  \"metrics\": [\n    \"operations per second\",\n    \"query execution time\",\n    \"disk utilization\",\n    \"memory usage\",\n    \"network I/O\",\n    \"connection count\"\n  ],\n  \"alerting\": {\n    \"cpuThreshold\": 80,\n    \"memoryThreshold\": 85,\n    \"diskThreshold\": 90\n  }\n}\n```\n\n**Custom Alert Configuration**:\n```javascript\n// Create alert via Atlas API\nconst alert = {\n  \"eventTypeName\": \"OUTSIDE_METRIC_THRESHOLD\",\n  \"enabled\": true,\n  \"notifications\": [\n    {\n      \"typeName\": \"EMAIL\",\n      \"emailAddress\": \"admin@myapp.com\",\n      \"delayMin\": 5\n    },\n    {\n      \"typeName\": \"SLACK\",\n      \"channelName\": \"#database-alerts\"\n    }\n  ],\n  \"matchers\": [\n    {\n      \"fieldName\": \"HOSTNAME_AND_PORT\",\n      \"operator\": \"EQUALS\",\n      \"value\": \"myapp-cluster-shard-0\"\n    }\n  ],\n  \"metricThreshold\": {\n    \"metricName\": \"DISK_PARTITION_UTILIZATION_DATA\",\n    \"operator\": \"GREATER_THAN\",\n    \"threshold\": 90,\n    \"units\": \"PERCENT\"\n  }\n}\n```\n\n**Atlas Backup and Recovery**:\n\n**Continuous Backup**:\n```javascript\n// Backup configuration\n{\n  \"backupEnabled\": true,\n  \"pitEnabled\": true,  // Point-in-time recovery\n  \"restoreWindowDays\": 7,\n  \"updateSnapshots\": true,\n  \"snapshotRetentionPolicy\": {\n    \"daily\": 7,\n    \"weekly\": 4,\n    \"monthly\": 12,\n    \"yearly\": 1\n  }\n}\n```\n\n**Application Connection**:\n\n**Connection String Formats**:\n```javascript\n// Standard connection\nconst uri = \"mongodb+srv://username:password@cluster.abc123.mongodb.net/myapp?retryWrites=true&w=majority\"\n\n// With read preference\nconst uri = \"mongodb+srv://username:password@cluster.abc123.mongodb.net/myapp?readPreference=secondaryPreferred\"\n\n// Connection with options\nconst client = new MongoClient(uri, {\n  maxPoolSize: 50,\n  wtimeoutMS: 2500,\n  useNewUrlParser: true,\n  useUnifiedTopology: true\n})\n```\n\n**Atlas Data API**:\n```javascript\n// REST API access to data\nconst response = await fetch('https://data.mongodb-api.com/app/data-abc123/endpoint/data/v1/action/findOne', {\n  method: 'POST',\n  headers: {\n    'Content-Type': 'application/json',\n    'api-key': 'your-api-key'\n  },\n  body: JSON.stringify({\n    collection: 'users',\n    database: 'myapp',\n    filter: { email: 'user@example.com' }\n  })\n})\n```\n\n**Atlas Advanced Features**:\n\n1. **Global Clusters**: Multi-region data distribution\n2. **Atlas Search**: Full-text search with Lucene\n3. **Atlas Data Lake**: Query data in S3 with MongoDB syntax\n4. **Atlas Charts**: Native data visualization\n5. **Realm/App Services**: Backend-as-a-Service integration\n\n**Cost Optimization Strategies**:\n- Use appropriate cluster tiers\n- Enable auto-scaling\n- Implement data archiving\n- Monitor and optimize queries\n- Use connection pooling\n- Consider serverless for variable workloads\n\n**Best Practices**:\n- Regular backup testing\n- Security best practices implementation\n- Performance monitoring and optimization\n- Cost monitoring and budgeting\n- Multi-environment setup (dev/staging/prod)\n\nMongoDB Atlas simplifies database operations while providing enterprise-grade security, monitoring, and scaling capabilities in the cloud.",
      "keywords": ["mongodb atlas", "cloud database", "managed service", "cluster deployment", "atlas cli", "terraform", "monitoring", "backup", "multi-cloud", "serverless"],
      "difficulty": "medium"
    },
    {
      "id": 13018,
      "tag": "mongodb",
      "question": "How do you optimize MongoDB performance and implement monitoring? Explain indexing strategies, query optimization, and performance profiling techniques.",
      "answer": "MongoDB performance optimization involves multiple strategies including proper indexing, query optimization, hardware tuning, and comprehensive monitoring to ensure optimal database performance at scale.\n\n**Index Optimization Strategies**:\n\n**1. Index Design Principles**:\n```javascript\n// ESR Rule: Equality, Sort, Range\n// Query: find({status: \"active\", category: \"electronics\"}).sort({price: -1})\n// Optimal index: equality fields first, sort fields, then range fields\ndb.products.createIndex({\n  status: 1,        // Equality\n  category: 1,      // Equality\n  price: -1         // Sort\n})\n\n// Compound index field order matters\ndb.users.createIndex({ status: 1, age: 1, created: 1 })\n// Supports: {status}, {status, age}, {status, age, created}\n// Does NOT efficiently support: {age}, {created}, {age, created}\n```\n\n**2. Index Types and Use Cases**:\n```javascript\n// Sparse indexes for optional fields\ndb.users.createIndex({ phone: 1 }, { sparse: true })\n\n// Partial indexes with conditions\ndb.orders.createIndex(\n  { status: 1, total: 1 },\n  { partialFilterExpression: { status: { $ne: \"cancelled\" } } }\n)\n\n// TTL indexes for automatic document expiration\ndb.sessions.createIndex(\n  { createdAt: 1 },\n  { expireAfterSeconds: 3600 }  // Expire after 1 hour\n)\n\n// Case-insensitive text search\ndb.articles.createIndex({\n  title: \"text\",\n  content: \"text\"\n}, {\n  default_language: \"english\",\n  language_override: \"lang\"\n})\n```\n\n**Query Optimization Techniques**:\n\n**1. Query Analysis with explain()**:\n```javascript\n// Analyze query execution\ndb.users.find({ age: { $gte: 25 }, status: \"active\" })\n  .sort({ created: -1 })\n  .explain(\"executionStats\")\n\n// Key metrics to analyze:\n{\n  \"executionStats\": {\n    \"totalExamined\": 1000,     // Documents examined\n    \"totalReturned\": 50,       // Documents returned\n    \"executionTimeMillis\": 45, // Query time\n    \"stage\": \"IXSCAN\",         // Index scan (good)\n    \"indexName\": \"status_1_age_1_created_-1\"\n  }\n}\n\n// Identify inefficient queries\n// High totalExamined vs totalReturned ratio indicates poor selectivity\n// COLLSCAN stage indicates missing index\n```\n\n**2. Query Optimization Patterns**:\n```javascript\n// Use projection to reduce data transfer\ndb.users.find(\n  { status: \"active\" },\n  { name: 1, email: 1, _id: 0 }  // Only return specific fields\n)\n\n// Optimize regex queries\ndb.users.find({ email: /^john/ })  // Anchored regex uses index\ndb.users.find({ email: /john/ })   // Non-anchored regex requires full scan\n\n// Use $in efficiently for small arrays\ndb.products.find({ category: { $in: [\"electronics\", \"books\"] } })\n\n// Optimize aggregation pipelines\ndb.orders.aggregate([\n  { $match: { status: \"completed\" } },  // Filter early\n  { $group: { _id: \"$customerId\", total: { $sum: \"$amount\" } } },\n  { $sort: { total: -1 } },\n  { $limit: 10 }  // Limit after processing\n])\n```\n\n**Performance Profiling and Monitoring**:\n\n**1. Database Profiler**:\n```javascript\n// Enable profiler for slow operations\ndb.setProfilingLevel(1, { slowms: 100 })\n\n// Profile all operations (use carefully in production)\ndb.setProfilingLevel(2)\n\n// Analyze profiler data\ndb.system.profile.find({\n  \"millis\": { $gt: 1000 }\n}).sort({ ts: -1 }).limit(5)\n\n// Find most common slow queries\ndb.system.profile.aggregate([\n  { $match: { millis: { $gt: 100 } } },\n  {\n    $group: {\n      _id: \"$command.find\",\n      count: { $sum: 1 },\n      avgMillis: { $avg: \"$millis\" },\n      maxMillis: { $max: \"$millis\" }\n    }\n  },\n  { $sort: { count: -1 } }\n])\n```\n\n**2. Real-time Monitoring**:\n```javascript\n// Server status monitoring\nconst serverStatus = db.serverStatus()\nconsole.log({\n  connections: serverStatus.connections,\n  opcounters: serverStatus.opcounters,\n  memory: serverStatus.mem,\n  locks: serverStatus.locks\n})\n\n// Current operations\ndb.currentOp({\n  \"active\": true,\n  \"secs_running\": { $gte: 5 }\n})\n\n// Kill long-running operations\ndb.killOp(operationId)\n```\n\n**3. Index Usage Analysis**:\n```javascript\n// Index usage statistics\ndb.users.aggregate([{ $indexStats: {} }])\n\n// Find unused indexes\ndb.runCommand({ collStats: \"users\", indexDetails: true })\n\n// Index hit ratio analysis\ndb.serverStatus().indexCounters\n```\n\n**Hardware and Configuration Optimization**:\n\n**1. Memory Configuration**:\n```javascript\n// WiredTiger cache configuration\n// mongod --wiredTigerCacheSizeGB 8\n\n// Monitor cache utilization\ndb.serverStatus().wiredTiger.cache\n\n// Optimal cache size: 50% of RAM - OS overhead\n```\n\n**2. Storage Optimization**:\n```bash\n# Use compression\nmongod --wiredTigerCollectionBlockCompressor snappy\n\n# Separate journal and data paths\nmongod --dbpath /data/db --journal --journalPath /journal\n\n# SSD-specific optimizations\nmongod --wiredTigerDirectoryForIndexes\n```\n\n**Connection Pool Optimization**:\n```javascript\n// Optimize connection pooling\nconst client = new MongoClient(uri, {\n  maxPoolSize: 100,           // Maximum connections\n  minPoolSize: 5,             // Minimum connections\n  maxIdleTimeMS: 30000,       // Close idle connections\n  waitQueueTimeoutMS: 5000,   // Wait timeout\n  serverSelectionTimeoutMS: 5000\n})\n\n// Monitor connection usage\ndb.serverStatus().connections\n```\n\n**Advanced Performance Monitoring Setup**:\n\n**1. MongoDB Monitoring Service (MMS) / Ops Manager**:\n```javascript\n// Custom metrics collection\nconst metrics = {\n  timestamp: new Date(),\n  database: db.stats(),\n  collections: db.runCommand({ listCollections: 1 }),\n  indexes: db.runCommand({ listIndexes: \"users\" }),\n  performance: {\n    slowQueries: db.system.profile.find({ millis: { $gt: 100 } }).count(),    operations: db.serverStatus().opcounters,\n    connections: db.serverStatus().connections.current\n  }\n}\n```\n\n**2. Prometheus Integration**:\n```yaml\n# MongoDB exporter configuration\nscrape_configs:\n  - job_name: 'mongodb'\n    static_configs:\n      - targets: ['localhost:9216']\n    scrape_interval: 30s\n    metrics_path: /metrics\n```\n\n**Application-Level Optimization**:\n\n**1. Query Patterns**:\n```javascript\n// Batch operations\nconst bulk = db.users.initializeUnorderedBulkOp()\nbulk.find({ status: \"pending\" }).update({ $set: { status: \"active\" } })\nbulk.find({ lastLogin: { $lt: oldDate } }).remove()\nconst result = await bulk.execute()\n\n// Use aggregation instead of multiple queries\nconst userStats = await db.users.aggregate([\n  { $match: { status: \"active\" } },\n  {\n    $facet: {\n      totalUsers: [{ $count: \"count\" }],\n      avgAge: [{ $group: { _id: null, avg: { $avg: \"$age\" } } }],\n      topLocations: [\n        { $group: { _id: \"$location\", count: { $sum: 1 } } },\n        { $sort: { count: -1 } },\n        { $limit: 5 }\n      ]\n    }\n  }\n])\n```\n\n**2. Schema Optimization**:\n```javascript\n// Avoid deep nesting and large arrays\n// Instead of:\n{\n  orders: [/* potentially thousands of orders */]\n}\n\n// Use references:\n{\n  userId: ObjectId(\"...\"),\n  recentOrderCount: 5\n}\n// Store orders in separate collection\n```\n\n**Performance Testing and Benchmarking**:\n```bash\n# Load testing with mongoperf\necho '{\n  \"nThreads\": 16,\n  \"fileSizeMB\": 1000,\n  \"r\": true,\n  \"w\": true\n}' | mongoperf\n\n# Benchmark specific operations\nfor i in {1..1000}; do\n  mongo --eval \"db.users.find({status: 'active'}).limit(10)\" --quiet\ndone | tail -1000 | awk '{sum += $NF} END {print \"Average:\", sum/NR}'\n```\n\n**Key Performance Metrics to Monitor**:\n- Query execution time\n- Index hit ratio\n- Connection pool utilization\n- Memory usage (resident/virtual)\n- Disk I/O patterns\n- Replication lag (for replica sets)\n- Lock percentage\n- Page faults\n\nEffective MongoDB performance optimization requires continuous monitoring, proper indexing strategies, and application-level optimizations to ensure scalable and responsive database operations.",
      "keywords": ["performance optimization", "indexing strategies", "query optimization", "profiling", "monitoring", "explain plan", "esr rule", "compound indexes", "performance metrics"],
      "difficulty": "hard"
    },
    {
      "id": 13019,
      "tag": "mongodb",
      "question": "Explain advanced MongoDB aggregation patterns and text search capabilities. How do you implement complex analytics and full-text search functionality?",
      "answer": "MongoDB's aggregation framework and text search capabilities provide powerful tools for complex data analytics, business intelligence, and full-text search functionality in modern applications.\n\n**Advanced Aggregation Patterns**:\n\n**1. Multi-Stage Complex Analytics**:\n```javascript\n// E-commerce analytics: Customer lifetime value and behavior analysis\ndb.orders.aggregate([\n  // Stage 1: Match recent orders\n  {\n    $match: {\n      orderDate: { $gte: new Date(\"2023-01-01\") },\n      status: { $in: [\"completed\", \"shipped\"] }\n    }\n  },\n  \n  // Stage 2: Lookup customer information\n  {\n    $lookup: {\n      from: \"customers\",\n      localField: \"customerId\",\n      foreignField: \"_id\",\n      as: \"customer\",\n      pipeline: [\n        { $project: { name: 1, email: 1, segment: 1, registrationDate: 1 } }\n      ]\n    }\n  },\n  \n  // Stage 3: Unwind customer array\n  { $unwind: \"$customer\" },\n  \n  // Stage 4: Add computed fields\n  {\n    $addFields: {\n      customerLifetimeDays: {\n        $divide: [\n          { $subtract: [\"$orderDate\", \"$customer.registrationDate\"] },\n          1000 * 60 * 60 * 24\n        ]\n      },\n      orderMonth: { $dateToString: { format: \"%Y-%m\", date: \"$orderDate\" } },\n      isHighValue: { $gte: [\"$totalAmount\", 500] }\n    }\n  },\n  \n  // Stage 5: Group by customer and calculate metrics\n  {\n    $group: {\n      _id: {\n        customerId: \"$customerId\",\n        customerName: \"$customer.name\",\n        segment: \"$customer.segment\"\n      },\n      totalOrders: { $sum: 1 },\n      totalRevenue: { $sum: \"$totalAmount\" },\n      avgOrderValue: { $avg: \"$totalAmount\" },\n      highValueOrders: { $sum: { $cond: [\"$isHighValue\", 1, 0] } },\n      firstOrderDate: { $min: \"$orderDate\" },\n      lastOrderDate: { $max: \"$orderDate\" },\n      monthlyOrders: {\n        $push: {\n          month: \"$orderMonth\",\n          amount: \"$totalAmount\"\n        }\n      },\n      avgCustomerLifetime: { $avg: \"$customerLifetimeDays\" }\n    }\n  },\n  \n  // Stage 6: Calculate customer lifetime value\n  {\n    $addFields: {\n      customerLifetimeValue: {\n        $multiply: [\n          \"$avgOrderValue\",\n          { $divide: [\"$totalOrders\", { $max: [\"$avgCustomerLifetime\", 1] }] },\n          365  // Annualized CLV\n        ]\n      },\n      retentionRate: {\n        $divide: [\n          { $subtract: [\"$lastOrderDate\", \"$firstOrderDate\"] },\n          1000 * 60 * 60 * 24 * 365\n        ]\n      }\n    }\n  },\n  \n  // Stage 7: Create customer segments\n  {\n    $addFields: {\n      valueSegment: {\n        $switch: {\n          branches: [\n            { case: { $gte: [\"$customerLifetimeValue\", 5000] }, then: \"Premium\" },\n            { case: { $gte: [\"$customerLifetimeValue\", 1000] }, then: \"High-Value\" },\n            { case: { $gte: [\"$customerLifetimeValue\", 500] }, then: \"Medium-Value\" }\n          ],\n          default: \"Low-Value\"\n        }\n      }\n    }\n  },\n  \n  // Stage 8: Sort by lifetime value\n  { $sort: { customerLifetimeValue: -1 } },\n  \n  // Stage 9: Final projection\n  {\n    $project: {\n      _id: 0,\n      customerId: \"$_id.customerId\",\n      customerName: \"$_id.customerName\",\n      originalSegment: \"$_id.segment\",\n      valueSegment: 1,\n      totalOrders: 1,\n      totalRevenue: 1,\n      avgOrderValue: { $round: [\"$avgOrderValue\", 2] },\n      customerLifetimeValue: { $round: [\"$customerLifetimeValue\", 2] },\n      highValueOrderRate: {\n        $round: [{ $divide: [\"$highValueOrders\", \"$totalOrders\"] }, 3]\n      },\n      retentionYears: { $round: [\"$retentionRate\", 2] }\n    }\n  }\n])\n```\n\n**2. Time Series Analytics with Window Functions**:\n```javascript\n// Advanced time series analysis with moving averages and trends\ndb.salesMetrics.aggregate([\n  { $match: { date: { $gte: new Date(\"2023-01-01\") } } },\n  { $sort: { date: 1 } },\n  \n  // Calculate moving averages and trends\n  {\n    $setWindowFields: {\n      partitionBy: \"$productCategory\",\n      sortBy: { date: 1 },\n      output: {\n        // 7-day moving average\n        movingAvg7d: {\n          $avg: \"$dailySales\",\n          window: { documents: [-6, 0] }\n        },\n        \n        // 30-day moving average\n        movingAvg30d: {\n          $avg: \"$dailySales\",\n          window: { documents: [-29, 0] }\n        },\n        \n        // Calculate rank within category\n        salesRank: {\n          $rank: {},\n          window: { documents: \"unbounded\" }\n        },\n        \n        // Running total\n        runningTotal: {\n          $sum: \"$dailySales\",\n          window: { documents: [\"unbounded\", \"current\"] }\n        },\n        \n        // Previous day comparison\n        previousDaySales: {\n          $first: \"$dailySales\",\n          window: { documents: [-1, -1] }\n        }\n      }\n    }\n  },\n  \n  // Calculate trends and growth rates\n  {\n    $addFields: {\n      trendDirection: {\n        $cond: {\n          if: { $gt: [\"$movingAvg7d\", \"$movingAvg30d\"] },\n          then: \"Upward\",\n          else: \"Downward\"\n        }\n      },\n      dayOverDayGrowth: {\n        $cond: {\n          if: { $gt: [\"$previousDaySales\", 0] },\n          then: {\n            $multiply: [\n              { $divide: [\n                { $subtract: [\"$dailySales\", \"$previousDaySales\"] },\n                \"$previousDaySales\"\n              ] },\n              100\n            ]\n          },\n          else: null\n        }\n      }\n    }\n  }\n])\n```\n\n**3. Advanced Faceted Search and Analytics**:\n```javascript\n// Multi-dimensional analytics with faceted search\ndb.products.aggregate([\n  {\n    $facet: {\n      // Price distribution analysis\n      priceDistribution: [\n        {\n          $bucket: {\n            groupBy: \"$price\",\n            boundaries: [0, 50, 100, 200, 500, 1000, Infinity],\n            default: \"Other\",\n            output: {\n              count: { $sum: 1 },\n              avgRating: { $avg: \"$rating\" },\n              categories: { $addToSet: \"$category\" }\n            }\n          }\n        }\n      ],\n      \n      // Category performance\n      categoryAnalysis: [\n        {\n          $group: {\n            _id: \"$category\",\n            totalProducts: { $sum: 1 },\n            avgPrice: { $avg: \"$price\" },\n            avgRating: { $avg: \"$rating\" },\n            totalRevenue: { $sum: { $multiply: [\"$price\", \"$soldCount\"] } }\n          }\n        },\n        { $sort: { totalRevenue: -1 } }\n      ],\n      \n      // Top performers\n      topProducts: [\n        { $match: { rating: { $gte: 4.5 } } },\n        { $sort: { soldCount: -1 } },\n        { $limit: 10 },\n        {\n          $project: {\n            name: 1,\n            price: 1,\n            rating: 1,\n            soldCount: 1,\n            revenue: { $multiply: [\"$price\", \"$soldCount\"] }\n          }\n        }\n      ],\n      \n      // Statistical summary\n      overallStats: [\n        {\n          $group: {\n            _id: null,\n            totalProducts: { $sum: 1 },\n            avgPrice: { $avg: \"$price\" },\n            medianPrice: { $median: { input: \"$price\", method: \"approximate\" } },\n            priceStdDev: { $stdDevPop: \"$price\" },\n            totalRevenue: { $sum: { $multiply: [\"$price\", \"$soldCount\"] } }\n          }\n        }\n      ]\n    }\n  }\n])\n```\n\n**Full-Text Search Implementation**:\n\n**1. Text Index Configuration**:\n```javascript\n// Create comprehensive text index\ndb.articles.createIndex(\n  {\n    title: \"text\",\n    content: \"text\",\n    tags: \"text\",\n    author: \"text\"\n  },\n  {\n    weights: {\n      title: 10,      // Higher weight for title matches\n      content: 5,     // Medium weight for content\n      tags: 8,        // High weight for tag matches\n      author: 2       // Lower weight for author\n    },\n    name: \"comprehensive_text_index\",\n    default_language: \"english\",\n    language_override: \"language\",\n    textIndexVersion: 3\n  }\n)\n\n// Multi-language text index\ndb.articles.createIndex(\n  { title: \"text\", content: \"text\" },\n  {\n    default_language: \"none\",  // Disable stemming for multi-language\n    weights: { title: 10, content: 5 }\n  }\n)\n```\n\n**2. Advanced Text Search Queries**:\n```javascript\n// Basic text search with scoring\ndb.articles.find(\n  { $text: { $search: \"mongodb aggregation pipeline\" } },\n  { score: { $meta: \"textScore\" } }\n).sort({ score: { $meta: \"textScore\" } })\n\n// Phrase search with exact matching\ndb.articles.find({\n  $text: { $search: '\"data modeling best practices\"' }\n})\n\n// Boolean search with inclusion and exclusion\ndb.articles.find({\n  $text: {\n    $search: \"mongodb performance -slow -legacy\",\n    $caseSensitive: false,\n    $diacriticSensitive: false\n  }\n})\n\n// Language-specific search\ndb.articles.find({\n  $text: {\n    $search: \"optimización rendimiento\",\n    $language: \"spanish\"\n  }\n})\n```\n\n**3. Advanced Search with Aggregation**:\n```javascript\n// Faceted search with text search and filters\ndb.articles.aggregate([\n  // Stage 1: Text search\n  {\n    $match: {\n      $text: { $search: \"mongodb performance optimization\" },\n      publishDate: { $gte: new Date(\"2023-01-01\") },\n      status: \"published\"\n    }\n  },\n  \n  // Stage 2: Add search score\n  {\n    $addFields: {\n      searchScore: { $meta: \"textScore\" },\n      relevanceCategory: {\n        $switch: {\n          branches: [\n            { case: { $gte: [{ $meta: \"textScore\" }, 5] }, then: \"Highly Relevant\" },\n            { case: { $gte: [{ $meta: \"textScore\" }, 2] }, then: \"Relevant\" },\n            { case: { $gte: [{ $meta: \"textScore\" }, 1] }, then: \"Somewhat Relevant\" }\n          ],\n          default: \"Low Relevance\"\n        }\n      }\n    }\n  },\n  \n  // Stage 3: Faceted analysis\n  {\n    $facet: {\n      // Main search results\n      results: [\n        { $sort: { searchScore: -1, publishDate: -1 } },\n        { $limit: 20 },\n        {\n          $project: {\n            title: 1,\n            summary: 1,\n            author: 1,\n            publishDate: 1,\n            tags: 1,\n            searchScore: { $round: [\"$searchScore\", 2] },\n            relevanceCategory: 1\n          }\n        }\n      ],\n      \n      // Category facets\n      categoryFacets: [\n        {\n          $group: {\n            _id: \"$category\",\n            count: { $sum: 1 },\n            avgScore: { $avg: \"$searchScore\" }\n          }\n        },\n        { $sort: { count: -1 } }\n      ],\n      \n      // Author facets\n      authorFacets: [\n        {\n          $group: {\n            _id: \"$author\",\n            articleCount: { $sum: 1 },\n            avgRelevance: { $avg: \"$searchScore\" }\n          }\n        },\n        { $sort: { articleCount: -1 } },\n        { $limit: 10 }\n      ],\n      \n      // Time-based facets\n      timeFacets: [\n        {\n          $group: {\n            _id: {\n              year: { $year: \"$publishDate\" },\n              month: { $month: \"$publishDate\" }\n            },\n            count: { $sum: 1 }\n          }\n        },\n        { $sort: { \"_id.year\": -1, \"_id.month\": -1 } }\n      ],\n      \n      // Search statistics\n      searchStats: [\n        {\n          $group: {\n            _id: null,\n            totalResults: { $sum: 1 },\n            avgScore: { $avg: \"$searchScore\" },\n            maxScore: { $max: \"$searchScore\" },\n            scoreDistribution: {\n              $push: \"$relevanceCategory\"\n            }\n          }\n        }\n      ]\n    }\n  }\n])\n```\n\n**4. Auto-complete and Suggestion Implementation**:\n```javascript\n// Create index for auto-complete\ndb.searchSuggestions.createIndex({\n  text: \"text\",\n  category: 1,\n  frequency: -1\n})\n\n// Auto-complete aggregation\ndb.searchSuggestions.aggregate([\n  {\n    $match: {\n      text: { $regex: \"^\" + searchTerm, $options: \"i\" },\n      category: { $in: allowedCategories }\n    }\n  },\n  {\n    $addFields: {\n      relevanceScore: {\n        $add: [\n          { $multiply: [\"$frequency\", 0.7] },\n          { $multiply: [{ $strLenCP: \"$text\" }, -0.1] },  // Prefer shorter suggestions\n          { $cond: [{ $eq: [\"$category\", preferredCategory] }, 0.3, 0] }\n        ]\n      }\n    }\n  },\n  { $sort: { relevanceScore: -1 } },\n  { $limit: 10 },\n  { $project: { text: 1, category: 1, frequency: 1 } }\n])\n```\n\n**5. Search Analytics and Optimization**:\n```javascript\n// Search analytics pipeline\ndb.searchLogs.aggregate([\n  {\n    $match: {\n      timestamp: { $gte: new Date(Date.now() - 7 * 24 * 60 * 60 * 1000) }\n    }\n  },\n  {\n    $group: {\n      _id: {\n        searchTerm: { $toLower: \"$query\" },\n        resultCount: \"$resultCount\"\n      },\n      searchCount: { $sum: 1 },\n      avgResponseTime: { $avg: \"$responseTimeMs\" },\n      zeroResultRate: {\n        $avg: { $cond: [{ $eq: [\"$resultCount\", 0] }, 1, 0] }\n      }\n    }\n  },\n  {\n    $project: {\n      searchTerm: \"$_id.searchTerm\",\n      searchCount: 1,\n      avgResponseTime: { $round: [\"$avgResponseTime\", 2] },\n      zeroResultRate: { $round: [\"$zeroResultRate\", 3] },\n      performance: {\n        $switch: {\n          branches: [\n            { case: { $lt: [\"$avgResponseTime\", 100] }, then: \"Excellent\" },\n            { case: { $lt: [\"$avgResponseTime\", 500] }, then: \"Good\" },\n            { case: { $lt: [\"$avgResponseTime\", 1000] }, then: \"Fair\" }\n          ],\n          default: \"Poor\"\n        }\n      }\n    }\n  },\n  { $sort: { searchCount: -1 } }\n])\n```\n\n**Performance Optimization for Complex Aggregations**:\n\n```javascript\n// Use $merge for incremental processing\ndb.dailyMetrics.aggregate([\n  { $match: { date: today } },\n  // ... complex aggregation stages ...\n  {\n    $merge: {\n      into: \"aggregatedResults\",\n      whenMatched: \"replace\",\n      whenNotMatched: \"insert\"\n    }\n  }\n])\n\n// Optimize with proper indexing for aggregation\ndb.orders.createIndex({ customerId: 1, orderDate: -1, status: 1 })\ndb.orders.createIndex({ \"items.productId\": 1, orderDate: -1 })\n```\n\nThese advanced patterns enable sophisticated analytics, business intelligence, and search functionality that can handle complex real-world requirements while maintaining good performance through proper indexing and query optimization strategies.",
      "keywords": ["advanced aggregation", "text search", "analytics", "faceted search", "$facet", "$setWindowFields", "text indexes", "search optimization", "time series analysis"],
      "difficulty": "hard"
    }
  ]
}