{
  "technology": "postgresql",
  "questions": [
    {
      "id": 14000,
      "tag": "postgresql",
      "question": "What are the key differences between PostgreSQL and MySQL?",
      "answer": "PostgreSQL and MySQL are both popular open-source relational database management systems, but they have several key differences:\n\n**Data Types and Standards Compliance:**\nPostgreSQL offers more advanced data types including arrays, JSON/JSONB, geometric types, and custom types. It has better SQL standards compliance and supports advanced SQL features like window functions, CTEs, and partial indexes. MySQL focuses on simpler data types and has some non-standard SQL extensions.\n\n**ACID Compliance:**\nPostgreSQL is fully ACID compliant by default with strong consistency guarantees. MySQL's ACID compliance depends on the storage engine (InnoDB is ACID compliant, MyISAM is not).\n\n**Concurrency and Locking:**\nPostgreSQL uses Multi-Version Concurrency Control (MVCC) which allows better concurrent read/write operations without blocking. MySQL also supports MVCC but historically had more locking issues.\n\n**Performance Characteristics:**\nMySQL traditionally performs better for simple read-heavy workloads and web applications. PostgreSQL excels in complex queries, data integrity, and write-heavy applications with concurrent users.\n\n**Extensibility:**\nPostgreSQL is highly extensible with support for custom functions, operators, and data types. It supports multiple procedural languages (PL/pgSQL, Python, Perl). MySQL has more limited extensibility options.\n\n**Replication:**\nBoth support master-slave replication, but PostgreSQL offers more sophisticated replication options including logical replication and built-in streaming replication.",
      "keywords": ["postgresql", "mysql", "comparison", "acid", "mvcc", "data types", "sql standards"],
      "difficulty": "easy"
    },
    {
      "id": 14001,
      "tag": "postgresql",
      "question": "Explain PostgreSQL's data types and give examples of when to use each.",
      "answer": "PostgreSQL offers a rich set of data types that go beyond traditional SQL databases:\n\n**Numeric Types:**\n- `INTEGER`, `BIGINT`: Standard integers for IDs, counters\n- `DECIMAL/NUMERIC`: Precise decimal calculations for financial data\n- `REAL/DOUBLE PRECISION`: Floating-point for scientific calculations\n- `SERIAL/BIGSERIAL`: Auto-incrementing integers for primary keys\n\n**Character Types:**\n- `VARCHAR(n)`: Variable-length strings with limit\n- `TEXT`: Unlimited length text (preferred for large text)\n- `CHAR(n)`: Fixed-length strings (rarely used)\n\n**Date/Time Types:**\n- `DATE`: Date only (2023-12-01)\n- `TIME`: Time only (14:30:00)\n- `TIMESTAMP`: Date and time\n- `TIMESTAMPTZ`: Timestamp with timezone (recommended)\n- `INTERVAL`: Time intervals (1 day, 2 hours)\n\n**Boolean:**\n- `BOOLEAN`: TRUE/FALSE/NULL values\n\n**Advanced Types:**\n- `JSON/JSONB`: JSON data (JSONB is binary, faster, supports indexing)\n- `ARRAY`: Arrays of any data type: `INTEGER[]`\n- `UUID`: Universally unique identifiers\n- `INET/CIDR`: IP addresses and network ranges\n- `ENUM`: Custom enumerated types\n\nExample usage:\n```sql\nCREATE TABLE users (\n    id SERIAL PRIMARY KEY,\n    email VARCHAR(255) UNIQUE,\n    profile JSONB,\n    tags TEXT[],\n    created_at TIMESTAMPTZ DEFAULT NOW()\n);\n```\n\nChoose data types based on your specific needs: use TEXT for flexibility, JSONB for semi-structured data, and appropriate numeric types for performance and storage efficiency.",
      "keywords": ["postgresql", "data types", "json", "arrays", "numeric", "text", "timestamp"],
      "difficulty": "easy"
    },
    {
      "id": 14002,
      "tag": "postgresql",
      "question": "How do you create tables with constraints in PostgreSQL?",
      "answer": "PostgreSQL supports various constraints to ensure data integrity and enforce business rules:\n\n**Primary Key Constraint:**\n```sql\nCREATE TABLE users (\n    id SERIAL PRIMARY KEY,\n    email VARCHAR(255)\n);\n```\n\n**Foreign Key Constraint:**\n```sql\nCREATE TABLE orders (\n    id SERIAL PRIMARY KEY,\n    user_id INTEGER REFERENCES users(id) ON DELETE CASCADE,\n    total DECIMAL(10,2)\n);\n```\n\n**Unique Constraint:**\n```sql\nCREATE TABLE products (\n    id SERIAL PRIMARY KEY,\n    sku VARCHAR(50) UNIQUE,\n    name VARCHAR(255)\n);\n```\n\n**Check Constraint:**\n```sql\nCREATE TABLE employees (\n    id SERIAL PRIMARY KEY,\n    age INTEGER CHECK (age >= 18 AND age <= 100),\n    salary DECIMAL(10,2) CHECK (salary > 0),\n    status VARCHAR(20) CHECK (status IN ('active', 'inactive', 'terminated'))\n);\n```\n\n**Not Null Constraint:**\n```sql\nCREATE TABLE customers (\n    id SERIAL PRIMARY KEY,\n    name VARCHAR(255) NOT NULL,\n    email VARCHAR(255) NOT NULL\n);\n```\n\n**Adding Constraints to Existing Tables:**\n```sql\nALTER TABLE customers \nADD CONSTRAINT unique_email UNIQUE (email);\n\nALTER TABLE orders \nADD CONSTRAINT fk_customer \nFOREIGN KEY (customer_id) REFERENCES customers(id);\n```\n\n**Constraint Options:**\n- `ON DELETE CASCADE/SET NULL/RESTRICT`: Defines behavior when referenced row is deleted\n- `DEFERRABLE`: Allows constraint checking to be deferred until transaction commit\n- `INITIALLY DEFERRED`: Constraint is checked at transaction end by default\n\nConstraints are essential for maintaining data quality and preventing invalid data from entering your database.",
      "keywords": ["postgresql", "constraints", "primary key", "foreign key", "check constraint", "unique", "not null"],
      "difficulty": "easy"
    },
    {
      "id": 14003,
      "tag": "postgresql",
      "question": "What are the basic CRUD operations in PostgreSQL and how do you perform them?",
      "answer": "CRUD operations (Create, Read, Update, Delete) are fundamental database operations in PostgreSQL:\n\n**CREATE (INSERT):**\n```sql\n-- Insert single row\nINSERT INTO users (name, email, age) \nVALUES ('John Doe', 'john@example.com', 30);\n\n-- Insert multiple rows\nINSERT INTO users (name, email, age) VALUES \n('Jane Smith', 'jane@example.com', 25),\n('Bob Johnson', 'bob@example.com', 35);\n\n-- Insert with RETURNING clause\nINSERT INTO users (name, email) \nVALUES ('Alice Brown', 'alice@example.com') \nRETURNING id, created_at;\n```\n\n**READ (SELECT):**\n```sql\n-- Select all columns\nSELECT * FROM users;\n\n-- Select specific columns\nSELECT name, email FROM users;\n\n-- Select with conditions\nSELECT * FROM users WHERE age > 25 AND active = true;\n\n-- Select with ordering and limiting\nSELECT * FROM users ORDER BY created_at DESC LIMIT 10;\n```\n\n**UPDATE:**\n```sql\n-- Update single column\nUPDATE users SET age = 31 WHERE id = 1;\n\n-- Update multiple columns\nUPDATE users \nSET name = 'John Smith', email = 'johnsmith@example.com' \nWHERE id = 1;\n\n-- Update with RETURNING\nUPDATE users SET last_login = NOW() \nWHERE email = 'john@example.com' \nRETURNING id, last_login;\n```\n\n**DELETE:**\n```sql\n-- Delete specific rows\nDELETE FROM users WHERE age < 18;\n\n-- Delete with RETURNING\nDELETE FROM users WHERE inactive_since < '2022-01-01' \nRETURNING id, name;\n\n-- Delete all rows (be careful!)\nDELETE FROM users;\n```\n\n**Best Practices:**\n- Always use WHERE clauses to avoid unintended operations\n- Use transactions for multiple related operations\n- Use RETURNING clause to get affected row data\n- Consider using UPSERT (INSERT ... ON CONFLICT) for insert-or-update scenarios",
      "keywords": ["postgresql", "crud", "insert", "select", "update", "delete", "sql", "returning"],
      "difficulty": "easy"
    },
    {
      "id": 14004,
      "tag": "postgresql",
      "question": "Explain different types of JOINs in PostgreSQL with examples.",
      "answer": "PostgreSQL supports several types of JOINs to combine data from multiple tables:\n\n**INNER JOIN:**\nReturns only rows that have matching values in both tables.\n```sql\nSELECT u.name, o.total\nFROM users u\nINNER JOIN orders o ON u.id = o.user_id;\n```\n\n**LEFT JOIN (LEFT OUTER JOIN):**\nReturns all rows from the left table and matched rows from the right table. NULL for non-matching right table rows.\n```sql\nSELECT u.name, o.total\nFROM users u\nLEFT JOIN orders o ON u.id = o.user_id;\n-- Shows all users, even those without orders\n```\n\n**RIGHT JOIN (RIGHT OUTER JOIN):**\nReturns all rows from the right table and matched rows from the left table.\n```sql\nSELECT u.name, o.total\nFROM users u\nRIGHT JOIN orders o ON u.id = o.user_id;\n-- Shows all orders, even those without user info\n```\n\n**FULL OUTER JOIN:**\nReturns all rows from both tables, with NULLs where there's no match.\n```sql\nSELECT u.name, o.total\nFROM users u\nFULL OUTER JOIN orders o ON u.id = o.user_id;\n```\n\n**CROSS JOIN:**\nReturns the Cartesian product of both tables (every row from first table combined with every row from second table).\n```sql\nSELECT u.name, p.product_name\nFROM users u\nCROSS JOIN products p;\n```\n\n**SELF JOIN:**\nJoining a table with itself.\n```sql\nSELECT e1.name as employee, e2.name as manager\nFROM employees e1\nINNER JOIN employees e2 ON e1.manager_id = e2.id;\n```\n\n**Multiple JOINs:**\n```sql\nSELECT u.name, o.total, p.product_name\nFROM users u\nINNER JOIN orders o ON u.id = o.user_id\nINNER JOIN order_items oi ON o.id = oi.order_id\nINNER JOIN products p ON oi.product_id = p.id;\n```\n\nChoose the appropriate JOIN type based on whether you want to include or exclude non-matching rows from either table.",
      "keywords": ["postgresql", "joins", "inner join", "left join", "right join", "full outer join", "cross join", "self join"],
      "difficulty": "easy"
    },
    {
      "id": 14005,
      "tag": "postgresql",
      "question": "What are PostgreSQL functions and how do you create custom functions?",
      "answer": "PostgreSQL functions are reusable code blocks that perform specific operations and return values. They can be written in SQL, PL/pgSQL, or other supported languages.\n\n**Built-in Functions:**\nPostgreSQL provides many built-in functions:\n```sql\n-- String functions\nSELECT UPPER('hello'), LENGTH('PostgreSQL'), SUBSTRING('PostgreSQL', 1, 4);\n\n-- Date functions\nSELECT NOW(), CURRENT_DATE, AGE('2023-01-01');\n\n-- Math functions\nSELECT ROUND(3.14159, 2), ABS(-5), RANDOM();\n\n-- Aggregate functions\nSELECT COUNT(*), AVG(age), MAX(salary) FROM employees;\n```\n\n**Creating SQL Functions:**\n```sql\n-- Simple calculation function\nCREATE OR REPLACE FUNCTION calculate_tax(amount DECIMAL)\nRETURNS DECIMAL AS $$\n    SELECT amount * 0.08;\n$$ LANGUAGE SQL;\n\n-- Usage\nSELECT calculate_tax(1000); -- Returns 80.00\n```\n\n**Creating PL/pgSQL Functions:**\n```sql\n-- Function with conditional logic\nCREATE OR REPLACE FUNCTION get_discount(customer_type TEXT, amount DECIMAL)\nRETURNS DECIMAL AS $$\nBEGIN\n    IF customer_type = 'premium' THEN\n        RETURN amount * 0.15;\n    ELSIF customer_type = 'regular' THEN\n        RETURN amount * 0.10;\n    ELSE\n        RETURN 0;\n    END IF;\nEND;\n$$ LANGUAGE plpgsql;\n```\n\n**Function with Table Operations:**\n```sql\nCREATE OR REPLACE FUNCTION update_last_login(user_email TEXT)\nRETURNS VOID AS $$\nBEGIN\n    UPDATE users \n    SET last_login = NOW() \n    WHERE email = user_email;\n    \n    IF NOT FOUND THEN\n        RAISE EXCEPTION 'User with email % not found', user_email;\n    END IF;\nEND;\n$$ LANGUAGE plpgsql;\n```\n\n**Function Returning Table:**\n```sql\nCREATE OR REPLACE FUNCTION get_active_users()\nRETURNS TABLE(id INTEGER, name TEXT, email TEXT) AS $$\nBEGIN\n    RETURN QUERY\n    SELECT u.id, u.name, u.email\n    FROM users u\n    WHERE u.active = true;\nEND;\n$$ LANGUAGE plpgsql;\n```\n\n**Function Features:**\n- Support for default parameters\n- Variable argument lists\n- Exception handling\n- Security definer vs invoker rights\n- Immutable, stable, or volatile classification for optimization\n\nFunctions improve code reusability, maintainability, and can enhance performance through caching.",
      "keywords": ["postgresql", "functions", "plpgsql", "sql functions", "custom functions", "procedures", "built-in functions"],
      "difficulty": "easy"
    },
    {
      "id": 14006,
      "tag": "postgresql",
      "question": "How do indexes work in PostgreSQL and when should you create them?",
      "answer": "Indexes in PostgreSQL are data structures that improve query performance by creating fast access paths to table data, similar to an index in a book.\n\n**How Indexes Work:**\nIndexes store sorted references to table rows, allowing the database to quickly locate specific rows without scanning the entire table. PostgreSQL uses various index types optimized for different scenarios.\n\n**Common Index Types:**\n\n**B-tree Index (Default):**\nBest for equality and range queries.\n```sql\nCREATE INDEX idx_users_email ON users(email);\nCREATE INDEX idx_orders_date ON orders(created_at);\n```\n\n**Partial Index:**\nIndexes only rows meeting specific conditions.\n```sql\nCREATE INDEX idx_active_users ON users(email) WHERE active = true;\n```\n\n**Composite Index:**\nIndexes multiple columns together.\n```sql\nCREATE INDEX idx_user_status_date ON users(status, created_at);\n```\n\n**Unique Index:**\nEnforces uniqueness while providing fast lookups.\n```sql\nCREATE UNIQUE INDEX idx_users_email_unique ON users(email);\n```\n\n**GIN Index:**\nFor full-text search and array operations.\n```sql\nCREATE INDEX idx_users_tags ON users USING GIN(tags);\n```\n\n**When to Create Indexes:**\n- Columns frequently used in WHERE clauses\n- Columns used in JOIN conditions\n- Columns used in ORDER BY clauses\n- Foreign key columns\n- Columns used in GROUP BY operations\n\n**When NOT to Create Indexes:**\n- Tables with frequent INSERT/UPDATE/DELETE operations\n- Small tables (< 1000 rows)\n- Columns that change frequently\n- Columns with low selectivity (few unique values)\n\n**Index Maintenance:**\n```sql\n-- Monitor index usage\nSELECT schemaname, tablename, indexname, idx_scan, idx_tup_read\nFROM pg_stat_user_indexes;\n\n-- Rebuild index\nREINDEX INDEX idx_users_email;\n\n-- Drop unused index\nDROP INDEX IF EXISTS idx_old_index;\n```\n\n**Best Practices:**\n- Create indexes based on actual query patterns\n- Monitor index usage statistics\n- Consider composite indexes for multi-column queries\n- Use partial indexes for filtered queries\n- Balance query performance with write performance",
      "keywords": ["postgresql", "indexes", "btree", "gin", "performance", "query optimization", "partial index", "composite index"],
      "difficulty": "easy"
    },
    {
      "id": 14007,
      "tag": "postgresql",
      "question": "What are views in PostgreSQL and how do you create and use them?",
      "answer": "Views in PostgreSQL are virtual tables that represent the result of a stored query. They don't store data themselves but provide a way to present data from one or more tables in a specific format.\n\n**Creating Basic Views:**\n```sql\n-- Simple view\nCREATE VIEW active_users AS\nSELECT id, name, email, created_at\nFROM users\nWHERE active = true;\n\n-- Complex view with joins\nCREATE VIEW user_order_summary AS\nSELECT \n    u.id,\n    u.name,\n    u.email,\n    COUNT(o.id) as order_count,\n    COALESCE(SUM(o.total), 0) as total_spent\nFROM users u\nLEFT JOIN orders o ON u.id = o.user_id\nGROUP BY u.id, u.name, u.email;\n```\n\n**Using Views:**\n```sql\n-- Query views like regular tables\nSELECT * FROM active_users WHERE name LIKE 'John%';\n\nSELECT name, total_spent \nFROM user_order_summary \nWHERE order_count > 5;\n```\n\n**Materialized Views:**\nStore the query result physically for better performance:\n```sql\nCREATE MATERIALIZED VIEW monthly_sales AS\nSELECT \n    DATE_TRUNC('month', created_at) as month,\n    COUNT(*) as order_count,\n    SUM(total) as total_revenue\nFROM orders\nGROUP BY DATE_TRUNC('month', created_at);\n\n-- Refresh materialized view\nREFRESH MATERIALIZED VIEW monthly_sales;\n```\n\n**Updatable Views:**\nSome views allow INSERT/UPDATE/DELETE operations:\n```sql\nCREATE VIEW recent_users AS\nSELECT id, name, email\nFROM users\nWHERE created_at > NOW() - INTERVAL '30 days';\n\n-- This works if view meets updatable criteria\nUPDATE recent_users SET name = 'Updated Name' WHERE id = 1;\n```\n\n**View Management:**\n```sql\n-- Replace existing view\nCREATE OR REPLACE VIEW active_users AS\nSELECT id, name, email, created_at, last_login\nFROM users\nWHERE active = true;\n\n-- Drop view\nDROP VIEW IF EXISTS old_view;\n\n-- Check view definition\n\\d+ active_users\n```\n\n**Benefits of Views:**\n- **Security:** Hide sensitive columns or rows\n- **Simplification:** Present complex queries as simple tables\n- **Consistency:** Ensure consistent data presentation across applications\n- **Performance:** Materialized views can cache expensive calculations\n\n**Use Cases:**\n- Creating user-specific data views\n- Simplifying complex reporting queries\n- Implementing row-level security\n- Creating API-friendly data formats\n- Aggregating data from multiple tables\n\nViews are powerful tools for data abstraction and can significantly improve application development and maintenance.",
      "keywords": ["postgresql", "views", "materialized views", "virtual tables", "query abstraction", "security", "updatable views"],
      "difficulty": "easy"
    },
    {
      "id": 14008,
      "tag": "postgresql",
      "question": "Explain database normalization and its normal forms in PostgreSQL context.",
      "answer": "Database normalization is the process of organizing data in a database to reduce redundancy and improve data integrity. It involves decomposing tables into smaller, related tables and defining relationships between them.\n\n**First Normal Form (1NF):**\nEliminates repeating groups and ensures atomic values.\n\n*Violation Example:*\n```sql\n-- BAD: Multiple phone numbers in one field\nCREATE TABLE customers_bad (\n    id SERIAL PRIMARY KEY,\n    name TEXT,\n    phones TEXT -- '555-1234, 555-5678, 555-9012'\n);\n```\n\n*1NF Compliant:*\n```sql\n-- GOOD: Separate table for phone numbers\nCREATE TABLE customers (\n    id SERIAL PRIMARY KEY,\n    name TEXT\n);\n\nCREATE TABLE customer_phones (\n    id SERIAL PRIMARY KEY,\n    customer_id INTEGER REFERENCES customers(id),\n    phone_number TEXT\n);\n```\n\n**Second Normal Form (2NF):**\nMeet 1NF and eliminate partial dependencies on composite primary keys.\n\n*Violation Example:*\n```sql\n-- BAD: order_date depends only on order_id, not the full key\nCREATE TABLE order_items_bad (\n    order_id INTEGER,\n    product_id INTEGER,\n    order_date DATE, -- Depends only on order_id\n    quantity INTEGER,\n    PRIMARY KEY (order_id, product_id)\n);\n```\n\n*2NF Compliant:*\n```sql\nCREATE TABLE orders (\n    id SERIAL PRIMARY KEY,\n    order_date DATE\n);\n\nCREATE TABLE order_items (\n    order_id INTEGER REFERENCES orders(id),\n    product_id INTEGER,\n    quantity INTEGER,\n    PRIMARY KEY (order_id, product_id)\n);\n```\n\n**Third Normal Form (3NF):**\nMeet 2NF and eliminate transitive dependencies.\n\n*Violation Example:*\n```sql\n-- BAD: customer_city depends on customer_id, which depends on order_id\nCREATE TABLE orders_bad (\n    id SERIAL PRIMARY KEY,\n    customer_id INTEGER,\n    customer_name TEXT,\n    customer_city TEXT -- Transitive dependency\n);\n```\n\n*3NF Compliant:*\n```sql\nCREATE TABLE customers (\n    id SERIAL PRIMARY KEY,\n    name TEXT,\n    city TEXT\n);\n\nCREATE TABLE orders (\n    id SERIAL PRIMARY KEY,\n    customer_id INTEGER REFERENCES customers(id)\n);\n```\n\n**Benefits of Normalization:**\n- Reduces data redundancy\n- Improves data consistency\n- Easier data maintenance\n- Smaller database size\n- Better data integrity\n\n**When to Consider Denormalization:**\n- Read-heavy applications requiring fast queries\n- Data warehousing scenarios\n- When JOIN operations become too expensive\n- Reporting and analytics use cases\n\n**PostgreSQL-Specific Considerations:**\n- Use JSONB for semi-structured data that doesn't fit normal forms\n- Leverage materialized views for denormalized reporting\n- Consider partial normalization with appropriate indexing\n\nNormalization is fundamental for maintaining data quality, but balance it with performance requirements for your specific use case.",
      "keywords": ["postgresql", "normalization", "1nf", "2nf", "3nf", "database design", "data integrity", "redundancy"],
      "difficulty": "easy"
    },
    {
      "id": 14009,
      "tag": "postgresql",
      "question": "How do you perform backup and restore operations in PostgreSQL?",
      "answer": "PostgreSQL provides several tools and methods for backing up and restoring databases, each suited for different scenarios and requirements.\n\n**pg_dump - Logical Backup:**\nCreates a logical backup of a single database.\n```bash\n# Basic database backup\npg_dump -h localhost -U username -d database_name > backup.sql\n\n# Compressed backup\npg_dump -h localhost -U username -d database_name | gzip > backup.sql.gz\n\n# Custom format (recommended for large databases)\npg_dump -h localhost -U username -F c -d database_name > backup.dump\n\n# Specific tables only\npg_dump -h localhost -U username -t table1 -t table2 -d database_name > tables_backup.sql\n\n# Schema only (no data)\npg_dump -h localhost -U username -s -d database_name > schema_only.sql\n\n# Data only (no schema)\npg_dump -h localhost -U username -a -d database_name > data_only.sql\n```\n\n**pg_dumpall - Cluster-wide Backup:**\nBacks up all databases, users, and global objects.\n```bash\n# Full cluster backup\npg_dumpall -h localhost -U postgres > full_cluster_backup.sql\n\n# Global objects only (roles, tablespaces)\npg_dumpall -h localhost -U postgres -g > globals_only.sql\n```\n\n**Restoring from Logical Backups:**\n```bash\n# Restore from SQL dump\npsql -h localhost -U username -d target_database < backup.sql\n\n# Restore from compressed dump\ngunzip -c backup.sql.gz | psql -h localhost -U username -d target_database\n\n# Restore from custom format\npg_restore -h localhost -U username -d target_database backup.dump\n\n# Restore specific tables\npg_restore -h localhost -U username -d target_database -t table1 backup.dump\n\n# Clean and create database before restore\npg_restore -h localhost -U username -d target_database -c -C backup.dump\n```\n\n**Physical Backup Methods:**\n\n**File System Backup:**\n```bash\n# Stop PostgreSQL\nsudo systemctl stop postgresql\n\n# Copy data directory\nsudo tar -czf postgres_backup.tar.gz /var/lib/postgresql/data/\n\n# Start PostgreSQL\nsudo systemctl start postgresql\n```\n\n**Continuous Archiving (WAL Backup):**\nConfiguration in postgresql.conf:\n```\nwal_level = replica\narchive_mode = on\narchive_command = 'cp %p /path/to/archive/%f'\n```\n\n**Point-in-Time Recovery (PITR):**\n```bash\n# Base backup\npg_basebackup -h localhost -U postgres -D /backup/base -P -W\n\n# Recovery from base backup + WAL files\n# Create recovery.conf in data directory\nrestore_command = 'cp /path/to/archive/%f %p'\nrecovery_target_time = '2023-12-01 14:30:00'\n```\n\n**Best Practices:**\n- Schedule regular automated backups\n- Test restore procedures regularly\n- Store backups in multiple locations\n- Use custom format for large databases\n- Monitor backup sizes and completion times\n- Consider hot backups for production systems\n- Use compression to save storage space\n- Maintain backup retention policies\n\n**Backup Strategy Recommendations:**\n- Daily full logical backups for small databases\n- Continuous archiving + periodic base backups for large production systems\n- Test backups by performing regular restore tests\n- Document recovery procedures for different scenarios",
      "keywords": ["postgresql", "backup", "restore", "pg_dump", "pg_restore", "pitr", "wal", "archiving", "recovery"],
      "difficulty": "easy"
    },
    {
      "id": 14010,
      "tag": "postgresql",
      "question": "How do you manage users, roles, and permissions in PostgreSQL?",
      "answer": "PostgreSQL uses a role-based access control system where users and groups are both represented as roles. Roles can own database objects and have privileges granted to them.\n\n**Creating Roles and Users:**\n```sql\n-- Create a role (cannot login by default)\nCREATE ROLE developer;\n\n-- Create a user (can login)\nCREATE USER john_doe WITH PASSWORD 'secure_password';\n\n-- Create role with specific attributes\nCREATE ROLE admin_user WITH\n    LOGIN\n    PASSWORD 'admin_password'\n    CREATEDB\n    CREATEROLE\n    SUPERUSER;\n```\n\n**Role Attributes:**\n```sql\n-- Modify existing role\nALTER ROLE john_doe WITH CREATEDB;\n\n-- Common role attributes:\n-- LOGIN: Can connect to database\n-- SUPERUSER: Bypass all permission checks\n-- CREATEDB: Can create databases\n-- CREATEROLE: Can create and manage roles\n-- REPLICATION: Can initiate streaming replication\n-- PASSWORD: Set login password\n```\n\n**Database and Schema Privileges:**\n```sql\n-- Grant database connection\nGRANT CONNECT ON DATABASE myapp TO john_doe;\n\n-- Grant schema usage\nGRANT USAGE ON SCHEMA public TO john_doe;\n\n-- Grant table privileges\nGRANT SELECT, INSERT, UPDATE ON TABLE users TO john_doe;\nGRANT ALL PRIVILEGES ON TABLE products TO admin_user;\n\n-- Grant privileges on all tables in schema\nGRANT SELECT ON ALL TABLES IN SCHEMA public TO read_only_user;\n\n-- Grant default privileges for future tables\nALTER DEFAULT PRIVILEGES IN SCHEMA public\nGRANT SELECT ON TABLES TO read_only_user;\n```\n\n**Role Inheritance and Groups:**\n```sql\n-- Create group roles\nCREATE ROLE developers;\nCREATE ROLE managers;\n\n-- Grant role membership\nGRANT developers TO john_doe;\nGRANT developers TO jane_smith;\nGRANT managers TO john_doe;\n\n-- Grant privileges to group\nGRANT SELECT, INSERT, UPDATE ON TABLE projects TO developers;\nGRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO managers;\n```\n\n**Revoking Privileges:**\n```sql\n-- Revoke specific privileges\nREVOKE INSERT, UPDATE ON TABLE users FROM john_doe;\n\n-- Revoke role membership\nREVOKE developers FROM john_doe;\n\n-- Revoke all privileges\nREVOKE ALL PRIVILEGES ON TABLE sensitive_data FROM john_doe;\n```\n\n**Row Level Security (RLS):**\n```sql\n-- Enable RLS on table\nALTER TABLE orders ENABLE ROW LEVEL SECURITY;\n\n-- Create policy\nCREATE POLICY user_orders ON orders\n    FOR ALL TO application_role\n    USING (user_id = current_setting('app.current_user_id')::INTEGER);\n\n-- Grant access to table (RLS policies will still apply)\nGRANT SELECT, INSERT, UPDATE ON orders TO application_role;\n```\n\n**Checking Permissions:**\n```sql\n-- List all roles\nSELECT rolname, rolsuper, rolcreatedb, rolcanlogin \nFROM pg_roles;\n\n-- Check table privileges\nSELECT grantee, privilege_type \nFROM information_schema.table_privileges \nWHERE table_name = 'users';\n\n-- Check role memberships\nSELECT r.rolname, m.rolname as member_of\nFROM pg_roles r \nJOIN pg_auth_members am ON r.oid = am.member\nJOIN pg_roles m ON am.roleid = m.oid;\n```\n\n**Best Practices:**\n- Use principle of least privilege\n- Create functional roles (read_only, app_user, admin)\n- Use groups for permission management\n- Regularly audit user permissions\n- Implement row-level security for multi-tenant applications\n- Use connection pooling with limited privilege accounts\n- Avoid using superuser accounts for applications",
      "keywords": ["postgresql", "users", "roles", "permissions", "privileges", "security", "rbac", "row level security"],
      "difficulty": "easy"
    },
    {
      "id": 14011,
      "tag": "postgresql",
      "question": "What are the key PostgreSQL configuration parameters and how do you tune them?",
      "answer": "PostgreSQL performance and behavior are controlled by configuration parameters in postgresql.conf. Understanding key parameters is essential for optimal database performance.\n\n**Memory Configuration:**\n```conf\n# Shared memory for caching data pages\nshared_buffers = 256MB          # Default: 128MB\n# Rule: 25% of total RAM for dedicated servers\n\n# Memory for internal operations (sorts, joins)\nwork_mem = 4MB                  # Default: 4MB\n# Rule: Total RAM / max_connections / 2-4\n\n# Memory for maintenance operations\nmaintenance_work_mem = 64MB     # Default: 64MB\n# Rule: 5-10% of total RAM\n\n# Memory for WAL operations\nwal_buffers = 16MB              # Default: -1 (auto)\n# Rule: 3% of shared_buffers\n\n# Total memory for all connections\nmax_connections = 100           # Default: 100\n# Balance with work_mem allocation\n```\n\n**Checkpoint and WAL Configuration:**\n```conf\n# How often to checkpoint\ncheckpoint_segments = 32        # PostgreSQL < 9.5\nmax_wal_size = 1GB             # PostgreSQL >= 9.5\n\n# Checkpoint completion target\ncheckpoint_completion_target = 0.7  # Default: 0.5\n\n# WAL level for replication\nwal_level = replica            # Default: replica\n\n# Synchronous commit for durability vs performance\nsynchronous_commit = on        # Default: on\n# Options: on, remote_apply, remote_write, local, off\n```\n\n**Query Planner Configuration:**\n```conf\n# Cost settings influence query planning\nrandom_page_cost = 1.1         # Default: 4.0 (SSD: 1.1, HDD: 4.0)\nseq_page_cost = 1.0            # Default: 1.0\ncpu_tuple_cost = 0.01          # Default: 0.01\ncpu_index_tuple_cost = 0.005   # Default: 0.005\n\n# Statistics target for better planning\ndefault_statistics_target = 100 # Default: 100\n# Higher values (500-1000) for complex queries\n```\n\n**Connection and Client Configuration:**\n```conf\n# Maximum connections\nmax_connections = 200\n\n# Connection timeout\ntcp_keepalives_idle = 600\ntcp_keepalives_interval = 30\ntcp_keepalives_count = 3\n\n# Statement timeout\nstatement_timeout = 0          # 0 = disabled\n\n# Lock timeout\nlock_timeout = 0               # 0 = disabled\n```\n\n**Logging Configuration:**\n```conf\n# Enable query logging\nlog_statement = 'all'          # none, ddl, mod, all\n\n# Log slow queries\nlog_min_duration_statement = 1000  # Log queries > 1 second\n\n# Log checkpoints\nlog_checkpoints = on\n\n# Log lock waits\nlog_lock_waits = on\n\n# Log line prefix for better parsing\nlog_line_prefix = '%t [%p-%l] %q%u@%d '\n```\n\n**Autovacuum Configuration:**\n```conf\n# Enable autovacuum\nautovacuum = on                # Default: on\n\n# Autovacuum worker processes\nautovacuum_max_workers = 3     # Default: 3\n\n# Vacuum thresholds\nautovacuum_vacuum_threshold = 50\nautovacuum_vacuum_scale_factor = 0.2\n\n# Analyze thresholds\nautovacuum_analyze_threshold = 50\nautovacuum_analyze_scale_factor = 0.1\n```\n\n**Configuration Management:**\n```sql\n-- View current settings\nSHOW ALL;\nSHOW shared_buffers;\n\n-- Check configuration file location\nSHOW config_file;\n\n-- Reload configuration (for settings that don't require restart)\nSELECT pg_reload_conf();\n\n-- View settings that require restart\nSELECT name, setting, pending_restart \nFROM pg_settings \nWHERE pending_restart = true;\n```\n\n**Tuning Guidelines:**\n1. Start with basic memory settings based on available RAM\n2. Monitor query performance and adjust work_mem\n3. Tune checkpoint settings based on write patterns\n4. Enable appropriate logging for monitoring\n5. Adjust autovacuum based on table update patterns\n6. Use configuration management tools for consistency\n7. Test changes in non-production environments first\n\nProper configuration tuning can significantly improve PostgreSQL performance and reliability.",
      "keywords": ["postgresql", "configuration", "postgresql.conf", "performance tuning", "memory", "wal", "checkpoints", "autovacuum"],
      "difficulty": "easy"
    },
    {
      "id": 14012,
      "tag": "postgresql",
      "question": "Explain advanced PostgreSQL queries including window functions, CTEs, and complex joins.",
      "answer": "PostgreSQL supports sophisticated SQL features that enable complex data analysis and reporting. These advanced features go beyond basic CRUD operations to provide powerful analytical capabilities.\n\n**Window Functions:**\nPerform calculations across a set of table rows related to the current row.\n\n```sql\n-- ROW_NUMBER and RANK\nSELECT \n    name,\n    salary,\n    department,\n    ROW_NUMBER() OVER (PARTITION BY department ORDER BY salary DESC) as row_num,\n    RANK() OVER (PARTITION BY department ORDER BY salary DESC) as rank,\n    DENSE_RANK() OVER (PARTITION BY department ORDER BY salary DESC) as dense_rank\nFROM employees;\n\n-- LAG and LEAD for comparing with previous/next rows\nSELECT \n    date,\n    revenue,\n    LAG(revenue, 1) OVER (ORDER BY date) as prev_revenue,\n    revenue - LAG(revenue, 1) OVER (ORDER BY date) as revenue_change,\n    LEAD(revenue, 1) OVER (ORDER BY date) as next_revenue\nFROM monthly_sales;\n\n-- Running totals and moving averages\nSELECT \n    date,\n    amount,\n    SUM(amount) OVER (ORDER BY date ROWS UNBOUNDED PRECEDING) as running_total,\n    AVG(amount) OVER (ORDER BY date ROWS 2 PRECEDING) as moving_avg_3_periods\nFROM transactions;\n\n-- NTILE for percentiles\nSELECT \n    customer_id,\n    total_purchases,\n    NTILE(4) OVER (ORDER BY total_purchases) as quartile\nFROM customer_totals;\n```\n\n**Common Table Expressions (CTEs):**\nTemporary named result sets that improve query readability and enable recursive queries.\n\n```sql\n-- Basic CTE\nWITH high_value_customers AS (\n    SELECT customer_id, SUM(total) as total_spent\n    FROM orders\n    GROUP BY customer_id\n    HAVING SUM(total) > 10000\n),\nrecent_orders AS (\n    SELECT customer_id, COUNT(*) as recent_order_count\n    FROM orders\n    WHERE created_at > NOW() - INTERVAL '30 days'\n    GROUP BY customer_id\n)\nSELECT \n    c.name,\n    hvc.total_spent,\n    COALESCE(ro.recent_order_count, 0) as recent_orders\nFROM customers c\nJOIN high_value_customers hvc ON c.id = hvc.customer_id\nLEFT JOIN recent_orders ro ON c.id = ro.customer_id;\n\n-- Recursive CTE for hierarchical data\nWITH RECURSIVE employee_hierarchy AS (\n    -- Base case: top-level managers\n    SELECT id, name, manager_id, 1 as level, name as path\n    FROM employees\n    WHERE manager_id IS NULL\n    \n    UNION ALL\n    \n    -- Recursive case\n    SELECT \n        e.id, \n        e.name, \n        e.manager_id, \n        eh.level + 1,\n        eh.path || ' -> ' || e.name\n    FROM employees e\n    JOIN employee_hierarchy eh ON e.manager_id = eh.id\n)\nSELECT * FROM employee_hierarchy ORDER BY level, path;\n```\n\n**Complex Joins and Subqueries:**\n\n```sql\n-- Multiple joins with aggregations\nSELECT \n    c.name as customer_name,\n    COUNT(DISTINCT o.id) as order_count,\n    COUNT(DISTINCT p.id) as unique_products_ordered,\n    SUM(oi.quantity * oi.price) as total_spent,\n    AVG(o.total) as avg_order_value\nFROM customers c\nLEFT JOIN orders o ON c.id = o.customer_id\nLEFT JOIN order_items oi ON o.id = oi.order_id\nLEFT JOIN products p ON oi.product_id = p.id\nWHERE c.created_at > '2023-01-01'\nGROUP BY c.id, c.name\nHAVING COUNT(DISTINCT o.id) > 0\nORDER BY total_spent DESC;\n\n-- Correlated subqueries\nSELECT \n    p.name,\n    p.price,\n    (\n        SELECT AVG(price) \n        FROM products p2 \n        WHERE p2.category_id = p.category_id\n    ) as category_avg_price,\n    CASE \n        WHEN p.price > (\n            SELECT AVG(price) \n            FROM products p3 \n            WHERE p3.category_id = p.category_id\n        ) \n        THEN 'Above Average'\n        ELSE 'Below Average'\n    END as price_category\nFROM products p;\n\n-- EXISTS and NOT EXISTS\nSELECT c.*\nFROM customers c\nWHERE EXISTS (\n    SELECT 1 FROM orders o \n    WHERE o.customer_id = c.id \n    AND o.created_at > NOW() - INTERVAL '6 months'\n)\nAND NOT EXISTS (\n    SELECT 1 FROM orders o2 \n    WHERE o2.customer_id = c.id \n    AND o2.status = 'cancelled'\n);\n```\n\n**Advanced Analytical Queries:**\n\n```sql\n-- Pivot-like queries using CASE\nSELECT \n    product_id,\n    SUM(CASE WHEN EXTRACT(quarter FROM created_at) = 1 THEN quantity ELSE 0 END) as q1_sales,\n    SUM(CASE WHEN EXTRACT(quarter FROM created_at) = 2 THEN quantity ELSE 0 END) as q2_sales,\n    SUM(CASE WHEN EXTRACT(quarter FROM created_at) = 3 THEN quantity ELSE 0 END) as q3_sales,\n    SUM(CASE WHEN EXTRACT(quarter FROM created_at) = 4 THEN quantity ELSE 0 END) as q4_sales\nFROM order_items oi\nJOIN orders o ON oi.order_id = o.id\nWHERE EXTRACT(year FROM o.created_at) = 2023\nGROUP BY product_id;\n\n-- Complex date calculations\nSELECT \n    customer_id,\n    first_order_date,\n    last_order_date,\n    last_order_date - first_order_date as customer_lifetime_days,\n    order_count,\n    total_spent,\n    total_spent / NULLIF(order_count, 0) as avg_order_value,\n    total_spent / NULLIF((last_order_date - first_order_date + 1), 0) as daily_spend_rate\nFROM (\n    SELECT \n        customer_id,\n        MIN(created_at::date) as first_order_date,\n        MAX(created_at::date) as last_order_date,\n        COUNT(*) as order_count,\n        SUM(total) as total_spent\n    FROM orders\n    GROUP BY customer_id\n) customer_stats;\n```\n\n**Performance Considerations:**\n- Window functions can be memory intensive; consider partitioning\n- CTEs are materialized in PostgreSQL, which can impact performance\n- Use appropriate indexes for JOIN conditions\n- Consider query execution plans for complex queries\n- Break down very complex queries into smaller, manageable parts\n\nThese advanced features enable sophisticated data analysis and reporting directly within the database, reducing the need for complex application logic.",
      "keywords": ["postgresql", "window functions", "cte", "recursive queries", "advanced sql", "analytical queries", "complex joins"],
      "difficulty": "medium"
    },
    {
      "id": 14013,
      "tag": "postgresql",
      "question": "How do transactions work in PostgreSQL and what are ACID properties?",
      "answer": "PostgreSQL transactions provide a way to group multiple database operations into a single, atomic unit of work. Understanding ACID properties and transaction management is crucial for maintaining data consistency and integrity.\n\n**ACID Properties:**\n\n**Atomicity:**\nAll operations in a transaction succeed or fail together.\n```sql\nBEGIN;\n    UPDATE accounts SET balance = balance - 100 WHERE id = 1;\n    UPDATE accounts SET balance = balance + 100 WHERE id = 2;\n    -- If either UPDATE fails, both are rolled back\nCOMMIT;\n```\n\n**Consistency:**\nDatabase remains in a valid state before and after transaction.\n```sql\nBEGIN;\n    INSERT INTO orders (customer_id, total) VALUES (123, 250.00);\n    INSERT INTO order_items (order_id, product_id, quantity, price) \n    VALUES (currval('orders_id_seq'), 456, 2, 125.00);\n    -- Total in orders matches sum of order_items\nCOMMIT;\n```\n\n**Isolation:**\nConcurrent transactions don't interfere with each other.\n```sql\n-- Transaction isolation levels\nBEGIN TRANSACTION ISOLATION LEVEL READ COMMITTED;\n-- or REPEATABLE READ, SERIALIZABLE\nSELECT * FROM products WHERE id = 1;\nCOMMIT;\n```\n\n**Durability:**\nCommitted changes persist even after system failure.\n```sql\n-- PostgreSQL ensures durability through WAL (Write-Ahead Logging)\nCOMMIT; -- Changes are guaranteed to survive crashes\n```\n\n**Transaction Control:**\n\n```sql\n-- Basic transaction\nBEGIN; -- or START TRANSACTION\n    INSERT INTO customers (name, email) VALUES ('John Doe', 'john@example.com');\n    UPDATE customer_stats SET total_customers = total_customers + 1;\nCOMMIT;\n\n-- Transaction with rollback\nBEGIN;\n    DELETE FROM orders WHERE created_at < '2020-01-01';\n    -- Oops, that was too many rows!\nROLLBACK;\n\n-- Savepoints for partial rollback\nBEGIN;\n    INSERT INTO products (name, price) VALUES ('Widget A', 10.00);\n    SAVEPOINT sp1;\n    \n    INSERT INTO products (name, price) VALUES ('Invalid Product', -5.00);\n    -- This violates a check constraint\n    ROLLBACK TO SAVEPOINT sp1;\n    \n    INSERT INTO products (name, price) VALUES ('Widget B', 15.00);\nCOMMIT;\n-- Only Widget A and Widget B are inserted\n```\n\n**Isolation Levels:**\n\n**READ uncommitted:**\n```sql\nBEGIN TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;\n-- Can read uncommitted changes from other transactions (dirty reads)\n-- Rarely used due to data consistency issues\nCOMMIT;\n```\n\n**read committed (Default):**\n```sql\nBEGIN TRANSACTION ISOLATION LEVEL READ COMMITTED;\n-- Sees only committed changes\n-- Each statement sees latest committed data\nSELECT balance FROM accounts WHERE id = 1; -- Sees current committed value\nCOMMIT;\n```\n\n**repeatable read:**\n```sql\nBEGIN TRANSACTION ISOLATION LEVEL REPEATABLE READ;\n-- Consistent snapshot throughout transaction\nSELECT balance FROM accounts WHERE id = 1; -- Returns 1000\n-- Another transaction commits: UPDATE accounts SET balance = 1500 WHERE id = 1\nSELECT balance FROM accounts WHERE id = 1; -- Still returns 1000\nCOMMIT;\n```\n\n**serializable:**\n```sql\nBEGIN TRANSACTION ISOLATION LEVEL SERIALIZABLE;\n-- Strictest isolation, prevents serialization anomalies\n-- May cause serialization failures requiring retry\nCOMMIT;\n```\n\n**Concurrency Control and Locking:**\n\n```sql\n-- Explicit locking\nBEGIN;\n    SELECT * FROM products WHERE id = 1 FOR UPDATE;\n    -- Row is locked until transaction ends\n    UPDATE products SET price = price * 1.1 WHERE id = 1;\nCOMMIT;\n\n-- Lock modes\nSELECT * FROM orders FOR SHARE; -- Shared lock\nSELECT * FROM orders FOR UPDATE; -- Exclusive lock\nSELECT * FROM orders FOR NO KEY UPDATE; -- Allows key-preserving updates\n\n-- Table-level locking\nBEGIN;\n    LOCK TABLE products IN EXCLUSIVE MODE;\n    -- Perform batch operations\nCOMMIT;\n```\n\n**Deadlock Handling:**\n\n```sql\n-- PostgreSQL automatically detects and resolves deadlocks\n-- Example deadlock scenario:\n-- Transaction 1:\nBEGIN;\n    UPDATE accounts SET balance = balance - 100 WHERE id = 1;\n    -- Wait for lock on account 2\n    UPDATE accounts SET balance = balance + 100 WHERE id = 2;\nCOMMIT;\n\n-- Transaction 2 (concurrent):\nBEGIN;\n    UPDATE accounts SET balance = balance - 50 WHERE id = 2;\n    -- Wait for lock on account 1 - DEADLOCK!\n    UPDATE accounts SET balance = balance + 50 WHERE id = 1;\nCOMMIT;\n-- One transaction will be aborted with deadlock error\n```\n\n**Transaction Best Practices:**\n\n```sql\n-- Use appropriate isolation levels\nBEGIN TRANSACTION ISOLATION LEVEL READ COMMITTED;\n\n-- Keep transactions short\nBEGIN;\n    -- Minimize time between BEGIN and COMMIT\n    UPDATE inventory SET quantity = quantity - 1 WHERE product_id = 123;\nCOMMIT;\n\n-- Handle exceptions properly\nDO $$\nBEGIN\n    BEGIN;\n        INSERT INTO orders (customer_id, total) VALUES (999, 100.00);\n        INSERT INTO order_items (order_id, product_id) VALUES (currval('orders_id_seq'), 456);\n    COMMIT;\nEXCEPTION\n    WHEN OTHERS THEN\n        ROLLBACK;\n        RAISE NOTICE 'Transaction failed: %', SQLERRM;\nEND;\n$$;\n```\n\n**Monitoring Transactions:**\n\n```sql\n-- View current transactions\nSELECT pid, state, query_start, query FROM pg_stat_activity WHERE state != 'idle';\n\n-- Check for long-running transactions\nSELECT pid, now() - pg_stat_activity.query_start AS duration, query \nFROM pg_stat_activity \nWHERE (now() - pg_stat_activity.query_start) > interval '5 minutes';\n\n-- Monitor locks\nSELECT blocked_locks.pid AS blocked_pid,\n       blocked_activity.usename AS blocked_user,\n       blocking_locks.pid AS blocking_pid,\n       blocking_activity.usename AS blocking_user,\n       blocked_activity.query AS blocked_statement\nFROM pg_catalog.pg_locks blocked_locks\nJOIN pg_catalog.pg_stat_activity blocked_activity ON blocked_activity.pid = blocked_locks.pid\nJOIN pg_catalog.pg_locks blocking_locks ON blocking_locks.locktype = blocked_locks.locktype\nJOIN pg_catalog.pg_stat_activity blocking_activity ON blocking_activity.pid = blocking_locks.pid\nWHERE NOT blocked_locks.granted;\n```\n\nProper transaction management ensures data integrity, handles concurrent access safely, and provides recovery mechanisms for system failures.",
      "keywords": ["postgresql", "transactions", "acid", "isolation levels", "concurrency", "locking", "deadlocks", "rollback", "savepoints"],
      "difficulty": "medium"
    },
    {
      "id": 14014,
      "tag": "postgresql",
      "question": "How do you create and use stored procedures, functions, and triggers in PostgreSQL?",
      "answer": "PostgreSQL supports stored procedures, functions, and triggers that enable server-side logic execution, data validation, and automated responses to database events.\n\n**Stored Procedures (PostgreSQL 11+):**\nProcedures can manage transactions and don't return values directly.\n\n```sql\n-- Create a stored procedure\nCREATE OR REPLACE PROCEDURE process_monthly_billing()\nLANGUAGE plpgsql AS $$\nDECLARE\n    customer_record RECORD;\n    bill_amount DECIMAL;\nBEGIN\n    -- Process each active customer\n    FOR customer_record IN \n        SELECT id, name, monthly_rate FROM customers WHERE active = true\n    LOOP\n        -- Calculate bill amount\n        bill_amount := customer_record.monthly_rate * 1.08; -- Add tax\n        \n        -- Insert billing record\n        INSERT INTO bills (customer_id, amount, bill_date, status)\n        VALUES (customer_record.id, bill_amount, CURRENT_DATE, 'pending');\n        \n        -- Update customer last_billed\n        UPDATE customers \n        SET last_billed = CURRENT_DATE \n        WHERE id = customer_record.id;\n        \n        -- Commit each customer separately\n        COMMIT;\n    END LOOP;\n    \n    RAISE NOTICE 'Monthly billing completed for % customers', \n        (SELECT COUNT(*) FROM customers WHERE active = true);\nEND;\n$$;\n\n-- Call procedure\nCALL process_monthly_billing();\n```\n\n**Functions:**\nFunctions return values and cannot manage transactions.\n\n```sql\n-- Simple calculation function\nCREATE OR REPLACE FUNCTION calculate_order_total(\n    order_id INTEGER\n) RETURNS DECIMAL AS $$\nDECLARE\n    total_amount DECIMAL := 0;\nBEGIN\n    SELECT COALESCE(SUM(quantity * price), 0)\n    INTO total_amount\n    FROM order_items\n    WHERE order_id = $1;\n    \n    RETURN total_amount;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Function returning table\nCREATE OR REPLACE FUNCTION get_customer_summary(\n    start_date DATE,\n    end_date DATE\n)\nRETURNS TABLE(\n    customer_id INTEGER,\n    customer_name TEXT,\n    order_count BIGINT,\n    total_spent DECIMAL\n) AS $$\nBEGIN\n    RETURN QUERY\n    SELECT \n        c.id,\n        c.name,\n        COUNT(o.id),\n        COALESCE(SUM(o.total), 0)\n    FROM customers c\n    LEFT JOIN orders o ON c.id = o.customer_id \n        AND o.created_at BETWEEN start_date AND end_date\n    GROUP BY c.id, c.name\n    ORDER BY total_spent DESC;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Usage\nSELECT * FROM get_customer_summary('2023-01-01', '2023-12-31');\n```\n\n**Triggers:**\nAutomatically execute functions in response to database events.\n\n```sql\n-- Audit trigger function\nCREATE OR REPLACE FUNCTION audit_changes()\nRETURNS TRIGGER AS $$\nBEGIN\n    IF TG_OP = 'INSERT' THEN\n        INSERT INTO audit_log (table_name, operation, new_values, changed_at, changed_by)\n        VALUES (TG_TABLE_NAME, TG_OP, row_to_json(NEW), NOW(), current_user);\n        RETURN NEW;\n    ELSIF TG_OP = 'UPDATE' THEN\n        INSERT INTO audit_log (table_name, operation, old_values, new_values, changed_at, changed_by)\n        VALUES (TG_TABLE_NAME, TG_OP, row_to_json(OLD), row_to_json(NEW), NOW(), current_user);\n        RETURN NEW;\n    ELSIF TG_OP = 'DELETE' THEN\n        INSERT INTO audit_log (table_name, operation, old_values, changed_at, changed_by)\n        VALUES (TG_TABLE_NAME, TG_OP, row_to_json(OLD), NOW(), current_user);\n        RETURN OLD;\n    END IF;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Create audit table\nCREATE TABLE audit_log (\n    id SERIAL PRIMARY KEY,\n    table_name TEXT,\n    operation TEXT,\n    old_values JSONB,\n    new_values JSONB,\n    changed_at TIMESTAMP DEFAULT NOW(),\n    changed_by TEXT\n);\n\n-- Apply trigger to tables\nCREATE TRIGGER users_audit_trigger\n    AFTER INSERT OR UPDATE OR DELETE ON users\n    FOR EACH ROW EXECUTE FUNCTION audit_changes();\n\nCREATE TRIGGER products_audit_trigger\n    AFTER INSERT OR UPDATE OR DELETE ON products\n    FOR EACH ROW EXECUTE FUNCTION audit_changes();\n```\n\n**Data Validation Triggers:**\n\n```sql\n-- Business rule validation\nCREATE OR REPLACE FUNCTION validate_order()\nRETURNS TRIGGER AS $$\nBEGIN\n    -- Check customer exists and is active\n    IF NOT EXISTS (SELECT 1 FROM customers WHERE id = NEW.customer_id AND active = true) THEN\n        RAISE EXCEPTION 'Customer % is not active or does not exist', NEW.customer_id;\n    END IF;\n    \n    -- Validate order total\n    IF NEW.total <= 0 THEN\n        RAISE EXCEPTION 'Order total must be greater than zero';\n    END IF;\n    \n    -- Check inventory for order items (if this is an update)\n    IF TG_OP = 'INSERT' OR (TG_OP = 'UPDATE' AND OLD.status != NEW.status AND NEW.status = 'confirmed') THEN\n        PERFORM check_inventory_availability(NEW.id);\n    END IF;\n    \n    RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n\nCREATE TRIGGER validate_order_trigger\n    BEFORE INSERT OR UPDATE ON orders\n    FOR EACH ROW EXECUTE FUNCTION validate_order();\n```\n\n**Automatic Timestamp Updates:**\n\n```sql\n-- Update timestamp function\nCREATE OR REPLACE FUNCTION update_modified_timestamp()\nRETURNS TRIGGER AS $$\nBEGIN\n    NEW.updated_at = NOW();\n    RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Apply to tables\nCREATE TRIGGER update_users_timestamp\n    BEFORE UPDATE ON users\n    FOR EACH ROW EXECUTE FUNCTION update_modified_timestamp();\n\nCREATE TRIGGER update_products_timestamp\n    BEFORE UPDATE ON products\n    FOR EACH ROW EXECUTE FUNCTION update_modified_timestamp();\n```\n\n**Advanced Function Features:**\n\n```sql\n-- Function with exception handling\nCREATE OR REPLACE FUNCTION safe_divide(a DECIMAL, b DECIMAL)\nRETURNS DECIMAL AS $$\nBEGIN\n    IF b = 0 THEN\n        RAISE EXCEPTION 'Division by zero';\n    END IF;\n    \n    RETURN a / b;\nEXCEPTION\n    WHEN division_by_zero THEN\n        RAISE NOTICE 'Division by zero attempted, returning NULL';\n        RETURN NULL;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Function with default parameters\nCREATE OR REPLACE FUNCTION get_recent_orders(\n    customer_id INTEGER,\n    days_back INTEGER DEFAULT 30,\n    order_status TEXT DEFAULT 'confirmed'\n)\nRETURNS TABLE(order_id INTEGER, order_date TIMESTAMP, total DECIMAL) AS $$\nBEGIN\n    RETURN QUERY\n    SELECT o.id, o.created_at, o.total\n    FROM orders o\n    WHERE o.customer_id = $1\n        AND o.created_at >= NOW() - (days_back || ' days')::INTERVAL\n        AND o.status = order_status\n    ORDER BY o.created_at DESC;\nEND;\n$$ LANGUAGE plpgsql;\n```\n\n**Management and Monitoring:**\n\n```sql\n-- List functions and procedures\nSELECT proname, prokind, proargnames, prosrc\nFROM pg_proc\nWHERE pronamespace = (SELECT oid FROM pg_namespace WHERE nspname = 'public');\n\n-- View triggers\nSELECT t.tgname, c.relname, p.proname\nFROM pg_trigger t\nJOIN pg_class c ON t.tgrelid = c.oid\nJOIN pg_proc p ON t.tgfoid = p.oid\nWHERE NOT t.tgisinternal;\n\n-- Drop function/procedure\nDROP FUNCTION IF EXISTS old_function(INTEGER);\nDROP PROCEDURE IF EXISTS old_procedure();\n\n-- Disable/enable trigger\nALTER TABLE users DISABLE TRIGGER users_audit_trigger;\nALTER TABLE users ENABLE TRIGGER users_audit_trigger;\n```\n\n**Best Practices:**\n- Keep functions focused and simple\n- Use appropriate SECURITY DEFINER/INVOKER settings\n- Handle exceptions gracefully\n- Use triggers sparingly to avoid performance issues\n- Test trigger logic thoroughly\n- Document complex business logic\n- Consider performance impact of trigger operations\n- Use procedures for transaction management, functions for calculations\n\nStored procedures, functions, and triggers provide powerful server-side capabilities but should be used judiciously to maintain performance and code maintainability.",
      "keywords": ["postgresql", "stored procedures", "functions", "triggers", "plpgsql", "server-side logic", "automation", "validation"],
      "difficulty": "medium"
    },
    {
      "id": 14015,
      "tag": "postgresql",
      "question": "How do you optimize PostgreSQL query performance and what tools are available for analysis?",
      "answer": "PostgreSQL query performance optimization involves understanding query execution plans, proper indexing strategies, and using built-in analysis tools to identify bottlenecks.\n\n**Query Execution Plans:**\nUse EXPLAIN to understand how PostgreSQL executes queries.\n\n```sql\n-- Basic execution plan\nEXPLAIN SELECT * FROM orders WHERE customer_id = 123;\n\n-- Detailed execution plan with costs\nEXPLAIN (ANALYZE, BUFFERS, VERBOSE) \nSELECT c.name, COUNT(o.id) as order_count\nFROM customers c\nLEFT JOIN orders o ON c.id = o.customer_id\nWHERE c.created_at > '2023-01-01'\nGROUP BY c.id, c.name\nORDER BY order_count DESC;\n\n-- Format options for better readability\nEXPLAIN (ANALYZE, BUFFERS, FORMAT JSON)\nSELECT * FROM products WHERE price BETWEEN 100 AND 500;\n```\n\n**Understanding Plan Nodes:**\n\n```sql\n-- Sequential Scan (slow for large tables)\nEXPLAIN SELECT * FROM orders WHERE total > 1000;\n-- Solution: Add index on total column\nCREATE INDEX idx_orders_total ON orders(total);\n\n-- Index Scan (good)\nEXPLAIN SELECT * FROM orders WHERE customer_id = 123;\n-- Requires index on customer_id\n\n-- Nested Loop (expensive for large datasets)\n-- Hash Join (better for large datasets)\n-- Merge Join (good for pre-sorted data)\nEXPLAIN ANALYZE\nSELECT o.id, c.name\nFROM orders o\nJOIN customers c ON o.customer_id = c.id;\n```\n\n**Index Optimization Strategies:**\n\n```sql\n-- Composite indexes for multi-column queries\nCREATE INDEX idx_orders_customer_date ON orders(customer_id, created_at);\n-- Benefits queries like:\nSELECT * FROM orders WHERE customer_id = 123 AND created_at > '2023-01-01';\n\n-- Partial indexes for filtered queries\nCREATE INDEX idx_orders_pending ON orders(created_at) WHERE status = 'pending';\n-- Only indexes rows where status = 'pending'\n\n-- Expression indexes for function-based queries\nCREATE INDEX idx_customers_lower_email ON customers(LOWER(email));\n-- Benefits queries like:\nSELECT * FROM customers WHERE LOWER(email) = 'john@example.com';\n\n-- Covering indexes to avoid table lookups\nCREATE INDEX idx_orders_covering ON orders(customer_id) INCLUDE (total, created_at);\n-- Can satisfy queries without accessing the table\n```\n\n**Query Optimization Techniques:**\n\n```sql\n-- Use appropriate WHERE clause ordering\n-- PostgreSQL optimizes this, but be explicit\nSELECT * FROM orders \nWHERE status = 'confirmed'     -- High selectivity first\n  AND customer_id = 123        -- Then specific filters\n  AND total > 100;\n\n-- Use EXISTS instead of IN for subqueries\n-- GOOD:\nSELECT * FROM customers c\nWHERE EXISTS (SELECT 1 FROM orders o WHERE o.customer_id = c.id);\n\n-- LESS EFFICIENT:\nSELECT * FROM customers c\nWHERE c.id IN (SELECT customer_id FROM orders);\n\n-- Use LIMIT with ORDER BY efficiently\nSELECT * FROM orders \nORDER BY created_at DESC \nLIMIT 10;\n-- Ensure index on created_at\n\n-- Avoid SELECT * in production queries\n-- GOOD:\nSELECT id, name, email FROM customers WHERE active = true;\n-- BAD:\nSELECT * FROM customers WHERE active = true;\n```\n\n**Analyzing Slow Queries:**\n\n```sql\n-- Enable query logging for slow queries\n-- In postgresql.conf:\n-- log_min_duration_statement = 1000  # Log queries > 1 second\n-- log_statement = 'all'             # Or log all statements\n\n-- View query statistics\nSELECT \n    query,\n    calls,\n    total_time,\n    mean_time,\n    rows\nFROM pg_stat_statements\nORDER BY total_time DESC\nLIMIT 10;\n\n-- Reset statistics\nSELECT pg_stat_statements_reset();\n```\n\n**Performance Monitoring Views:**\n\n```sql\n-- Table statistics\nSELECT \n    schemaname,\n    tablename,\n    seq_scan,\n    seq_tup_read,\n    idx_scan,\n    idx_tup_fetch,\n    n_tup_ins,\n    n_tup_upd,\n    n_tup_del\nFROM pg_stat_user_tables\nORDER BY seq_scan DESC;\n\n-- Index usage statistics\nSELECT \n    schemaname,\n    tablename,\n    indexname,\n    idx_scan,\n    idx_tup_read,\n    idx_tup_fetch\nFROM pg_stat_user_indexes\nWHERE idx_scan = 0  -- Unused indexes\nORDER BY schemaname, tablename;\n\n-- Table sizes\nSELECT \n    schemaname,\n    tablename,\n    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size\nFROM pg_tables\nWHERE schemaname = 'public'\nORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;\n```\n\n**VACUUM and ANALYZE:**\n\n```sql\n-- Manual maintenance\nVACUUM ANALYZE orders;  -- Clean up and update statistics\nVACUUM FULL orders;     -- Reclaim space (locks table)\nANALYZE orders;         -- Update query planner statistics\n\n-- Check when tables were last vacuumed/analyzed\nSELECT \n    schemaname,\n    tablename,\n    last_vacuum,\n    last_autovacuum,\n    last_analyze,\n    last_autoanalyze,\n    n_dead_tup,\n    n_live_tup\nFROM pg_stat_user_tables\nORDER BY n_dead_tup DESC;\n```\n\n**Connection and Resource Monitoring:**\n\n```sql\n-- Active connections and queries\nSELECT \n    pid,\n    usename,\n    datname,\n    state,\n    query_start,\n    now() - query_start as duration,\n    query\nFROM pg_stat_activity\nWHERE state != 'idle'\nORDER BY query_start;\n\n-- Lock monitoring\nSELECT \n    bl.pid AS blocked_pid,\n    ba.usename AS blocked_user,\n    kl.pid AS blocking_pid,\n    ka.usename AS blocking_user,\n    ba.query AS blocked_statement\nFROM pg_catalog.pg_locks bl\nJOIN pg_catalog.pg_stat_activity ba ON ba.pid = bl.pid\nJOIN pg_catalog.pg_locks kl ON kl.transactionid = bl.transactionid AND kl.pid != bl.pid\nJOIN pg_catalog.pg_stat_activity ka ON ka.pid = kl.pid\nWHERE NOT bl.granted;\n```\n\n**Performance Optimization Tools:**\n\n```sql\n-- pg_stat_statements extension (must be enabled)\nCREATE EXTENSION IF NOT EXISTS pg_stat_statements;\n\n-- pgBench for performance testing\n-- Command line tool for load testing:\n-- pgbench -i -s 50 testdb  # Initialize with scale factor 50\n-- pgbench -c 10 -j 2 -t 1000 testdb  # 10 clients, 2 threads, 1000 transactions\n\n-- auto_explain extension for automatic EXPLAIN\n-- In postgresql.conf:\n-- shared_preload_libraries = 'auto_explain'\n-- auto_explain.log_min_duration = 1000\n-- auto_explain.log_analyze = on\n```\n\n**Best Practices:**\n- Use EXPLAIN ANALYZE to understand actual query performance\n- Create indexes based on actual query patterns, not assumptions\n- Monitor pg_stat_statements regularly\n- Keep table statistics up to date with ANALYZE\n- Use connection pooling to manage connections efficiently\n- Consider partitioning for very large tables\n- Regular VACUUM maintenance for write-heavy tables\n- Monitor and tune PostgreSQL configuration parameters\n- Use appropriate data types and constraints\n- Avoid unnecessary JOINs and subqueries\n\nRegular performance monitoring and optimization ensure your PostgreSQL database scales effectively with your application's growth.",
      "keywords": ["postgresql", "performance optimization", "explain", "indexes", "query tuning", "pg_stat_statements", "vacuum", "analyze"],
      "difficulty": "medium"
    },
    {
      "id": 14016,
      "tag": "postgresql",
      "question": "Explain PostgreSQL replication methods and how to set up streaming replication.",
      "answer": "PostgreSQL offers several replication methods for high availability, load distribution, and disaster recovery. Understanding these options helps design robust database architectures.\n\n**Replication Types:**\n\n**Streaming Replication (Most Common):**\nReal-time replication using WAL (Write-Ahead Log) streaming.\n\n**Logical Replication:**\nRow-level replication allowing selective replication and cross-version compatibility.\n\n**Physical vs Logical:**\n- Physical: Block-level, exact replica, same PostgreSQL version\n- Logical: Row-level, selective, different PostgreSQL versions supported\n\n**Setting Up Streaming Replication:**\n\n**Master Server Configuration:**\n\n1. **Edit postgresql.conf:**\n```conf\n# Enable WAL archiving and replication\nwal_level = replica                    # or 'hot_standby' for older versions\nmax_wal_senders = 3                   # Number of standby servers + 1\nwal_keep_segments = 64                # Keep WAL files for standby servers\narchive_mode = on                     # Enable WAL archiving\narchive_command = 'cp %p /archive/%f' # Archive command\n\n# Hot standby settings\nhot_standby = on                      # Allow read queries on standby\nmax_standby_archive_delay = 30s       # Max delay for archived WAL\nmax_standby_streaming_delay = 30s     # Max delay for streaming WAL\n```\n\n2. **Edit pg_hba.conf for replication access:**\n```conf\n# Allow replication connections\nhost    replication     replica_user    10.0.0.0/24    md5\nhost    replication     replica_user    standby_server  md5\n```\n\n3. **Create replication user:**\n```sql\nCREATE USER replica_user WITH REPLICATION ENCRYPTED PASSWORD 'secure_password';\n```\n\n4. **Restart PostgreSQL:**\n```bash\nsudo systemctl restart postgresql\n```\n\n**Standby Server Setup:**\n\n1. **Create base backup:**\n```bash\n# On standby server, stop PostgreSQL if running\nsudo systemctl stop postgresql\n\n# Remove existing data directory\nsudo rm -rf /var/lib/postgresql/data/*\n\n# Create base backup from master\npg_basebackup -h master_server -D /var/lib/postgresql/data -U replica_user -P -W -R\n\n# The -R flag automatically creates recovery.conf\n```\n\n2. **Configure standby server:**\n```conf\n# postgresql.conf on standby\nhot_standby = on\nmax_standby_archive_delay = 30s\nmax_standby_streaming_delay = 30s\n```\n\n3. **Start standby server:**\n```bash\nsudo systemctl start postgresql\n```\n\n**Monitoring Replication:**\n\n```sql\n-- On master: Check replication status\nSELECT \n    client_addr,\n    state,\n    sent_lsn,\n    write_lsn,\n    flush_lsn,\n    replay_lsn,\n    sync_state\nFROM pg_stat_replication;\n\n-- On standby: Check recovery status\nSELECT \n    pg_is_in_recovery(),\n    pg_last_wal_receive_lsn(),\n    pg_last_wal_replay_lsn(),\n    pg_last_xact_replay_timestamp();\n\n-- Check replication lag\nSELECT \n    client_addr,\n    state,\n    pg_wal_lsn_diff(pg_current_wal_lsn(), replay_lsn) AS lag_bytes,\n    EXTRACT(EPOCH FROM (now() - pg_last_xact_replay_timestamp())) AS lag_seconds\nFROM pg_stat_replication;\n```\n\n**Synchronous Replication:**\nEnsures data consistency at the cost of performance.\n\n```conf\n# On master postgresql.conf\nsynchronous_standby_names = 'standby1,standby2'\nsynchronous_commit = on  # Default, but be explicit\n\n# For specific priority:\nsynchronous_standby_names = '2 (standby1, standby2, standby3)'\n# Waits for any 2 standbys to confirm\n```\n\n**Logical Replication Setup:**\n\n**Publisher (Source) Configuration:**\n```sql\n-- Enable logical replication\n-- In postgresql.conf:\n-- wal_level = logical\n-- max_replication_slots = 10\n-- max_wal_senders = 10\n\n-- Create publication\nCREATE PUBLICATION my_publication FOR TABLE products, orders;\n\n-- Or for all tables:\nCREATE PUBLICATION all_tables FOR ALL TABLES;\n\n-- Add/remove tables\nALTER PUBLICATION my_publication ADD TABLE customers;\nALTER PUBLICATION my_publication DROP TABLE old_table;\n```\n\n**Subscriber (Target) Configuration:**\n```sql\n-- Create subscription\nCREATE SUBSCRIPTION my_subscription\nCONNECTION 'host=publisher_host dbname=source_db user=replica_user password=password'\nPUBLICATION my_publication;\n\n-- Monitor subscription\nSELECT subname, pid, received_lsn, latest_end_lsn, latest_end_time\nFROM pg_stat_subscription;\n\n-- Disable/enable subscription\nALTER SUBSCRIPTION my_subscription DISABLE;\nALTER SUBSCRIPTION my_subscription ENABLE;\n```\n\n**Failover Procedures:**\n\n**Planned Failover:**\n```bash\n# 1. Stop application connections to master\n\n# 2. Ensure standby is caught up\n# On master:\nSELECT pg_current_wal_lsn();\n# On standby:\nSELECT pg_last_wal_replay_lsn();\n\n# 3. Promote standby to master\npg_ctl promote -D /var/lib/postgresql/data\n# Or:\nSELECT pg_promote();\n\n# 4. Update application connection strings\n# 5. Configure old master as new standby (if needed)\n```\n\n**Unplanned Failover:**\n```bash\n# 1. Verify master is truly down\n# 2. Promote standby immediately\npg_ctl promote -D /var/lib/postgresql/data\n\n# 3. Update applications to connect to new master\n# 4. Investigate master failure\n# 5. Rebuild failed master as standby when ready\n```\n\n**Replication Management Scripts:**\n\n```bash\n#!/bin/bash\n# replication_status.sh - Monitor replication lag\n\npsql -h master_server -c \"\nSELECT \n    client_addr,\n    state,\n    ROUND(pg_wal_lsn_diff(pg_current_wal_lsn(), replay_lsn) / 1024 / 1024, 2) AS lag_mb,\n    EXTRACT(EPOCH FROM (now() - pg_last_xact_replay_timestamp())) AS lag_seconds\nFROM pg_stat_replication;\n\"\n\n# Alert if lag > threshold\nLAG_THRESHOLD=60  # seconds\nCURRENT_LAG=$(psql -h master_server -t -c \"SELECT EXTRACT(EPOCH FROM (now() - pg_last_xact_replay_timestamp())) FROM pg_stat_replication LIMIT 1;\")\n\nif (( $(echo \"$CURRENT_LAG > $LAG_THRESHOLD\" | bc -l) )); then\n    echo \"ALERT: Replication lag is ${CURRENT_LAG} seconds\"\n    # Send notification\nfi\n```\n\n**Best Practices:**\n- Use streaming replication for most use cases\n- Configure appropriate wal_keep_segments or use replication slots\n- Monitor replication lag regularly\n- Test failover procedures regularly\n- Use synchronous replication for critical data consistency requirements\n- Consider using connection pooling with read replicas\n- Plan for network partitions and split-brain scenarios\n- Use logical replication for selective replication or upgrades\n- Implement monitoring and alerting for replication health\n- Document failover procedures clearly\n\n**Common Issues:**\n- WAL files accumulating on master (increase wal_keep_segments or use slots)\n- Network connectivity issues between servers\n- Standby falling too far behind (tune postgresql.conf parameters)\n- Authentication problems (check pg_hba.conf)\n- Disk space issues on archive directory\n\nProper replication setup provides high availability, disaster recovery, and the ability to distribute read workloads across multiple servers.",
      "keywords": ["postgresql", "replication", "streaming replication", "logical replication", "high availability", "standby", "failover", "wal"],
      "difficulty": "medium"
    },
    {
      "id": 14017,
      "tag": "postgresql",
      "question": "How does PostgreSQL handle JSON data and what are the differences between JSON and JSONB?",
      "answer": "PostgreSQL provides robust support for JSON data through two data types: JSON and JSONB. This makes PostgreSQL an excellent choice for applications that need both relational and document database capabilities.\n\n**JSON vs JSONB Differences:**\n\n**JSON Type:**\n- Stores exact text representation\n- Preserves formatting, whitespace, and key order\n- Faster input (no processing)\n- Slower queries (must parse on each operation)\n- No indexing support\n- Validates JSON syntax on input\n\n**JSONB Type (Binary JSON - Recommended):**\n- Stores decomposed binary format\n- Removes whitespace, doesn't preserve key order\n- Slower input (processing overhead)\n- Much faster queries\n- Full indexing support\n- Additional operators and functions\n\n**Creating Tables with JSON/JSONB:**\n\n```sql\nCREATE TABLE products (\n    id SERIAL PRIMARY KEY,\n    name VARCHAR(255),\n    specifications JSON,    -- or JSONB\n    metadata JSONB,\n    created_at TIMESTAMP DEFAULT NOW()\n);\n\n-- Sample data insertion\nINSERT INTO products (name, specifications, metadata) VALUES \n('Laptop', \n '{\n   \"processor\": \"Intel i7\",\n   \"ram\": \"16GB\",\n   \"storage\": \"512GB SSD\",\n   \"ports\": [\"USB-C\", \"HDMI\", \"USB-A\"]\n }',\n '{\n   \"warranty\": \"2 years\",\n   \"tags\": [\"electronics\", \"computer\"],\n   \"supplier\": {\n     \"name\": \"TechCorp\",\n     \"contact\": \"sales@techcorp.com\"\n   }\n }');\n```\n\n**Querying JSON/JSONB Data:**\n\n```sql\n-- Extract values using -> (returns JSON) and ->> (returns text)\nSELECT \n    name,\n    specifications->>'processor' as processor,\n    specifications->>'ram' as ram,\n    metadata->'supplier'->>'name' as supplier_name\nFROM products;\n\n-- Query nested objects\nSELECT name, metadata->'supplier'->>'contact' as supplier_contact\nFROM products\nWHERE metadata->'supplier'->>'name' = 'TechCorp';\n\n-- Query arrays\nSELECT name\nFROM products\nWHERE specifications->'ports' ? 'HDMI';  -- Check if array contains value\n\n-- Extract array elements\nSELECT \n    name,\n    jsonb_array_elements_text(specifications->'ports') as port\nFROM products;\n```\n\n**JSONB Operators:**\n\n```sql\n-- Contains (@>)\nSELECT name FROM products \nWHERE metadata @> '{\"warranty\": \"2 years\"}';\n\n-- Contained by (<@)\nSELECT name FROM products \nWHERE '{\"warranty\": \"2 years\"}' <@ metadata;\n\n-- Key exists (?)\nSELECT name FROM products \nWHERE metadata ? 'warranty';\n\n-- Any key exists (?|)\nSELECT name FROM products \nWHERE metadata ?| array['warranty', 'guarantee'];\n\n-- All keys exist (?&)\nSELECT name FROM products \nWHERE metadata ?& array['warranty', 'tags'];\n\n-- Concatenation (||)\nUPDATE products \nSET metadata = metadata || '{\"updated\": true}'\nWHERE id = 1;\n\n-- Remove key (-)\nUPDATE products \nSET metadata = metadata - 'temporary_field'\nWHERE id = 1;\n```\n\n**JSON/JSONB Functions:**\n\n```sql\n-- jsonb_build_object: Create JSON objects\nSELECT jsonb_build_object(\n    'id', id,\n    'name', name,\n    'processor', specifications->>'processor'\n) as product_summary\nFROM products;\n\n-- jsonb_agg: Aggregate into JSON array\nSELECT \n    metadata->'supplier'->>'name' as supplier,\n    jsonb_agg(jsonb_build_object(\n        'id', id,\n        'name', name,\n        'processor', specifications->>'processor'\n    )) as products\nFROM products\nGROUP BY metadata->'supplier'->>'name';\n\n-- jsonb_object_agg: Create JSON object from key-value pairs\nSELECT jsonb_object_agg(name, specifications->>'processor') as processors\nFROM products;\n\n-- jsonb_each: Expand JSON object to key-value pairs\nSELECT \n    p.name,\n    spec.key,\n    spec.value\nFROM products p,\n     jsonb_each_text(p.specifications) as spec(key, value);\n```\n\n**Indexing JSONB Data:**\n\n```sql\n-- GIN index for general JSONB queries\nCREATE INDEX idx_products_metadata_gin ON products USING GIN (metadata);\n\n-- This supports:\n-- @>, @<, ?, ?|, ?& operators\n\n-- Expression index for specific paths\nCREATE INDEX idx_products_supplier ON products \nUSING BTREE ((metadata->'supplier'->>'name'));\n\n-- This supports:\n-- WHERE metadata->'supplier'->>'name' = 'TechCorp'\n\n-- Partial index for specific conditions\nCREATE INDEX idx_products_warranty ON products \nUSING BTREE ((metadata->>'warranty')) \nWHERE metadata->>'warranty' IS NOT NULL;\n```\n\n**Updating JSON/JSONB Data:**\n\n```sql\n-- Update specific field\nUPDATE products \nSET specifications = jsonb_set(\n    specifications,\n    '{ram}',\n    '\"32GB\"'\n)\nWHERE id = 1;\n\n-- Update nested field\nUPDATE products \nSET metadata = jsonb_set(\n    metadata,\n    '{supplier, contact}',\n    '\"newsales@techcorp.com\"'\n)\nWHERE id = 1;\n\n-- Add new field\nUPDATE products \nSET metadata = metadata || '{\"last_updated\": \"2023-12-01\"}'\nWHERE id = 1;\n\n-- Remove field\nUPDATE products \nSET metadata = metadata - 'temporary_field'\nWHERE id = 1;\n\n-- Update array element\nUPDATE products \nSET specifications = jsonb_set(\n    specifications,\n    '{ports, 0}',    -- First array element\n    '\"USB-C 3.1\"'\n)\nWHERE id = 1;\n```\n\n**Advanced JSON Operations:**\n\n```sql\n-- JSON path queries (PostgreSQL 12+)\nSELECT name, jsonb_path_query(metadata, '$.tags[*]') as tag\nFROM products;\n\n-- JSON path exists\nSELECT name\nFROM products\nWHERE jsonb_path_exists(metadata, '$.supplier.contact');\n\n-- Complex JSON transformations\nWITH processed_products AS (\n    SELECT \n        id,\n        name,\n        jsonb_build_object(\n            'basic_info', jsonb_build_object(\n                'name', name,\n                'processor', specifications->>'processor',\n                'ram', specifications->>'ram'\n            ),\n            'supplier_info', metadata->'supplier',\n            'tags', metadata->'tags'\n        ) as summary\n    FROM products\n)\nSELECT * FROM processed_products;\n```\n\n**JSON Validation and Constraints:**\n\n```sql\n-- Add check constraint for JSON structure\nALTER TABLE products \nADD CONSTRAINT valid_metadata_structure \nCHECK (metadata ? 'warranty' AND metadata ? 'tags');\n\n-- Custom validation function\nCREATE OR REPLACE FUNCTION validate_product_specs(specs JSONB)\nRETURNS BOOLEAN AS $$\nBEGIN\n    RETURN specs ? 'processor' \n       AND specs ? 'ram' \n       AND specs ? 'storage';\nEND;\n$$ LANGUAGE plpgsql;\n\nALTER TABLE products \nADD CONSTRAINT valid_specifications \nCHECK (validate_product_specs(specifications));\n```\n\n**Performance Considerations:**\n\n```sql\n-- Use JSONB over JSON for most cases\n-- Create appropriate indexes:\n\n-- For containment queries (@>, <@)\nCREATE INDEX ON products USING GIN (metadata);\n\n-- For specific path queries\nCREATE INDEX ON products ((metadata->>'warranty'));\n\n-- For array membership\nCREATE INDEX ON products USING GIN ((metadata->'tags'));\n\n-- Monitor query performance\nEXPLAIN (ANALYZE, BUFFERS) \nSELECT * FROM products \nWHERE metadata @> '{\"warranty\": \"2 years\"}';\n```\n\n**Use Cases for JSON/JSONB:**\n- Storing flexible, schema-less data\n- Product catalogs with varying attributes\n- User preferences and settings\n- API response caching\n- Event logging and analytics\n- Configuration storage\n- Document management systems\n\n**Best Practices:**\n- Use JSONB unless you specifically need to preserve formatting\n- Create appropriate indexes for your query patterns\n- Validate JSON structure with constraints\n- Don't store large binary data in JSON fields\n- Consider normalization for frequently queried nested data\n- Use JSON for semi-structured data, not as a replacement for proper relational design\n\nPostgreSQL's JSON support makes it a powerful hybrid database, combining the strengths of relational and document databases.",
      "keywords": ["postgresql", "json", "jsonb", "document database", "nosql", "gin index", "json operators", "json functions"],
      "difficulty": "medium"
    },
    {
      "id": 14018,
      "tag": "postgresql",
      "question": "What are advanced PostgreSQL performance optimization techniques for large-scale applications?",
      "answer": "Large-scale PostgreSQL deployments require sophisticated optimization strategies beyond basic indexing and query tuning. These advanced techniques address complex performance challenges in high-volume, high-concurrency environments.\n\n**Table Partitioning:**\nDistribute large tables across multiple smaller tables for improved query performance and maintenance.\n\n```sql\n-- Range partitioning by date\nCREATE TABLE orders (\n    id BIGSERIAL,\n    customer_id INTEGER,\n    order_date DATE NOT NULL,\n    total DECIMAL(10,2),\n    status VARCHAR(20)\n) PARTITION BY RANGE (order_date);\n\n-- Create partitions\nCREATE TABLE orders_2023_q1 PARTITION OF orders\n    FOR VALUES FROM ('2023-01-01') TO ('2023-04-01');\n\nCREATE TABLE orders_2023_q2 PARTITION OF orders\n    FOR VALUES FROM ('2023-04-01') TO ('2023-07-01');\n\n-- Hash partitioning for even distribution\nCREATE TABLE user_sessions (\n    session_id UUID PRIMARY KEY,\n    user_id INTEGER,\n    created_at TIMESTAMP,\n    data JSONB\n) PARTITION BY HASH (user_id);\n\nCREATE TABLE user_sessions_0 PARTITION OF user_sessions\n    FOR VALUES WITH (MODULUS 4, REMAINDER 0);\n\nCREATE TABLE user_sessions_1 PARTITION OF user_sessions\n    FOR VALUES WITH (MODULUS 4, REMAINDER 1);\n\n-- List partitioning by category\nCREATE TABLE products (\n    id INTEGER,\n    name VARCHAR(255),\n    category VARCHAR(50),\n    price DECIMAL(10,2)\n) PARTITION BY LIST (category);\n\nCREATE TABLE products_electronics PARTITION OF products\n    FOR VALUES IN ('laptop', 'phone', 'tablet');\n\nCREATE TABLE products_clothing PARTITION OF products\n    FOR VALUES IN ('shirt', 'pants', 'shoes');\n```\n\n**Advanced Indexing Strategies:**\n\n```sql\n-- Bloom indexes for multiple equality conditions\nCREATE EXTENSION IF NOT EXISTS bloom;\nCREATE INDEX bloom_idx ON large_table USING bloom (col1, col2, col3, col4);\n-- Useful when you need to filter on any combination of columns\n\n-- BRIN indexes for naturally ordered large tables\nCREATE INDEX brin_idx ON time_series_data USING brin (timestamp);\n-- Very space-efficient for time-series or naturally ordered data\n\n-- Conditional indexes with multiple predicates\nCREATE INDEX idx_active_premium_users ON users (last_login)\nWHERE status = 'active' AND subscription_type = 'premium';\n\n-- Multi-column statistics for better query planning\nCREATE STATISTICS multi_col_stats (dependencies) ON customer_id, order_date FROM orders;\nANALYZE orders;\n\n-- Expression indexes for computed values\nCREATE INDEX idx_revenue_per_customer ON orders (\n    (total / NULLIF(customer_discount_rate, 0))\n);\n```\n\n**Connection Pooling and Resource Management:**\n\n```sql\n-- Configure connection limits\n-- postgresql.conf\nmax_connections = 200\nsuperuser_reserved_connections = 3\n\n-- Use PgBouncer for connection pooling\n-- pgbouncer.ini example:\n-- [databases]\n-- myapp = host=localhost port=5432 dbname=myapp\n-- \n-- [pgbouncer]\n-- pool_mode = transaction\n-- max_client_conn = 1000\n-- default_pool_size = 20\n-- server_idle_timeout = 600\n\n-- Monitor connection usage\nSELECT \n    datname,\n    COUNT(*) as connection_count,\n    COUNT(*) FILTER (WHERE state = 'active') as active_connections,\n    COUNT(*) FILTER (WHERE state = 'idle') as idle_connections\nFROM pg_stat_activity\nGROUP BY datname;\n```\n\n**Query Optimization for Complex Analytics:**\n\n```sql\n-- Use materialized views for expensive aggregations\nCREATE MATERIALIZED VIEW daily_sales_summary AS\nSELECT \n    DATE_TRUNC('day', order_date) as day,\n    COUNT(*) as order_count,\n    SUM(total) as total_revenue,\n    AVG(total) as avg_order_value,\n    COUNT(DISTINCT customer_id) as unique_customers\nFROM orders\nWHERE order_date >= CURRENT_DATE - INTERVAL '90 days'\nGROUP BY DATE_TRUNC('day', order_date);\n\n-- Create index on materialized view\nCREATE UNIQUE INDEX ON daily_sales_summary (day);\n\n-- Refresh strategy\nREFRESH MATERIALIZED VIEW CONCURRENTLY daily_sales_summary;\n\n-- Parallel query configuration\n-- postgresql.conf\nmax_parallel_workers_per_gather = 4\nmax_parallel_workers = 8\nparallel_tuple_cost = 0.1\nparallel_setup_cost = 1000\n\n-- Force parallel execution for testing\nSET force_parallel_mode = on;\nSET max_parallel_workers_per_gather = 4;\n```\n\n**Advanced Memory and I/O Optimization:**\n\n```sql\n-- Tune work_mem based on query patterns\n-- For large sorts/joins in specific sessions:\nSET work_mem = '256MB';\n\n-- Optimize shared_buffers (25-40% of RAM)\n-- postgresql.conf\nshared_buffers = '8GB'\neffective_cache_size = '24GB'  -- OS cache + shared_buffers\n\n-- WAL optimization for write-heavy workloads\nwal_buffers = '64MB'\nwal_writer_delay = '10ms'\ncommit_delay = '100'\ncommit_siblings = '10'\n\n-- Checkpoint tuning\nmax_wal_size = '4GB'\nmin_wal_size = '1GB'\ncheckpoint_completion_target = 0.9\ncheckpoint_timeout = '15min'\n```\n\n**Monitoring and Diagnostics:**\n\n```sql\n-- Advanced query monitoring\nCREATE EXTENSION IF NOT EXISTS pg_stat_statements;\n\n-- Most expensive queries by total time\nSELECT \n    query,\n    calls,\n    total_time,\n    mean_time,\n    rows,\n    100.0 * shared_blks_hit / nullif(shared_blks_hit + shared_blks_read, 0) AS hit_percent\nFROM pg_stat_statements\nORDER BY total_time DESC\nLIMIT 10;\n\n-- Buffer cache hit ratio\nSELECT \n    schemaname,\n    tablename,\n    heap_blks_read,\n    heap_blks_hit,\n    ROUND(100.0 * heap_blks_hit / NULLIF(heap_blks_hit + heap_blks_read, 0), 2) as cache_hit_ratio\nFROM pg_statio_user_tables\nWHERE heap_blks_read > 0\nORDER BY cache_hit_ratio ASC;\n\n-- I/O intensive queries\nSELECT \n    query,\n    calls,\n    total_time,\n    shared_blks_read,\n    shared_blks_written,\n    temp_blks_read,\n    temp_blks_written\nFROM pg_stat_statements\nWHERE shared_blks_read > 1000\nORDER BY shared_blks_read DESC;\n```\n\n**Write Optimization Techniques:**\n\n```sql\n-- Bulk insert optimization\n-- Use COPY for large data loads\nCOPY large_table FROM '/path/to/data.csv' WITH (FORMAT csv, HEADER true);\n\n-- Batch inserts with multiple VALUES\nINSERT INTO orders (customer_id, total, order_date) VALUES \n(1, 100.00, '2023-01-01'),\n(2, 150.00, '2023-01-01'),\n(3, 200.00, '2023-01-01');\n-- Better than multiple single INSERT statements\n\n-- Use unlogged tables for temporary processing\nCREATE UNLOGGED TABLE temp_processing_table (\n    id INTEGER,\n    data TEXT\n);\n-- No WAL logging, faster writes, but not crash-safe\n\n-- Optimize autovacuum for write-heavy tables\nALTER TABLE high_write_table SET (\n    autovacuum_vacuum_scale_factor = 0.1,\n    autovacuum_analyze_scale_factor = 0.05,\n    autovacuum_vacuum_cost_limit = 1000\n);\n```\n\n**Scaling Patterns:**\n\n```sql\n-- Read replicas for read scaling\n-- Use streaming replication with load balancing\n-- Configure application to route reads to replicas\n\n-- Functional partitioning\n-- Separate tables by access patterns:\n-- - Hot data (recent, frequently accessed)\n-- - Warm data (older, occasionally accessed)\n-- - Cold data (archived, rarely accessed)\n\nCREATE TABLE orders_hot \nPARTITION OF orders \nFOR VALUES FROM ('2023-01-01') TO ('2024-01-01');\n\nCREATE TABLE orders_warm \nPARTITION OF orders \nFOR VALUES FROM ('2022-01-01') TO ('2023-01-01');\n\n-- Use different storage for cold data\nCREATE TABLESPACE cold_storage LOCATION '/mnt/slow_storage';\nCREATE TABLE orders_cold \nPARTITION OF orders \nFOR VALUES FROM ('2020-01-01') TO ('2022-01-01')\nTABLESPACE cold_storage;\n```\n\n**Memory-Optimized Structures:**\n\n```sql\n-- Use appropriate data types to minimize memory usage\n-- SMALLINT vs INTEGER vs BIGINT\n-- TEXT vs VARCHAR(n) - TEXT is often better\n-- NUMERIC vs DECIMAL vs REAL vs DOUBLE PRECISION\n\n-- Compressed TOAST storage for large values\nALTER TABLE large_text_table ALTER COLUMN content SET STORAGE EXTENDED;\n\n-- Consider using arrays instead of separate tables for simple lists\nCREATE TABLE user_preferences (\n    user_id INTEGER,\n    notification_types TEXT[],  -- Instead of separate junction table\n    preferred_categories INTEGER[]\n);\n```\n\n**Performance Testing and Benchmarking:**\n\n```bash\n#!/bin/bash\n# Load testing with pgbench\n\n# Initialize pgbench tables\npgbench -i -s 100 testdb  # Scale factor 100\n\n# Run concurrent test\npgbench -c 50 -j 4 -T 300 -P 10 testdb\n# 50 clients, 4 threads, 300 seconds, progress every 10 seconds\n\n# Custom benchmark with specific queries\npgbench -c 20 -j 2 -T 60 -f custom_queries.sql testdb\n```\n\n**Continuous Optimization Strategy:**\n\n1. **Establish Baselines:** Regular performance metrics collection\n2. **Monitor Key Metrics:** Query response times, throughput, resource utilization\n3. **Automated Alerting:** Set up alerts for performance degradation\n4. **Regular Review:** Weekly/monthly performance review sessions\n5. **Capacity Planning:** Proactive scaling based on growth trends\n6. **Testing Environment:** Mirror production for performance testing\n\n**Common Anti-Patterns to Avoid:**\n- Over-indexing (too many indexes slow down writes)\n- Ignoring query patterns when designing indexes\n- Not monitoring long-running transactions\n- Insufficient connection pooling\n- Poor partitioning strategy\n- Not updating table statistics regularly\n- Using generic configurations without workload-specific tuning\n\nAdvanced PostgreSQL optimization requires understanding your specific workload patterns, continuous monitoring, and iterative improvements based on real-world performance data.",
      "keywords": ["postgresql", "performance optimization", "partitioning", "advanced indexing", "connection pooling", "scaling", "monitoring", "large scale"],
      "difficulty": "hard"
    },
    {
      "id": 14019,
      "tag": "postgresql",
      "question": "Explain PostgreSQL extensions, custom data types, and advanced features like full-text search and geospatial capabilities.",
      "answer": "PostgreSQL's extensibility is one of its greatest strengths, allowing developers to add custom functionality, data types, and specialized capabilities beyond standard SQL features. This extensibility makes PostgreSQL suitable for diverse use cases from traditional applications to specialized domains like GIS and text analytics.\n\n**PostgreSQL Extensions System:**\n\nExtensions provide packaged functionality that can be easily installed and managed.\n\n```sql\n-- List available extensions\nSELECT name, default_version, comment \nFROM pg_available_extensions \nORDER BY name;\n\n-- Install extensions\nCREATE EXTENSION IF NOT EXISTS hstore;\nCREATE EXTENSION IF NOT EXISTS \"uuid-ossp\";\nCREATE EXTENSION IF NOT EXISTS pg_trgm;\nCREATE EXTENSION IF NOT EXISTS btree_gin;\n\n-- View installed extensions\nSELECT extname, extversion, nspname as schema\nFROM pg_extension e\nJOIN pg_namespace n ON e.extnamespace = n.oid;\n\n-- Update extension\nALTER EXTENSION hstore UPDATE TO '1.8';\n\n-- Drop extension\nDROP EXTENSION IF EXISTS old_extension CASCADE;\n```\n\n**Custom Data Types:**\n\nCreate domain types and composite types for specialized data handling.\n\n```sql\n-- Domain types (constrained base types)\nCREATE DOMAIN email_address AS TEXT\nCHECK (VALUE ~ '^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}$');\n\nCREATE DOMAIN positive_numeric AS NUMERIC\nCHECK (VALUE > 0);\n\nCREATE DOMAIN us_postal_code AS TEXT\nCHECK (VALUE ~ '^\\d{5}(-\\d{4})?$');\n\n-- Composite types\nCREATE TYPE address AS (\n    street TEXT,\n    city TEXT,\n    state CHAR(2),\n    zip_code us_postal_code\n);\n\nCREATE TYPE money_amount AS (\n    amount DECIMAL(15,2),\n    currency CHAR(3)\n);\n\n-- Enum types\nCREATE TYPE order_status AS ENUM (\n    'pending', 'processing', 'shipped', 'delivered', 'cancelled'\n);\n\n-- Using custom types\nCREATE TABLE customers (\n    id SERIAL PRIMARY KEY,\n    name TEXT NOT NULL,\n    email email_address UNIQUE,\n    billing_address address,\n    shipping_address address\n);\n\nCREATE TABLE orders (\n    id SERIAL PRIMARY KEY,\n    customer_id INTEGER REFERENCES customers(id),\n    status order_status DEFAULT 'pending',\n    total money_amount\n);\n\n-- Querying composite types\nSELECT \n    name,\n    (billing_address).city,\n    (billing_address).state\nFROM customers\nWHERE (billing_address).state = 'CA';\n```\n\n**Full-Text Search:**\n\nPostgreSQL provides sophisticated full-text search capabilities with ranking and highlighting.\n\n```sql\n-- Create table with text search vector\nCREATE TABLE documents (\n    id SERIAL PRIMARY KEY,\n    title TEXT,\n    content TEXT,\n    search_vector tsvector\n);\n\n-- Create GIN index for full-text search\nCREATE INDEX idx_documents_search ON documents USING GIN(search_vector);\n\n-- Insert sample data with search vectors\nINSERT INTO documents (title, content, search_vector) VALUES\n('PostgreSQL Guide', 'Complete guide to PostgreSQL database management', \n to_tsvector('english', 'Complete guide to PostgreSQL database management')),\n('Database Performance', 'Optimizing database queries for better performance',\n to_tsvector('english', 'Optimizing database queries for better performance'));\n\n-- Automatic search vector updates with triggers\nCREATE OR REPLACE FUNCTION update_search_vector()\nRETURNS TRIGGER AS $$\nBEGIN\n    NEW.search_vector := to_tsvector('english', COALESCE(NEW.title, '') || ' ' || COALESCE(NEW.content, ''));\n    RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n\nCREATE TRIGGER documents_search_update\n    BEFORE INSERT OR UPDATE ON documents\n    FOR EACH ROW EXECUTE FUNCTION update_search_vector();\n\n-- Full-text search queries\n-- Basic search\nSELECT title, content\nFROM documents\nWHERE search_vector @@ to_tsquery('english', 'PostgreSQL & database');\n\n-- Search with ranking\nSELECT \n    title,\n    content,\n    ts_rank(search_vector, query) as rank\nFROM documents,\n     to_tsquery('english', 'PostgreSQL | database') as query\nWHERE search_vector @@ query\nORDER BY rank DESC;\n\n-- Search with highlighting\nSELECT \n    title,\n    ts_headline('english', content, query, 'MaxWords=20, MinWords=5') as highlight\nFROM documents,\n     to_tsquery('english', 'PostgreSQL & performance') as query\nWHERE search_vector @@ query;\n\n-- Fuzzy matching with pg_trgm\nCREATE EXTENSION IF NOT EXISTS pg_trgm;\n\n-- Similarity search\nSELECT title, similarity(title, 'PostgreSQL') as sim\nFROM documents\nWHERE title % 'PostgreSQL'  -- % operator for similarity\nORDER BY sim DESC;\n\n-- GIN index for trigram search\nCREATE INDEX idx_documents_title_trgm ON documents USING GIN(title gin_trgm_ops);\n```\n\n**Geospatial Capabilities with PostGIS:**\n\nPostGIS extends PostgreSQL with comprehensive GIS functionality.\n\n```sql\n-- Install PostGIS extension\nCREATE EXTENSION IF NOT EXISTS postgis;\n\n-- Create table with geometry columns\nCREATE TABLE locations (\n    id SERIAL PRIMARY KEY,\n    name TEXT,\n    location GEOMETRY(POINT, 4326),  -- WGS 84 coordinate system\n    boundary GEOMETRY(POLYGON, 4326)\n);\n\n-- Create spatial index\nCREATE INDEX idx_locations_geom ON locations USING GIST(location);\nCREATE INDEX idx_locations_boundary ON locations USING GIST(boundary);\n\n-- Insert spatial data\nINSERT INTO locations (name, location) VALUES\n('New York City', ST_SetSRID(ST_Point(-74.006, 40.7128), 4326)),\n('Los Angeles', ST_SetSRID(ST_Point(-118.2437, 34.0522), 4326)),\n('Chicago', ST_SetSRID(ST_Point(-87.6298, 41.8781), 4326));\n\n-- Spatial queries\n-- Distance calculations\nSELECT \n    l1.name as city1,\n    l2.name as city2,\n    ST_Distance(\n        ST_Transform(l1.location, 3857),  -- Web Mercator for accurate distance\n        ST_Transform(l2.location, 3857)\n    ) / 1000 as distance_km\nFROM locations l1\nCROSS JOIN locations l2\nWHERE l1.id < l2.id;\n\n-- Find locations within radius\nSELECT name\nFROM locations\nWHERE ST_DWithin(\n    ST_Transform(location, 3857),\n    ST_Transform(ST_SetSRID(ST_Point(-74.006, 40.7128), 4326), 3857),\n    500000  -- 500 km\n);\n\n-- Point in polygon queries\nSELECT l.name\nFROM locations l\nJOIN administrative_boundaries ab ON ST_Within(l.location, ab.boundary)\nWHERE ab.name = 'New York State';\n\n-- Create buffer zones\nUPDATE locations \nSET boundary = ST_Buffer(\n    ST_Transform(location, 3857),\n    10000  -- 10 km buffer\n)\nWHERE boundary IS NULL;\n```\n\n**Advanced JSON/JSONB Operations:**\n\n```sql\n-- JSON Path queries (PostgreSQL 12+)\nCREATE TABLE api_logs (\n    id SERIAL PRIMARY KEY,\n    timestamp TIMESTAMP DEFAULT NOW(),\n    request_data JSONB,\n    response_data JSONB\n);\n\n-- Insert sample data\nINSERT INTO api_logs (request_data, response_data) VALUES\n('{\n    \"method\": \"GET\",\n    \"path\": \"/api/users\",\n    \"params\": {\"page\": 1, \"limit\": 10},\n    \"headers\": {\"authorization\": \"Bearer token123\"}\n}',\n'{\n    \"status\": 200,\n    \"data\": [{\"id\": 1, \"name\": \"John\"}, {\"id\": 2, \"name\": \"Jane\"}],\n    \"meta\": {\"total\": 150, \"pages\": 15}\n}');\n\n-- JSON Path queries\nSELECT \n    id,\n    jsonb_path_query(request_data, '$.method') as method,\n    jsonb_path_query(response_data, '$.status') as status,\n    jsonb_path_query_array(response_data, '$.data[*].name') as user_names\nFROM api_logs;\n\n-- Advanced JSON aggregations\nSELECT \n    jsonb_path_query(request_data, '$.method') as method,\n    COUNT(*) as request_count,\n    jsonb_agg(jsonb_path_query(response_data, '$.status')) as status_codes\nFROM api_logs\nGROUP BY jsonb_path_query(request_data, '$.method');\n```\n\n**Time Series and Analytics:**\n\n```sql\n-- Install TimescaleDB extension (if available)\n-- CREATE EXTENSION IF NOT EXISTS timescaledb;\n\n-- Window functions for analytics\nCREATE TABLE sensor_data (\n    sensor_id INTEGER,\n    timestamp TIMESTAMP,\n    temperature DECIMAL(5,2),\n    humidity DECIMAL(5,2)\n);\n\n-- Advanced analytics queries\nSELECT \n    sensor_id,\n    timestamp,\n    temperature,\n    -- Moving averages\n    AVG(temperature) OVER (\n        PARTITION BY sensor_id \n        ORDER BY timestamp \n        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW\n    ) as temp_5_point_avg,\n    -- Lag/Lead for change detection\n    temperature - LAG(temperature, 1) OVER (\n        PARTITION BY sensor_id \n        ORDER BY timestamp\n    ) as temp_change,\n    -- Percentile functions\n    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY temperature) OVER (\n        PARTITION BY sensor_id, DATE_TRUNC('hour', timestamp)\n    ) as hourly_median_temp\nFROM sensor_data\nORDER BY sensor_id, timestamp;\n```\n\n**Custom Operators and Functions in C:**\n\n```c\n// Example C extension (simplified)\n#include \"postgres.h\"\n#include \"fmgr.h\"\n#include \"utils/builtins.h\"\n\nPG_MODULE_MAGIC;\n\n// Custom function\nPG_FUNCTION_INFO_V1(custom_add);\n\nDatum\ncustom_add(PG_FUNCTION_ARGS)\n{\n    int32 arg1 = PG_GETARG_INT32(0);\n    int32 arg2 = PG_GETARG_INT32(1);\n    \n    PG_RETURN_INT32(arg1 + arg2);\n}\n```\n\n```sql\n-- Register C function\nCREATE OR REPLACE FUNCTION custom_add(integer, integer)\nRETURNS integer\nAS 'my_extension', 'custom_add'\nLANGUAGE C STRICT;\n\n-- Create custom operator\nCREATE OPERATOR +++ (\n    LEFTARG = integer,\n    RIGHTARG = integer,\n    FUNCTION = custom_add,\n    COMMUTATOR = +++\n);\n\n-- Usage\nSELECT 5 +++ 3;  -- Returns 8\n```\n\n**Extension Management Best Practices:**\n\n```sql\n-- Create extensions in separate schemas for organization\nCREATE SCHEMA extensions;\nCREATE EXTENSION hstore SCHEMA extensions;\n\n-- Version management\nSELECT \n    extname,\n    extversion,\n    (SELECT version FROM pg_available_extensions WHERE name = extname) as available_version\nFROM pg_extension;\n\n-- Backup considerations\n-- Extensions are included in pg_dump by default\n-- Custom types and functions need special consideration\n\n-- Security considerations\nGRANT USAGE ON SCHEMA extensions TO application_role;\nGRANT EXECUTE ON ALL FUNCTIONS IN SCHEMA extensions TO application_role;\n```\n\n**Popular PostgreSQL Extensions:**\n\n- **PostGIS:** Geospatial functionality\n- **pg_stat_statements:** Query performance monitoring\n- **hstore:** Key-value store data type\n- **uuid-ossp:** UUID generation functions\n- **pg_trgm:** Trigram matching for fuzzy text search\n- **btree_gin/btree_gist:** Additional index types\n- **pgcrypto:** Cryptographic functions\n- **TimescaleDB:** Time-series database capabilities\n- **pg_partman:** Partition management\n- **foreign data wrappers:** Access external data sources\n\n**Development and Testing:**\n\n```sql\n-- Extension development workflow\n-- 1. Create extension files (control, sql, shared library)\n-- 2. Install in PostgreSQL extension directory\n-- 3. Test installation and functionality\n-- 4. Version management and upgrades\n\n-- Testing custom extensions\nBEGIN;\n    CREATE EXTENSION test_extension;\n    -- Run tests\n    SELECT test_function(1, 2);\n    -- Verify results\nROLLBACK;  -- Clean up test\n```\n\nPostgreSQL's extensibility enables it to adapt to virtually any use case, from traditional OLTP applications to specialized domains like GIS, time-series analysis, and full-text search. This flexibility, combined with its reliability and performance, makes PostgreSQL a powerful platform for complex applications.",
      "keywords": ["postgresql", "extensions", "custom data types", "full-text search", "postgis", "geospatial", "json path", "tsvector", "composite types", "domains"],
      "difficulty": "hard"
    }
  ]
}